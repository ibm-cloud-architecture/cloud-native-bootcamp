{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBM Cloud Native Bootcamp","text":""},{"location":"#bootcamp-overview","title":"Bootcamp Overview","text":"<p>This Cloud Native Bootcamp has been created to teach and guide IBMers, Business Partners, and clients what it takes to move to the cloud. We want to provide a way for anyone using this site to come away with hands-on experience in each of the different technologies listed below.</p>"},{"location":"#concepts-covered","title":"Concepts Covered","text":"<ul> <li> <p> Cloud Native</p> <p>Moving to the cloud comes with new concepts and standards that should be understood before starting your journey to cloud. Learn about them by clicking the link below.</p> <p> Getting started</p> </li> <li> <p> Containers</p> <p>The first task when moving to the cloud is getting your applications running in containers. Get your hands on with containers by clicking the link below.</p> <p> Containerization</p> </li> <li> <p> Kubernetes/OpenShift</p> <p>Managing hundreds of containers is chaos to manage on your own. Learn how container orchestration can make it easy using Kubernetes or OpenShift.</p> <p> Container Orchestration</p> </li> <li> <p> DevOps/GitOps</p> <p>DevOps/GitOps adds standards and reliability to your development lifecycle. Learn how to use it by clicking below.</p> <p> DevOps</p> </li> </ul>"},{"location":"#how-to-approach-the-bootcamp","title":"How to approach the Bootcamp","text":"<p>This bootcamp has been designed to give the students a better hands-on experience than just copy/paste. The bootcamp uses the approach of \"Read, listen, watch, try it out\". We want students to take what they have heard and use their resources to solve the labs on their own without much instruction.</p>"},{"location":"agenda/","title":"Agenda","text":"<p>The following table lists the topics and coding activities for the week. Click on the name of the topic to open a pdf of the material. Click on the link to the solution code to view the solution.</p> In-PersonSelf Paced"},{"location":"agenda/#day-1","title":"Day 1","text":"Topic Type of Activity Kickoff Activity Introductions Activity Introduction Cloud Native Presentation Containers Presentation Container Activities Activity Lunch Activity Container Activities (Cont.) Activity Kubernetes Presentation Wrap up"},{"location":"agenda/#day-2","title":"Day 2","text":"Topic Type of Activity Recap and review from Monday; Q&amp;A Presentation Kubernetes Activities Activity Lunch Activity Kubernetes Presentation Wrap up"},{"location":"agenda/#day-3","title":"Day 3","text":"Topic Type of Activity Recap and review from Tuesday; Q&amp;A Presentation Kubernetes Activities Activity Continuous Integration Presentation Lunch Activity Continuous Integration Lab Activity Continuous Deployment Presentation Wrap up"},{"location":"agenda/#day-4","title":"Day 4","text":"Topic Type of Activity Recap and review from Wednesday; Q&amp;A Presentation Continuous Deployment Lab Activity Lunch Project Work Activity"},{"location":"agenda/#day-5","title":"Day 5","text":"Topic Type of Activity Recap and review from Thursday ; Q&amp;A Presentation Project Work Activity Retrospective Activity"},{"location":"agenda/#modules","title":"Modules","text":"Topic Type of Activity Duration Containers Presentation 1 Hour Container Activities Activity 30 mins Kubernetes Presentation 6 Hours Kubernetes Activities Activity 4 Hours Continuous Integration Presentation 1 Hour Continuous Integration Lab Activity 1 Hour Continuous Deployment Presentation 1 Hour Continuous Deployment Lab Activity 1 Hour Project Work Activity 2 Hours"},{"location":"cloudnative-challenge/","title":"Cloud Native Challenge","text":""},{"location":"cloudnative-challenge/#phase-1-local-develop","title":"Phase 1 - Local Develop","text":"<ul> <li>Start by creating a Github Repo for your application.</li> <li>Choose <code>NodeJS</code>, <code>Python</code>, or <code>React</code>.</li> <li>Site about one of the following:<ul> <li>Yourself</li> <li>Hobby</li> <li>Place you live</li> </ul> </li> <li>Must be able to run locally</li> </ul>"},{"location":"cloudnative-challenge/#application-requirements","title":"Application Requirements","text":"<ul> <li>Minimum of 3 webpages</li> <li>Minimum of 1 GET and POST method each.</li> <li>SwaggerUI Configured for API Testing.</li> <li>API's exposed through Swagger</li> <li>Custom CSS files for added formatting.</li> </ul>"},{"location":"cloudnative-challenge/#testing","title":"Testing","text":"<p>Setup each of the following tests that apply:</p> <ul> <li>Page tests</li> <li>API tests</li> <li>Connection Tests</li> </ul>"},{"location":"cloudnative-challenge/#phase-2-application-enhancements","title":"Phase 2 - Application Enhancements","text":""},{"location":"cloudnative-challenge/#database-connectivity-and-functionality","title":"Database Connectivity and Functionality","text":"<ul> <li>Add local or cloud DB to use for data collection.</li> <li>Use 3rd party API calls to get data.<ul> <li>Post Data to DB via API Call</li> <li>Retrieve Data from DB via API Call</li> <li>Delete Data from DB via API Call</li> </ul> </li> </ul>"},{"location":"cloudnative-challenge/#phase-3-containerize","title":"Phase 3 - Containerize","text":""},{"location":"cloudnative-challenge/#container-image","title":"Container Image","text":"<ul> <li>Create a DockerFile</li> <li>Build your docker image from the dockerfile</li> <li>Run it locally via Docker Desktop or another docker engine.</li> </ul>"},{"location":"cloudnative-challenge/#image-registries","title":"Image Registries","text":"<ul> <li>Once validation of working docker image, push the image up to a registry.</li> <li>Use one of the following registries:<ul> <li>Docker</li> <li>Quay.io</li> <li>IBM Container</li> </ul> </li> <li>Push the image up with the following name: <code>{DockerRegistry}/{yourusername}/techdemos-cn:v1</code></li> </ul>"},{"location":"cloudnative-challenge/#phase-4-kubernetes-ready","title":"Phase 4 - Kubernetes Ready","text":""},{"location":"cloudnative-challenge/#create-pod-and-deployment-files","title":"Create Pod and Deployment files","text":"<ul> <li>Create a <code>Pod</code> YAML to validate your image.</li> <li>Next, create a <code>deployment</code> yaml file with the setting of 3 replicas.</li> <li>Verify starting of deployment</li> <li>Push all YAML files to Github</li> </ul>"},{"location":"cloudnative-challenge/#application-exposing","title":"Application Exposing","text":"<ul> <li>Create a <code>Service</code> and <code>Route</code> yaml</li> <li>Save <code>Service</code> and <code>Route</code> yamls in Github</li> </ul>"},{"location":"cloudnative-challenge/#configuration-setup","title":"Configuration Setup","text":"<ul> <li>Create a <code>ConfigMap</code> for all site configuration.</li> <li>Setup <code>Secrets</code> for API keys or Passwords to 3rd parties.</li> <li>Add storage where needed to deployment.</li> </ul>"},{"location":"cloudnative-challenge/#phase-5-devopsgitops","title":"Phase 5 - Devops/Gitops","text":""},{"location":"cloudnative-challenge/#tekton-pipeline-setup","title":"Tekton Pipeline Setup","text":"<ul> <li>Create a Tekton pipeline to do the following:<ul> <li>Setup</li> <li>Test</li> <li>Build and Push Image</li> <li>GitOps Version Update</li> </ul> </li> <li>Make each of the above their own task.</li> <li>Setup triggers to respond to Github commits and PR's</li> </ul>"},{"location":"cloudnative-challenge/#gitsops-configuration","title":"GitsOps Configuration","text":"<ul> <li>Use ArgoCD to setup Deployment.</li> <li>Test your ArgoCD deployment</li> <li>Make a change to site and push them.</li> <li>Validate new image version.</li> </ul>"},{"location":"cloudnative-challenge/#extras","title":"Extras","text":""},{"location":"cloudnative-challenge/#chatbot-functions","title":"Chatbot Functions","text":"<ul> <li>Watson Assistant Integration</li> <li>Conversation about your sites topic.</li> <li>Have Chat window or page.</li> <li>Integrate Watson Assistant Actions.</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#required-skills","title":"Required skills","text":"<p>This activities contained here require you to be proficient in working from the command line with a linux shell (Bash, Zsh, etc.) Below is a partial list of activities you should be able to perform. </p> <ul> <li>Copy, move, and rename files</li> <li>Understand linux file permissions</li> <li>Edit text files (vi, vim, emacs, etc)</li> <li>Edit environment variables ($PATH)</li> </ul> <p>Here is a course for learning (or brushing up) on working from the linux command line Linux Command Line Basics</p>"},{"location":"prerequisites/#workstation-setup","title":"Workstation Setup","text":"Openshift (MacOS/Linux)Openshift (Windows)Kubernetes (MacOS/Linux)Kubernetes (Windows)"},{"location":"prerequisites/#create-accounts","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one aleady. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> <li> <p>RedHat Account: Needed for CodeReady Containers.</p> </li> </ul>"},{"location":"prerequisites/#run-system-check-script","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using <code>wget</code>:</p> <pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>After the script is run, make sure to install any missing tools.</p>"},{"location":"prerequisites/#install-clis-and-tools","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"prerequisites/#create-accounts_1","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one aleady. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> <li> <p>RedHat Account: Needed for CodeReady Containers.</p> </li> </ul>"},{"location":"prerequisites/#cloud-native-vm","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"prerequisites/#install-clis-and-tools_1","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.</p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>OpenShift CodeReady Containers (CRC)</p> </li> </ul> <p> <p>Warning: Make sure you have Cisco VPN turned off when using CRC.</p> <p></p>"},{"location":"prerequisites/#create-accounts_2","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one aleady. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> </ul>"},{"location":"prerequisites/#run-system-check-script_1","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using wget: <pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre></p> <p>Using curl: <pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre></p> <p>After the script is run, make sure to install any missing tools.</p>"},{"location":"prerequisites/#install-clis-and-tools_2","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>!!! Note     If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>Minikube: Follow the instructions for your Operating System.</p> </li> </ul> <p> <p>Warning: Make sure you have Cisco VPN turned off when using minikube.</p> <p></p>"},{"location":"prerequisites/#create-accounts_3","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one aleady. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> </ul>"},{"location":"prerequisites/#cloud-native-vm_1","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"prerequisites/#install-clis-and-tools_3","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>Minikube: Follow the instructions for your Operating System.</p> </li> </ul> <p> <p>Warning: Make sure you have Cisco VPN turned off when using minikube.</p> <p></p>"},{"location":"prerequisites/#environment-setup","title":"Environment Setup","text":"MiniKubeOpenShift LocalIKSOpenShift on IBM Cloud (4.x) <ul> <li>Verify your cluster has 4GB+ memory, and kubernetes 1.16+     <pre><code>minikube config view\n</code></pre></li> <li>Verify your <code>vm-driver</code> is set for <code>hyperkit</code> <pre><code>minikube config set vm-driver hyperkit\n</code></pre></li> <li>In case memory is not set, or need to increase set the memory and recreate the VM     <pre><code>minikube config set memory 4096\nminikube config set kubernetes-version v1.16.6\nminikube delete\nminikube start\n</code></pre></li> <li>Kubernetes should be v1.15+     <pre><code>kubectl version\n</code></pre></li> </ul> <p>Make sure OpenShift Local is installed. Check out the OpenShift Local Page.</p> <p>** Setup CRC ** <pre><code>crc setup\n</code></pre> ** Start CRC ** <pre><code>crc start\n</code></pre></p> <ul> <li> <p>Login to IBM Cloud with your IBM ID.</p> </li> <li> <p>Click \"Create Resource\" and search for \"kubernetes service\".</p> </li> <li> <p>Select the tile for \"Kubernetes Service\" and do the following:</p> </li> <li>Select the \"Free Cluster\" plan.</li> <li>Name your cluster.</li> <li> <p>Select \"Create\" at the bottom right of the screen.</p> </li> <li> <p>Once the Cluster is provisioned, Click on the \"Connect via CLI\" in the top right corner.</p> </li> <li> <p>Follow the instructions to connect and you are set to go.</p> </li> </ul> <ul> <li> <p>In this approach you share an OpenShift cluster on IBM Cloud with other bootcamp attendees.</p> </li> <li> <p>Considering 10-15 attendees we recommend a cluster with 3 worker nodes (each 8 vCPUs + 32GB RAM - b3c.8x32).</p> </li> <li> <p>Ask your IBM cloud account owner to provide access to an OpenShift cluster.</p> </li> <li> <p>In addition to the IBM Cloud CLI also install the OpenShift Origin CLI to be able to execute all commands.</p> </li> <li> <p>Open your OpenShift web console from within your IBM cloud account, select your profile and choose \"copy login command\" to retrieve an access token for the login.</p> </li> <li> <p>Login with your OpenShift Origin CLI.     <pre><code>oc login --token=&lt;token&gt; --server=&lt;server-url&gt;:&lt;server-port&gt;\n</code></pre></p> </li> <li> <p>Create your own project / namespace in OpenShift that you will leverage across all labs.     <pre><code>oc new-project &lt;dev-your_initials&gt;\n</code></pre></p> </li> <li> <p>Validate in the OpenShift web console that your project has been created (Administrator view -&gt; Home -&gt; Projects)</p> </li> </ul>"},{"location":"prerequisites/#next-steps","title":"Next Steps","text":"<p>Once Setup is complete, you can now begin reading our about [Cloud Native](./cloud-native/index.md by clicking the link, or the <code>Next</code> button below.</p>"},{"location":"cloud/","title":"Journey to the Cloud","text":""},{"location":"cloud/#introduction","title":"Introduction","text":"<p>Cloud is everywhere. Today, many companies want to migrate their applications on to cloud. For this migration to be done, the applications must be re-architected in a way that they fully utilize the advantages of the cloud.</p>"},{"location":"cloud/benefits/","title":"Benefits of Cloud","text":""},{"location":"cloud/benefits/#cost-efficiency","title":"Cost Efficiency","text":"<p>Moving to the cloud can significantly reduce capital expenditure (CapEx) by eliminating the need for expensive data center infrastructure, hardware maintenance, and physical security. Cloud providers offer pay-as-you-go pricing models, allowing organizations to pay only for the resources they actually use. This flexibility helps optimize operational costs and converts fixed costs into variable expenses.</p>"},{"location":"cloud/benefits/#scalability-and-flexibility","title":"Scalability and Flexibility","text":"<p>Cloud platforms provide unprecedented scalability options. Applications can automatically scale up or down based on demand, ensuring optimal performance during peak times while reducing costs during slower periods. This elastic nature of cloud resources is impossible to achieve with traditional on-premises infrastructure without significant over provisioning.</p>"},{"location":"cloud/benefits/#global-reach-and-availability","title":"Global Reach and Availability","text":"<p>Cloud providers maintain data centers worldwide, enabling organizations to deploy applications closer to their users for better performance. This global infrastructure also provides built-in redundancy and disaster recovery capabilities, offering higher availability than most on-premises solutions can achieve cost-effectively.</p>"},{"location":"cloud/benefits/#security-and-compliance","title":"Security and Compliance","text":"<p>Leading cloud providers invest heavily in security measures that often exceed what individual organizations can implement. They offer advanced security features, regular security updates, and compliance certifications that help organizations meet various regulatory requirements. While security remains a shared responsibility, cloud providers handle much of the infrastructure security burden.</p>"},{"location":"cloud/benefits/#innovation-and-time-to-market","title":"Innovation and Time-to-Market","text":"<p>Cloud services provide access to cutting-edge technologies like artificial intelligence, machine learning, and IoT platforms without requiring significant investment in research and development. This accessibility accelerates innovation and reduces time-to-market for new features and products. Organizations can quickly experiment with new technologies and scale successful initiatives.</p>"},{"location":"cloud/benefits/#maintenance-and-updates","title":"Maintenance and Updates","text":"<p>Cloud providers handle infrastructure maintenance, hardware updates, and system patching, allowing IT teams to focus on business-critical tasks rather than routine maintenance. This automated maintenance ensures systems are always up-to-date and running on the latest hardware without service interruptions.</p>"},{"location":"cloud/benefits/#environmental-impact","title":"Environmental Impact","text":"<p>Cloud data centers typically operate more efficiently than individual company data centers, leading to reduced energy consumption and carbon footprint. Cloud providers often invest in renewable energy sources and implement advanced cooling technologies, making cloud computing a more environmentally sustainable choice.</p>"},{"location":"cloud/benefits/#business-continuity","title":"Business Continuity","text":"<p>Cloud platforms offer robust disaster recovery and backup solutions that can be implemented more easily and cost-effectively than traditional methods. Automatic data replication across multiple regions ensures business continuity even in the face of major disruptions, reducing downtime and data loss.</p>"},{"location":"cloud/cn-apps/","title":"Cloud Native Applications","text":""},{"location":"cloud/cn-apps/#what-is-cloud-native","title":"What is Cloud-Native?","text":"<p>Cloud-native is about how we build and run applications taking full advantage of cloud computing rather than worrying about where we deploy it.</p> <p>Cloud-native refers less to where an application resides and more to how it is built and deployed.</p> <ul> <li> <p>A cloud-native application consists of discrete, reusable components     known as microservices that are designed to integrate into any cloud     environment.</p> </li> <li> <p>These microservices act as building blocks and are often packaged in     containers.</p> </li> <li> <p>Microservices work together as a whole to comprise an application,     yet each can be independently scaled, continuously improved, and     quickly iterated through automation and orchestration processes.</p> </li> <li> <p>The flexibility of each microservice adds to the agility and     continuous improvement of cloud-native applications.</p> </li> </ul>"},{"location":"cloud/cn-apps/#why-cloud-native","title":"Why Cloud-Native?","text":"<p>Cloud-native applications are different from the traditional applications that run in your data centres. The applications that are designed in the traditional way are not built keeping cloud compatibility in mind. They may have strong ties with the internal systems. Also, they cannot take advantage of all the benefits of the cloud.</p> <p>So, we need a new architecture for our applications to utilize the benefits of cloud. There is a need to design the applications keeping cloud in mind and take advantage of several cloud services like storage, queuing, caching etc.</p> <ul> <li> <p>Speed, safety, and scalability comes with cloud-native applications.</p> </li> <li> <p>Helps you to quickly deliver the advancements.</p> </li> <li> <p>Allows you to have loose ties into the corporate IT where it most     certainly would destabilize legacy architectures.</p> </li> <li> <p>Helps you to continuously deliver your applications with zero     downtime.</p> </li> <li> <p>Infrastructure is less predictable.</p> </li> <li> <p>Service instances are all disposable.</p> </li> <li> <p>Deployments are immutable.</p> </li> <li> <p>To meet the expectations of the today\u2019s world customers, these     systems are architected for elastic scalability.</p> </li> </ul>"},{"location":"cloud/cn-concepts/","title":"Cloud Concepts","text":""},{"location":"cloud/cn-concepts/#cloud-native-concepts","title":"Cloud-native concepts","text":"<p>Some of the important characteristics of cloud-native applications are as follows.</p> <ul> <li> <p>Disposable Infrastructure</p> </li> <li> <p>Isolation</p> </li> <li> <p>Scalability</p> </li> <li> <p>Disposable architecture</p> </li> <li> <p>Value added cloud services</p> </li> <li> <p>Polyglot cloud</p> </li> <li> <p>Self-sufficient, full-stack teams</p> </li> <li> <p>Cultural Change</p> </li> </ul> <p>Disposable Infrastructure</p> <p>While creating applications on cloud, you need several cloud resources as part of it. We often hear how easy it is to create all these resources. But did you ever think how easy is it to dispose them. It is definitely not that easy to dispose them and that is why you don\u2019t hear a lot about it.</p> <p>In traditional or legacy applications, we have all these resources residing on machines. If these go down, we need to redo them again and most of this is handled by the operations team manually. So, when we are creating applications on cloud, we bring those resources like load balancers, databases, gateways, etc on to cloud as well along with machine images and containers.</p> <p>While creating these applications, you should always keep in mind that if you are creating a resource when required, you should also be able to destroy it when not required. Without this, we cannot achieve the factors speed, safety and scalability. If you want this to happen, we need automation.</p> <p>Automation allows you to</p> <ul> <li> <p>Deliver new features at any time.</p> </li> <li> <p>Deliver patches faster.</p> </li> <li> <p>Improves the system quality.</p> </li> <li> <p>Facilitates team scale and efficiency.</p> </li> </ul> <p>Now you know what we are talking about. Disposable infrastructure is nothing but <code>Infrastructure as Code</code>.</p> <p>Infrastructure as Code</p> <p>Here, you develop the code for automation exactly as same as the you do for the rest of the application using agile methodologies.</p> <ul> <li> <p>Automation code is driven by a story.</p> </li> <li> <p>Versioned in the same repository as rest of the code.</p> </li> <li> <p>Continuously tested as part of CI/CD pipeline.</p> </li> <li> <p>Test environments are created and destroyed along with test runs.</p> </li> </ul> <p>Thus, disposable infrastructure lays the ground work for scalability and elasticity.</p> <p>Isolation</p> <p>In traditional or legacy applications, the applications are monoliths. So, when there is bug or error in the application, you need to fix it. Once you changed the code, the entire application should be redeployed. Also, there may be side effects which you can never predict. New changes may break any components in the application as they are all inter related.</p> <p>In cloud-native applications, to avoid the above scenario, the system is decomposed into bounded isolated components. Each service will be defined as one component and they are all independent of each other. So, in this case, when there is a bug or error in the application, you know which component to fix and this also avoids any side effects as the components are all unrelated pieces of code.</p> <p>Thus, cloud-native systems must be resilient to man made errors. To achieve this we need isolation and this avoids a problem in one component affecting the entire system. Also, it helps you to introduce changes quickly in the application with confidence.</p> <p>Scalability</p> <p>Simply deploying your application on cloud does not make it cloud-native. To be cloud native it should be able to take full benefits of the cloud. One of the key features is Scalability.</p> <p>In today\u2019s world, once your business starts growing, the number of users keep increasing and they may be from different locations. Your application should be able to support more number of devices and it should also be able to maintain its responsiveness. Moreover, this should be efficient and cost-effective.</p> <p>To achieve this, cloud native application runs in multiple runtimes spread across multiple hosts. The applications should be designed and architected in a way that they support multi regional, active-active deployments. This helps you to increase the availability and avoids single point of failures.</p> <p>Disposable architecture</p> <p>Leveraging the disposable infrastructure and scaling isolated components is important for cloud native applications. Disposable architecture is based on this and it takes the idea of disposability and replacement to the next level.</p> <p>Most of us think in a monolithic way because we got used to traditional or legacy applications a lot. This may lead us to take decisions in monolithic way rather than in cloud native way. In monoliths, we tend to be safe and don\u2019t do a lot of experimentation. But Disposable architecture is exactly opposite to monolithic thinking. In this approach, we develop small pieces of the component and keep experimenting with it to find an optimal solution.</p> <p>When there is a breakthrough in the application, you can\u2019t simply take decisions based on the available information which may be incomplete or inaccurate. So, with disposable architecture, you start with small increments, and invest time to find the optimal solution. Sometimes, there may be a need to completely replace the component, but that initial work was just the cost of getting the information that caused the breakthrough. This helps you to minimize waste allowing you to use your resources on controlled experiments efficiently and get good value out of it in the end.</p> <p>Value added cloud services</p> <p>When you are defining an application, there are many things you need to care of. Each and every service will be associated with many things like databases, storage, redundancy, monitoring, etc. For your application, along with your components, you also need to scale the data. You can reduce the operational risk and also get all such things at greater velocity by leveraging the value-added services that are available on cloud. Sometimes, you may need third party services if they are not available on your cloud. You can externally hook them up with your application as needed.</p> <p>By using the value added services provided by your cloud provider, you will get to know all the available options on your cloud and you can also learn about all the new services. This will help you to take good long-termed decisions. You can definitely exit the service if you find something more suitable for your component and hook that up with your application based on the requirements.</p> <p>Polyglot cloud</p> <p>Most of you are familiar with Polyglot programming. For your application, based on the component, you can choose the programming languages that best suits it. You need not stick to a single programming language for the entire application. If you consider Polyglot persistence, the idea is choose the storage mechanism that suits better on a component by component basis. It allows a better global scale.</p> <p>Similarly, the next thing will be Polyglot cloud. Like above, here you choose a cloud provider that better suits on a component by component basis. For majority of your components, you may have a go to cloud provider. But, this does not stop you from choosing a different one if it suits well for any of your application components. So, you can run different components of your cloud native system on different cloud providers based on your requirements.</p> <p>Self-sufficient, full-stack teams</p> <p>In a traditional set up, many organizations have teams based on skill set like backend, user interface, database, operations etc. Such a structure will not allow you to build cloud native systems.</p> <p>In cloud native systems, the system is composed of bounded isolated components. They have their own resources. Each of such component must be owned by self-sufficient, full stack team. That team is entirely responsible for all the resources that belong to that particular component. In this set up, team tends to build quality up front in as they are the ones who deploy it and they will be taking care of it if the component is broken. It is more like you build it and then you run it. So, the team can continuously deliver advancements to the components at their own pace. Also, they are completely responsible for delivering it safely.</p> <p>Cultural Change</p> <p>Cloud native is different way of thinking. We need to first make up our minds, not just the systems, to utilize the full benefits of cloud. Compared to the traditional systems, there will be lots of things we do differently in cloud-native systems.</p> <p>To make that happen, cultural change is really important. To change the thinking at high level, we just to first prove that the low level practices can truly deliver and encourage lean thinking. With this practice, you can conduct experimentation. Based on the feedback from business, you can quickly and safely deliver your applications that can scale.</p>"},{"location":"cloud/cn-concepts/#presentations","title":"Presentations","text":"<p>Cloud-Native Presentation </p>"},{"location":"cloud/cn-concepts/#cloud-native-roadmap","title":"Cloud-native Roadmap","text":"<p>You can define your cloud native road map in many ways. You can get there by choosing different paths. Let us see the trail map defined by CNCF.</p> <p>CNCF defined the Cloud Native Trail Map providing an overview for enterprises starting their cloud native journey as follows.</p> <p>This cloud map gives us various steps that an engineering team may use while considering the cloud native technologies and exploring them. The most common ones among them are Containerization, CI/CD, and Orchestration. Next crucial pieces will be Observability &amp; Analysis and Service Mesh. And later comes the rest of them like Networking, Distributed Database, Messaging, Container runtime, and software distribution based on your requirements.</p> <p></p> <ul> <li> <p>With out Containerization, you cannot build cloud native     applications. This helps your application to run in any computing     environment. Basically, all your code and dependencies are packaged     up together in to a single unit here. Among different container     platforms available, Docker is a preferred one.</p> </li> <li> <p>To bring all the changes in the code to container automatically, it     is nice to set up a CI/CD pipeline which does that. There are many     tools available like jenkins, travis, etc.</p> </li> <li> <p>Since we have containers, we need container orchestration to manage     the container lifecycles. Currently, Kubernetes is one solution     which is popular.</p> </li> <li> <p>Monitoring and Observability plays a very important role. It is good     to set up some of them like logging, tracing, metrics etc.</p> </li> <li> <p>To enable more complex operational requirements, you can use a     service mesh. It helps you out with several things like service     discovery, health, routing, A/B testing etc. Istio is one of the     examples of service mesh.</p> </li> <li> <p>Networking plays a crucial role. You should define flexible     networking layers based on your requirements. For this, you can use     Calico, Weave Net etc.</p> </li> <li> <p>Sometimes, you may need distributed databases. Based on your     requirements, if you need more scalability and resiliency, these are     required.</p> </li> <li> <p>Messaging may be required sometimes too. Go with different messaging     queues like Kafka, RabbitMQ etc available when you need them.</p> </li> <li> <p>Container Registry helps you to store all your containers. You can     also enable image scanning and signing if required.</p> </li> <li> <p>As a part of your application, sometimes you may need a secure     software distribution.</p> </li> </ul> <p>Also, if you want to see the cloud native landscape, check it out here.</p>"},{"location":"cloud/cn-concepts/#summary","title":"Summary","text":"<p>In this, we covered the fundamentals of cloud native systems. You now know what cloud native is, why we need it and how it is important. Cloud native is not just deploying your application on cloud but it is more of taking full advantages of cloud. Also, from cloud-native roadmap, you will get an idea on how to design and architect your cloud-native system. You can also get the idea of different tools, frameworks, platforms etc from the cloud-native landscapes.</p> <p>Also, if you are interesting in knowing more, we have Cloud-Native: A Complete Guide. Feel free to check this out.</p>"},{"location":"cloud/cn-concepts/#references","title":"References","text":"<ul> <li> <p>John Gilbert, (2018). Cloud Native Development Patterns and Best     Practices. Publisher: Packt     Publishing</p> </li> <li> <p>CNCF landscape</p> </li> <li> <p>CNCF     Definition</p> </li> </ul>"},{"location":"containers/","title":"What are Containers?","text":"<p>You wanted to run your application on different computing environments. It may be your laptop, test environment, staging environment or production environment.</p> <p>So, when you run it on these different environments, will your application work reliably ?</p> <p>What if some underlying software changes ? What if the security policies are different ? or something else changes ?</p> <p>To solve this problems, we need Containers.</p>"},{"location":"containers/#containers","title":"Containers","text":"<p>Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized.</p> <p>For example, Docker created standard way to create images for Linux Containers.</p>"},{"location":"containers/#presentations","title":"Presentations","text":"<p>Container Basics </p>"},{"location":"containers/#why-containers","title":"Why Containers?","text":"<ul> <li>We can run them anywhere.</li> <li>They are lightweight .</li> <li>Isolate your application from others.</li> </ul>"},{"location":"containers/#different-container-standards","title":"Different Container Standards","text":"<p>There are many different container standards available today. Some of them are as follows.</p> <p>Docker - The most common standard, made Linux containers usable by the masses.</p> <p>Rocket (rkt) - An emerging container standard from CoreOS, the company that developed etcd.</p> <p>Garden - The format Cloud Foundry builds using buildpacks.</p> <p>Among them, Docker was one of the most popular mainstream container software tools.</p> <p>Open Container Initiative (OCI)</p> <p>A Linux Foundation project developing a governed container standard. Docker and Rocket are OCI-compliant. But, Garden is not.</p>"},{"location":"containers/#benefits","title":"Benefits","text":"<ul> <li>Lightweight</li> <li>Scalable</li> <li>Efficient</li> <li>Portable</li> <li>Supports agile development</li> </ul> Containerization Guides <p>To know more about Containerization, we have couple of guides. Feel free to check them out.</p> <ul> <li>Containerization: A Complete Guide.</li> <li>Containers: A Complete Guide.</li> </ul>"},{"location":"containers/#docker","title":"Docker","text":"<p>Docker is one of the most popular Containerization platforms which allows you to develop, deploy, and run application inside containers.</p> <ul> <li>It is an open source project.</li> <li>Can run it anywhere.</li> </ul> <p>An installation of Docker includes an engine. This comes with a daemon, REST APIs, and CLI. Users can use CLI to interact with the docker using commands. These commands are sent to the daemon which listens for the Docker Rest APIs which in turn manages images and containers. The engine runs a container by retrieving its image from the local system or registry. A running container starts one or more processes in the Linux kernel.</p>"},{"location":"containers/#docker-image","title":"Docker Image","text":"<p>A read-only snapshot of a container that is stored in Docker Hub or in private repository. You use an image as a template for building containers.</p> <p>These images are build from the <code>Dockerfile</code>.</p> <p>Dockerfile</p> <ul> <li>It is a text document that contains all the instructions that are necessary to build a docker image.</li> <li>It is written in an easy-to-understand syntax.</li> <li>It specifies the operating system.</li> <li>It also includes things like environmental variables, ports, file locations etc.</li> </ul> Interactive Learning <p>If you want to try building docker images, try this course on O'Reilly (Interactive Learning Platform).</p> <ul> <li>Building Container Images - Estimated Time: 12 minutes.</li> </ul>"},{"location":"containers/#docker-container","title":"Docker Container","text":"<p>The standard unit where the application service is located or transported. It packages up all code and its dependencies so that the application runs quickly and reliably from one computing environment to another.</p> Interactive Learning <p>If you want to try deploying a docker container, try this course on O'Reilly (Interactive Learning Platform).</p> <ul> <li>Deploying a Docker Container</li> </ul>"},{"location":"containers/#docker-engine","title":"Docker Engine","text":"<p>Docker Engine is a program that creates, ships, and runs application containers. The engine runs on any physical or virtual machine or server locally, in private or public cloud. The client communicates with the engine to run commands.</p> Interactive Learning <p>If you want to learn more about docker engines, try this course on O'Reilly (Interactive Learning Platform).</p> <ul> <li>Docker Networking</li> </ul>"},{"location":"containers/#docker-registry","title":"Docker Registry","text":"<p>The registry stores, distributes, and shares container images. It is available in software as a service (SaaS) or in an enterprise to deploy anywhere you that you choose.</p> <p>Docker Hub is a popular registry. It is a registry which allows you to download docker images which are built by different communities. You can also store your own images there. You can check out various images available on docker hub here.</p>"},{"location":"containers/#references","title":"References","text":"<ul> <li>Docker resources</li> <li>Docker tutorial</li> <li>The Evolution of Linux Containers and Their Future</li> <li>Open Container Initiative (OCI)</li> <li>Cloud Native Computing Foundation (CNCF)</li> <li>Demystifying the Open Container Initiative (OCI) Specifications</li> </ul>"},{"location":"containers/benefitsContainers/","title":"Benefits of Containers","text":""},{"location":"containers/benefitsContainers/#consistency-across-environments","title":"Consistency Across Environments","text":"<p>Containers package applications and their dependencies into a single unit, ensuring consistent behavior across different environments - from development to production. This \"build once, run anywhere\" approach eliminates the common \"it works on my machine\" problem and reduces deployment issues caused by environmental differences.</p>"},{"location":"containers/benefitsContainers/#isolation-and-resource-efficiency","title":"Isolation and Resource Efficiency","text":"<p>Unlike traditional virtual machines, containers share the host operating system's kernel while maintaining isolation between applications. This lightweight approach means containers start faster, use fewer resources, and allow higher density deployment on the same infrastructure. Multiple containers can run efficiently on a single host without interfering with each other.</p>"},{"location":"containers/benefitsContainers/#rapid-development-and-deployment","title":"Rapid Development and Deployment","text":"<p>Containers enable faster application development and deployment cycles. Developers can quickly create standardized development environments, test new features in isolation, and ship updates with confidence. Container images can be versioned, rolled back easily, and shared across teams, streamlining the continuous integration and deployment (CI/CD) pipeline.</p>"},{"location":"containers/benefitsContainers/#microservices-architecture-support","title":"Microservices Architecture Support","text":"<p>Containers are ideal for microservices architecture, allowing complex applications to be broken down into smaller, independently deployable services. Each service can be developed, updated, and scaled independently, making it easier to maintain and evolve large applications over time.</p>"},{"location":"containers/benefitsContainers/#portability","title":"Portability","text":"<p>Container images are platform-independent and can run on any system that supports container runtime - whether it's a developer's laptop, an on-premises server, or any cloud provider. This portability eliminates vendor lock-in and provides flexibility in choosing deployment environments.</p>"},{"location":"containers/benefitsContainers/#version-control-and-image-management","title":"Version Control and Image Management","text":"<p>Container images can be versioned like source code, making it easy to track changes, roll back to previous versions, and maintain different variants of the same application. Container registries provide centralized storage and distribution of container images, simplifying collaboration and deployment.</p>"},{"location":"containers/benefitsContainers/#auto-scaling-and-high-availability","title":"Auto-scaling and High Availability","text":"<p>Container orchestration platforms like Kubernetes enable automatic scaling of applications based on demand. They also provide built-in features for high availability, load balancing, and self-healing, ensuring applications remain responsive and reliable under varying loads.</p>"},{"location":"containers/benefitsContainers/#resource-optimization","title":"Resource Optimization","text":"<p>Containers allow for better resource utilization through fine-grained control over CPU, memory, and storage allocation. This precise control helps optimize infrastructure costs and improve application performance by ensuring resources are used efficiently.</p>"},{"location":"containers/benefitsContainers/#security-benefits","title":"Security Benefits","text":"<p>Containers provide security through isolation, reducing the attack surface of applications. Container images can be scanned for vulnerabilities, signed for authenticity, and updated quickly when security patches are needed. Role-based access control and network policies can be applied at the container level for enhanced security management.</p>"},{"location":"containers/imageregistry/","title":"What are Image Registries","text":"<p>A registry is a repository used to store and access container images. Container registries can support container-based application development, often as part of DevOps processes.</p> <p>Container registries save developers valuable time in the creation and delivery of cloud-native applications, acting as the intermediary for sharing container images between systems. They essentially act as a place for developers to store container images and share them out via a process of uploading (pushing) to the registry and downloading (pulling) into another system, like a Kubernetes cluster.</p> <p>Some examples of an image registry are Red Hat Quay and IBM Cloud Registry.</p> <p>Learn More </p>"},{"location":"containers/imageregistry/#quay-tutorial","title":"Quay Tutorial","text":"PodmanDocker <p>Make sure you have Podman Desktop installed and up and running.</p> <p>Here's how to find a list of publicly available container images on DockerHub.</p> Find Container Image<pre><code>podman search docker.io/busybox\n</code></pre> Output:<pre><code>NAME                                         DESCRIPTION\ndocker.io/library/busybox                    Busybox base image.\ndocker.io/rancher/busybox\ndocker.io/openebs/busybox-client\ndocker.io/antrea/busybox\ndocker.io/hugegraph/busybox                  test image\n...\n</code></pre> <p>We can create a busybox container image based off of the busybox base image you see listed in the output above.</p> Create Image<pre><code>podman run -it docker.io/library/busybox\n</code></pre> <p>The -it flag allows you to run the image in interactive mode. Interactive mode in Podman allows you to run a shell in a container and interact with it. However, you'll see that running small containers like this one don't have much to play around with in the interactive mode. To exit out of the interactive mode:</p> Exit Interactive mode<pre><code>exit\n</code></pre> <p>You can also share images in a public registry so that others can use and review them. Here's how to push your image up to quay.io.</p> <p>Login to quay.io<pre><code>podman login quay.io\n</code></pre>   Enter in the following info:</p> <pre><code>Username: your_username\nPassword: your_password\n</code></pre> <p>Next, tag the image so that you can push it and find it in your account.</p> Tag Image<pre><code>podman tag &lt;image_name&gt; quay.io/your_username/image_registry_name\n</code></pre> <p>Make sure to replace \"image_name\" with the name of the image you want to push up to quay.   Replace \"user_name\" with your quay.io username.   Replace \"image_registry_name\" with what you want the image to be named/labeled as in quay.</p> <p>Once the image is tagged, you can push it up to quay.</p> Push Image<pre><code>podman push quay.io/your_username/image_registry_name\n</code></pre> <p>Your respository has now been pushed to Quay Container Registry!</p> <p>To view your repository, click on the button below:</p> <p>Repositories</p> <p>Make sure you have Docker Desktop installed and up and running.</p> Login to Quay<pre><code>docker login quay.io\nUsername: your_username\nPassword: your_password\nEmail: your_email\n</code></pre> <p>First we'll create a container with a single new file based off of the busybox base image:   Create a new container<pre><code>docker run busybox echo \"fun\" &gt; newfile\n</code></pre>   The container will immediately terminate, so we'll use the command below to list it:   <pre><code>docker ps -l\n</code></pre>   The next step is to commit the container to an image and then tag that image with a relevant name so it can be saved to a respository.</p> <p>Replace \"container_id\" with your container id from the previous command.   Create a new image<pre><code>docker commit container_id quay.io/your_username/repository_name\n</code></pre>   Be sure to replace \"your_username\" with your quay.io username and \"respository_name\" with a unique name for your repository.</p> <p>Now that we've tagged our image with a repository name, we can push the respository to Quay Container Registry:   Push the image to Quay<pre><code>docker push quay.io/your_username/repository_name\n</code></pre>   Your respository has now been pushed to Quay Container Registry!</p> <p>To view your repository, click on the button below:</p> <p>Repositories</p>"},{"location":"containers/imageregistry/#ibm-cloud-registry-tutorial","title":"IBM Cloud Registry Tutorial","text":"PodmanDocker <p>1. Install the Container Registry CLI</p> <p>Before you begin, you need to install the IBM Cloud CLI so that you can run the IBM Cloud ibmcloud commands.   Install the container-registry CLI<pre><code>ibmcloud plugin install container-registry\n</code></pre> 2. Set up a namespace</p> <p>Then, you need to create a namespace. The namespace is created in the resource group that you specify so that you can configure access to resources within the namespace at the resource group level. If you don't specify a resource group, then the default is used.   Log in to IBM Cloud<pre><code>ibmcloud login\n</code></pre></p> <p>Create namespace<pre><code>ibmcloud cr namespace-add &lt;my_namespace&gt;\n</code></pre>   Make sure to replace &lt;my_namespace&gt; with your preferred namespace.</p> <p>If you want to create the namespace in a specific resource group, use the following code before creating the namespace.   Specify a resource group<pre><code>ibmcloud target -g &lt;resource_group&gt;\n</code></pre>   Replace &lt;resource_group&gt; with the resource group you want to create the namespace in.</p> <p>To validate the namespace was created, run the following command.   Validate namespace is created<pre><code>ibmcloud cr namespace-list -v\n</code></pre></p> <p>3. Pull images from a registry to your local computer</p> <p>Next, you can pull images from IBM Cloud Registry to your local computer. Make sure Podman is installed and up and running.</p> <p>Pull image to local computer<pre><code>podman pull &lt;source_image&gt;:&lt;tag&gt;\n</code></pre>   Replace &lt;source_image&gt; with the respository of the image and &lt;tag&gt; with the tag of the image that you want to use.</p> <p>Below is an example where &lt;source_image&gt; is \"hello-world\" and &lt;tag&gt; is \"latest\".</p> <pre><code>podman pull hello-world:latest\n</code></pre> <p>4. Tag the image</p> <p>Tags are used as an optional identifier to specify a particular version of an image.</p> <p>Tag image<pre><code>podman tag &lt;source_image&gt;:&lt;tag&gt; &lt;region&gt;.icr.io/&lt;my_namespace&gt;/&lt;new_image_repo&gt;:&lt;new_tag&gt;\n</code></pre>   Replace &lt;source_image&gt; with the respository of the image, &lt;tag&gt; with the tag of your local image that you previously pulled, &lt;region&gt; with the name of your region, and &lt;my_namespace&gt; with the namespace you created in step 2. You'll want to define the repository and tag of the image that you want to use in your namespace by replacing &lt;new_image_repo&gt; and &lt;new_tag&gt; respectively.</p> <p>Below is an example where &lt;source_image&gt; is \"hello-world\", &lt;tag&gt; is \"latest\", &lt;region&gt; is \"uk\", &lt;my_namespace&gt; is \"namespace1\", &lt;new_image_repo&gt; is \"hw_repo\", and &lt;new_tag&gt; is \"1\".</p> <pre><code>podman tag hello-word:latest uk.icr.io/namespace1/hw_repo:1\n</code></pre> <p>5. Push images to your namespace</p> <p>First, you'll need to log in to IBM Cloud Container Registry.</p> Log in to ICR<pre><code>ibmcloud cr login --client podman\n</code></pre> <p>Once you've logged in, you can push the image up to your namespace in the registry.</p> Push image to your namespace<pre><code>podman push &lt;region&gt;.icr.io/&lt;my_namespace&gt;/&lt;image_repo&gt;:&lt;tag&gt;\n</code></pre> <p>Replace &lt;my_namespace&gt; with the namespace you created in step 2 and &lt;image_repo&gt; and &lt;tag&gt; with the repository and tag of the image you chose when you tagged the image in step 4.</p> <p>Below is an example where &lt;region&gt; is \"uk\", &lt;my_namespace&gt; is \"namespace1\", &lt;image_repo&gt; is \"hw_repo\", and &lt;tag&gt; is \"1\".</p> <pre><code>podman push uk.icr.io/namespace1/hw_repo:1\n</code></pre> <p>6. Verify that the image was pushed</p> <p>Verify that the image was pushed successfully by running the comand below.</p> <pre><code>ibmcloud cr image-list\n</code></pre> <p>You can also view your pushed images by clicking on the button below:</p> <p>Images</p> <p>1. Install the Container Registry CLI</p> <p>Before you begin, you need to install the IBM Cloud CLI so that you can run the IBM Cloud ibmcloud commands.   Install the container-registry CLI<pre><code>ibmcloud plugin install container-registry\n</code></pre></p> <p>2. Set up a namespace</p> <p>Then, you need to create a namespace. The namespace is created in the resource group that you specify so that you can configure access to resources within the namespace at the resource group level. If you don't specify a resource group, then the default is used.   Log in to IBM Cloud<pre><code>ibmcloud login\n</code></pre></p> <p>Create namespace<pre><code>ibmcloud cr namespace-add &lt;my_namespace&gt;\n</code></pre>   Make sure to replace &lt;my_namespace&gt; with your preferred namespace.</p> <p>If you want to create the namespace in a specific resource group, use the following code before creating the namespace.   Specify a resource group<pre><code>ibmcloud target -g &lt;resource_group&gt;\n</code></pre>   Replace &lt;resource_group&gt; with the resource group you want to create the namespace in.</p> <p>To validate the namespace was created, run the following command.   Validate namespace is created<pre><code>ibmcloud cr namespace-list -v\n</code></pre></p> <p>3. Pull images from a registry to your local computer</p> <p>Next, you can pull images from IBM Cloud Registry to your local computer. Make sure Docker is installed and up and running.</p> <p>Pull image to local computer<pre><code>docker pull &lt;source_image&gt;:&lt;tag&gt;\n</code></pre>   Replace &lt;source_image&gt; with the respository of the image and &lt;tag&gt; with the tag of the image that you want to use.</p> <p>Below is an example where &lt;source_image&gt; is \"hello-world\" and &lt;tag&gt; is \"latest\".</p> <pre><code>docker pull hello-world:latest\n</code></pre> <p>4. Tag the image</p> <p>Tags are used as an optional identifier to specify a particular version of an image.</p> <p>Tag image<pre><code>docker tag &lt;source_image&gt;:&lt;tag&gt; &lt;region&gt;.icr.io/&lt;my_namespace&gt;/&lt;new_image_repo&gt;:&lt;new_tag&gt;\n</code></pre>   Replace &lt;source_image&gt; with the respository of the image, &lt;tag&gt; with the tag of your local image that you previously pulled, &lt;region&gt; with the name of your region, and &lt;my_namespace&gt; with the namespace you created in step 2. You'll want to define the repository and tag of the image that you want to use in your namespace by replacing &lt;new_image_repo&gt; and &lt;new_tag&gt; respectively.</p> <p>Below is an example where &lt;source_image&gt; is \"hello-world\", &lt;tag&gt; is \"latest\", &lt;region&gt; is \"uk\", &lt;my_namespace&gt; is \"namespace1\", &lt;new_image_repo&gt; is \"hw_repo\", and &lt;new_tag&gt; is \"1\".</p> <pre><code>docker tag hello-word:latest uk.icr.io/namespace1/hw_repo:1\n</code></pre> <p>5. Push images to your namespace</p> <p>First, you'll need to log in to IBM Cloud Container Registry.</p> Log in to ICR<pre><code>ibmcloud cr login --client docker\n</code></pre> <p>Once you've logged in, you can push the image up to your namespace in the registry.</p> Push image to your namespace<pre><code>docker push &lt;region&gt;.icr.io/&lt;my_namespace&gt;/&lt;image_repo&gt;:&lt;tag&gt;\n</code></pre> <p>Replace &lt;my_namespace&gt; with the namespace you created in step 2 and &lt;image_repo&gt; and &lt;tag&gt; with the repository and tag of the image you chose when you tagged the image in step 4.</p> <p>Below is an example where &lt;region&gt; is \"uk\", &lt;my_namespace&gt; is \"namespace1\", &lt;image_repo&gt; is \"hw_repo\", and &lt;tag&gt; is \"1\".</p> <pre><code>docker push uk.icr.io/namespace1/hw_repo:1\n</code></pre> <p>6. Verify that the image was pushed</p> <p>Verify that the image was pushed successfully by running the comand below.</p> <pre><code>ibmcloud cr image-list\n</code></pre> <p>You can also view your pushed images by clicking on the button below:</p> <p>Images</p> <p>Learn More about Quay  Learn More about ICR </p>"},{"location":"containers/reference/","title":"Container References","text":""},{"location":"containers/reference/#basic-cli-commands","title":"Basic CLI Commands","text":"PodmanDocker Action Command Check Podman version <code>podman --version</code> Check host information <code>podman info</code> List images <code>podman images</code> Pull image <code>podman pull &lt;container-name/container-id&gt;</code> Run image <code>podman run &lt;container-name/container-id&gt;</code> List running containers <code>podman ps</code> Inspect a container <code>podman inspect &lt;container-id&gt;</code> Stop a running container <code>podman stop &lt;container-id&gt;</code> Kill a running container <code>podman kill &lt;container-id\"</code> Build Podman image <code>podman build -t &lt;image_name&gt;:&lt;tag&gt; -f containerfile .</code> Push image <code>podman push &lt;image_name&gt;:&lt;tag&gt;</code> Remove stopped containers <code>podman rm &lt;container-id&gt;</code> Remove image <code>podman rmi -f &lt;image-name&gt;</code> or <code>podman rmi -f &lt;image-id&gt;</code> Action Command Get Docker version <code>docker version</code> Run <code>hello-world</code> Container <code>docker run hello-world</code> List Running Containers <code>docker ps</code> Stop a container <code>docker stop &lt;container-name/container-id&gt;</code> List Docker Images <code>docker images</code> Login into registry <code>docker login</code> Build an image <code>docker build -t &lt;image_name&gt;:&lt;tag&gt; .</code> Inspect a docker object <code>docker inspect &lt;name/id&gt;</code> Inspect a docker image <code>docker inspect image &lt;name/id&gt;</code> Pull an image <code>docker pull &lt;image_name&gt;:&lt;tag&gt;</code> Push an Image <code>docker push &lt;image_name&gt;:&lt;tag&gt;</code> Remove a container <code>docker rm &lt;container-name/container-id&gt;</code>"},{"location":"containers/reference/#running-the-cli","title":"Running the CLI","text":"Local PodmanLocal DockerIBM Cloud <ol> <li> <p>a. RECOMMENDED: Download Podman here</p> <p>b. NOT RECOMMENDED: Brew Installation <pre><code>brew install podman\n</code></pre></p> </li> <li> <p>Create and start your first podman machine <pre><code>podman machine init\npodman machine start\n</code></pre></p> </li> <li> <p>Verify Podman installation <pre><code>podman info\n</code></pre></p> </li> <li> <p>Test it out: Podman Introduction</p> </li> </ol> <ol> <li> <p>Install Docker Desktop here</p> </li> <li> <p>Test it out: Getting Started with Docker</p> </li> </ol> <ol> <li> <p>Install ibmcloud CLI <pre><code>curl -fsSL https://clis.cloud.ibm.com/install/osx | sh\n</code></pre></p> </li> <li> <p>Verify installation <pre><code>ibmcloud help\n</code></pre></p> </li> <li> <p>Configure environment. Go to cloud.ibm.com -&gt; click on your profile -&gt; Log into CLI and API and copy IBM Cloud CLI command. It will look something like this: <pre><code>ibmcloud login -a https://cloud.ibm.com -u passcode -p &lt;password&gt;\n</code></pre></p> </li> <li> <p>Log into docker through IBM Cloud <pre><code>ibmcloud cr login --client docker\n</code></pre></p> </li> </ol>"},{"location":"containers/reference/#activities","title":"Activities","text":"Task Description Link Time IBM Container Registry Build and Deploy Run using IBM Container Registry IBM Container Registry 30 min Docker Lab Running a Sample Application on Docker Docker Lab 30 min <p>Once you have completed these tasks, you should have a base understanding of containers and how to use Docker.</p>"},{"location":"devops/","title":"What is DevOps?","text":"<p>DevOps has recently become a popular buzzword in the Cloud World. It varies from business to business and it means a lot different things to different people. In traditional IT, organizations have separate teams for Development and Operations. The development team is responsible for coding and operations team is responsible for releasing it to production. When it comes to this two different teams, there will always be some sort of differences. It may be due to the usage of different system environments, software libraries etc. In order to level this up, DevOps came into play.</p> <p>\u201cDevOps is a philosophy, a cultural shift that merges operations with development and demands a linked toolchain of technologies to facilitate collaborative change. DevOps toolchains \u2026 can include dozens of non-collaborative tools, making the task of automation a technically complex and arduous one.\u201d - Gartner</p> <p></p> <p>These days every business has critical applications which can never go down. Some of the examples are as follows.</p> <p></p> <p>In order to make sure that these applications are up and running smoothly, we need DevOps.</p> <p>Adopting DevOps allows enterprises to create, maintain and improve their applications at a faster pace than the traditional methods. Today, most of the global organizations adopted DevOps.</p>"},{"location":"devops/#presentations","title":"Presentations","text":"<p>Tekton Overview </p> <p>GitOps Overview </p>"},{"location":"devops/#benefits-of-devops","title":"Benefits of DevOps","text":"<ul> <li>Continuous software delivery</li> <li>High quality software</li> <li>Increased speed and faster problem resolution</li> <li>Increased reliability</li> <li>Easier to manage the software</li> <li>Collaboration and enhanced team communication</li> <li>Customer satisfaction etc.</li> </ul>"},{"location":"devops/#understanding-devops","title":"Understanding DevOps","text":"<p>Like we mentioned before, often development teams and operation teams are in conflict with each other. Developers keeping changing the software to include new features where as operation engineers wants to keep the system stable.</p> <ul> <li>Their goals are different.</li> <li>They use different processes.</li> <li>They use different tools.</li> </ul> <p>All these may be different reasons for the gap between these two teams.</p> <p>To solve this gap between the two teams, we need DevOps. It closes the gap by aligning incentives and sharing approaches for tools and processes. It helps us to streamline the software delivery process. From the time we begin the project till its delivery, it helps us to improve the cycle time by emphasizing the learning by gathering feedback from production to development.</p> <p>It includes several aspects like the below.</p> <ul> <li>Automation - It is quite essential for DevOps. It helps us to gather quick feedback.</li> <li>Culture - Processes and tools are important. But, people are always more important.</li> <li>Measurement - Shared incentives are important. Quality is critical.</li> <li>Sharing - Need a Culture where people can share ideas, processes and tools.</li> </ul> <p> </p>"},{"location":"devops/#where-to-start","title":"Where to start ?","text":"<p>Understanding the eco system of your software is important. Identify all the environments like dev, test, prod etc. you have in your system and how the delivery happens from end to end.</p> <ul> <li>Define continuous delivery</li> <li>Establish proper collaboration between teams</li> <li>Make sure the teams are on same pace</li> <li>Identify the pain points in your system and start working on them.</li> </ul>"},{"location":"devops/#devops-best-practices","title":"DevOps Best Practices","text":"<p>These are some of the standard practices adopted in DevOps.</p> <ul> <li>Source Code Management</li> <li>Code Review</li> <li>Configuration Management</li> <li>Build Management</li> <li>Artifact Repository Management</li> <li>Release Management</li> <li>Test Automation</li> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Continuous Deployment</li> <li>Infrastructure As Code</li> <li>Automation</li> <li>Key Application Performance Monitoring/Indicators</li> </ul> <p>Source Code Management</p> <p>Source Code Management (SCM) systems helps to maintain the code base. It allows multiple developers to work on the code concurrently. It prevents them from overwriting the code and helps them to work in parallel from different locations.</p> <p>Collaboration is an important concept in devOps and SCM helps us to achieve it by coordination of services across the development team. It also tracks co-authoring, collaboration, and individual contributions. It helps the developers to audit the code changes. It also allows rollbacks if required. It also enables backup and allows recovery when required.</p> <p>Code Review</p> <p>Code reviews allows the developer to improve the quality of code. They help us to identify the problems in advance. By reviewing the code, we can fix some of the problems like memory leaks, buffer overflow, formatting errors etc.</p> <p>This process improves the collaboration across the team. Also, code defects are identified and removed before merging them with the main stream there by improving the quality of the code.</p> <p>Configuration Management</p> <p>Configuration Management is managing the configurations by identifying, verifying, and maintaining them. This is done for both software and hardware. The configuration management tools make sure that configurations are properly configured across different systems as per the requirements.</p> <p>This helps to analyze the impact on the systems due to configurations. It makes sure the provisioning is done correctly on different systems like dev, QA, prod etc. It simplifies the coordination between development and operations teams.</p> <p>Build Management</p> <p>Build Management helps to assmble the build environment by packaging all the required components such as the source code, dependencies, etc of the software application together in to a workable unit. Builds can be done manually, on-demand or automated.</p> <p>It ensures that the software is stable and it is reusable. It improves the quality of the software and makes sure it is reliable. It also increases the efficiency.</p> <p>Artifact Repository Management</p> <p>Artifact Repository Management system is used to manage the builds. It is dedicated server which is used to store all the binaries which were outputs of the successful builds.</p> <p>It manages the life cycles of different artifacts. It helps you to easily share the builds across the team. It controls access to the build artifacts by access control.</p> <p>Release Management</p> <p>Release management is a part of software development lifecycle which manages the release from development till deployment to support. Requests keep coming for the addition of the new features. Also, sometimes there may be need to change the existing functionality. This is when the cycle begins for the release management. Once, the new feature or change is approved, it is designed, built, tested, reviewed, and after acceptance, deployed to production. After this, it goes to maintainence and even at this point, there may be need for enhancement. If that is the case, it will be a new cycle again.</p> <p>It helps us to track all the phases and status of deployments in different environments.</p> <p>Test Automation</p> <p>Manual testing takes lots of time. We can automate some of the manual tests which are repetitive, time consuming, and have defined input by test automation.</p> <p>Automatic tests helps to improve the code quality, reduces the amount of time spent on testing, and improves the effectiveness of the overall testing life cycle.</p> <p>Continuous Integration</p> <p>Continuous integration allows the developers to continuously integrate the code they developed. Whenever a latest code change is made and committed to the source control system, the source code is rebuilt and this is then forwarded to testing.</p> <p>With this, the latest code is always available, the builds are faster and the tests are quick.</p> <p>Continuous Delivery</p> <p>Continuous Delivery is the next step to Continuous Integration. In the integration, the code is built and tested. Now in the delivery, this is taken to staging environment. This is done in small frequencies and it makes sure the functionality of the software is stable.</p> <p>It reduces the manual overhead. The code is continuously delivered and constantly reviewed.</p> <p>Continuous Deployment</p> <p>Continuous Deployment comes after Continuous Delivery. In the deployment stage, the code is deployed to the production environment. The entire process is automated in this stage.</p> <p>This allows faster software releases. Improves the collaboration across the teams. Enhances the code quality.</p> <p>Infrastructure As Code</p> <p>Infrastructure as Code is defining the infrastructure services as a software code. they are defines as configuration files. Traditionally, in on-premise application, these are run by system administrators but in cloud, the infrastructure is maintained like any other software code.</p> <p>Helps us to change the system configuration quickly. Tracking is easy and end to end testing is possible. Infrastructure availability is high.</p> <p>Automation</p> <p>Automation is key part to DevOps. Without automation, DevOps is not efficient.</p> <p>Automation comes into play whenever there is a repetitive task. Developers can automate infrastructure, applications, load balancers, etc.</p> <p>Key Application Performance Monitoring/Indicators</p> <p>DevOps is all about measuring the metrics and feedback, with continuous improvement processes. Collecting metrics and monitoring the software plays an important role. Different measures like uptime versus downtime, resolutions time lines etc. helps us to understand the performance of the system.</p>"},{"location":"devops/#devops-in-twelve-factor-apps","title":"Devops in Twelve factor apps","text":"<p>If you are new to Twelve factor methodology, have a look here. For more details, checkout Cloud-Native module.</p>"},{"location":"devops/#devops-reference-architecture","title":"DevOps Reference Architecture","text":"<ol> <li>Collaboration tools enable a culture of innovation. Developers, designers, operations teams, and managers must communicate constantly. Development and operations tools must be integrated to post updates and alerts as new builds are completed and deployed and as performance is monitored. The team can discuss the alerts as a group in the context of the tool.</li> <li>As the team brainstorms ideas, responds to feedback and metrics, and fixes defects, team members create work items and rank them in the backlog. The team work on items from the top of the backlog, delivering to production as they complete work.</li> <li>Developers write source code in a code editor to implement the architecture. They construct, change, and correct applications by using various coding models and tools.</li> <li>Developers manage the versions and configuration of assets, merge changes, and manage the integration of changes. The source control tool that a team uses should support social coding.</li> <li>Developers compile, package, and prepare software assets. They need tools that can assess the quality of the code that is being delivered to source control. Those assessments are done before delivery, are associated with automated build systems, and include practices such as code reviews, unit tests, code quality scans, and security scans.</li> <li>Binary files and other output from the build are sent to and managed in a build artifact repository.</li> <li>The release is scheduled. The team needs tools that support release communication and managing, preparing, and deploying releases.</li> <li>The team coordinates the manual and automated processes that are required for the solution to operate effectively. The team must strive towards continuous delivery with zero downtime. A/B deployments can help to gauge the effectiveness of new changes.</li> <li>The team must understand the application and the options for the application's runtime environment, security, management, and release requirements.</li> <li>Depending on the application requirements, some or all of the application stack must be considered, including middleware, the operating system, and virtual machines.</li> <li>The team must ensure that all aspects of the application and its supporting infrastructure are secured.</li> <li>The team plans, configures, monitors, defines criteria, and reports on application availability and performance. Predictive analytics can indicate problems before they occur.</li> <li>The right people on the team or systems are notified when issues occur.</li> <li>The team manages the process for responding to operations incidents, and delivers the changes to fix any incidents.</li> <li>The team uses analytics to learn how users interact with the application and measure success through metrics.</li> <li>When users interact with the application, they can provide feedback on their requirements and how the application is meeting them, which is captured by analytics as well.</li> <li>DevOps engineers manage the entire application lifecycle while they respond to feedback and analytics from the running application.</li> <li>The enterprise network is protected by a firewall and must be accessed through transformation and connectivity services and secure messaging services.</li> <li>The security team uses the user directory throughout the flow. The directory contains information about the user accounts for the enterprise.</li> </ol> <p>For a cloud native implementation, the reference architecture will be as follows.</p> <p></p>"},{"location":"devops/devopsTools/","title":"DevOps Tools","text":""},{"location":"devops/gitops/","title":"What is GitOps","text":""},{"location":"devops/gitops/#benefits-of-gitops","title":"Benefits of GitOps","text":""},{"location":"devops/argocd/","title":"Continuous Deployment","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/argocd/#what-is-gitops","title":"What is GitOps?","text":"<p>GitOps in short is a set of practices to use Git pull requests to manage infrastructure and application configurations. Git repository in GitOps is considered the only source of truth and contains the entire state of the system so that the trail of changes to the system state are visible and auditable.</p> <ul> <li>Traceability of changes in GitOps is no novelty in itself as this approach is almost universally employed for the application source code. However GitOps advocates applying the same principles (reviews, pull requests, tagging, etc) to infrastructure and application configuration so that teams can benefit from the same assurance as they do for the application source code.</li> <li>Although there is no precise definition or agreed upon set of rules, the following principles are an approximation of what constitutes a GitOps practice:</li> <li>Declarative description of the system is stored in Git (configs, monitoring, etc)</li> <li>Changes to the state are made via pull requests</li> <li>Git push reconciled with the state of the running system with the state in the Git repository</li> </ul>"},{"location":"devops/argocd/#argocd-overview","title":"ArgoCD Overview","text":""},{"location":"devops/argocd/#presentations","title":"Presentations","text":"<p>GitOps Overview </p>"},{"location":"devops/argocd/#activities","title":"Activities","text":"<p>These activities give you a chance to walkthrough building CD pipelines using ArgoCD.</p> <p>These tasks assume that you have:  - Reviewed the Continuous Deployment concept page.</p> Task Description Link Time Walkthroughs GitOps Introduction to GitOps with OpenShift Learn OpenShift GitOps 20 min Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD 30 min <p>Once you have completed these tasks, you will have created an ArgoCD deployment and have an understanding of Continuous Deployment.</p>"},{"location":"devops/ibm-toolchain/","title":"IBM ToolChain","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"devops/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     <pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.</p> </li> </ol>"},{"location":"devops/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"devops/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"devops/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"devops/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"devops/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"devops/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"devops/tekton/","title":"Continuous Integration","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/tekton/#tekton-overview","title":"Tekton Overview","text":"<p>Tekton is a cloud-native solution for building CI/CD systems. It consists of Tekton Pipelines, which provides the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete ecosystem.</p>"},{"location":"devops/tekton/#presentations","title":"Presentations","text":"<p>Tekton Overview  IBM Cloud DevOps with Tekton </p>"},{"location":"devops/tekton/#activities","title":"Activities","text":"<p>The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment.</p> <p>These tasks assume that you have:</p> <ul> <li>Reviewed the continuous integration concept page.</li> <li>Installed Tekton into your cluster.</li> </ul> Task Description Link Time Walkthroughs Deploying Applications From Source Using OpenShift 4 S2I 30 min Try It Yourself Tekton Lab Using Tekton to build container images Tekton 1 hour IBM Cloud DevOps Using IBM Cloud ToolChain with Tekton Tekton on IBM Cloud 1 hour Jenkins Lab Using Jenkins to build and deploy applications. Jenkins 1 hour <p>Once you have completed these tasks, you will have an understanding of continuous integration and how to use Tekton to build a pipeline.</p>"},{"location":"labs/","title":"Labs","text":""},{"location":"labs/#containers","title":"Containers","text":"Task Description Link Try It Yourself IBM Container Registry Build and Deploy Run using IBM Container Registry IBM Container Registry Docker Lab Running a Sample Application on Docker Docker Lab"},{"location":"labs/#kubernetes","title":"Kubernetes","text":"Task Description Link Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation Probes Create some Health &amp; Startup Probes to find what's causing an issue. Probes Debugging Find which service is breaking in your cluster and find out why. Debugging Multiple Containers Build a container using legacy container image. Multiple Containers Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Using Tekton to test new versions of applications. Crons Jobs Creating Services Create two services with certain requirements. Setting up Services Network Policies Create a policy to allow client pods with labels to access secure pod. Network Policies IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress Solutions Lab Solutions Solutions for the Kubernetes Labs Solutions"},{"location":"labs/#devops","title":"DevOps","text":"Task Description Link Walkthroughs Deploying Applications From Source Using OpenShift 4 S2I Try It Yourself Tekton Lab Using Tekton to test new versions of applications. Tekton IBM Cloud DevOps Using IBM Cloud ToolChain with Tekton Tekton on IBM Cloud Jenkins Lab Using Jenkins to test new versions of applications. Jenkins"},{"location":"labs/#gitops","title":"GitOps","text":"Task Description Link Walkthroughs GitOps Introduction to GitOps with OpenShift Learn OpenShift Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD"},{"location":"labs/#projects","title":"Projects","text":"Task Description Link Try It Yourself Cloud Native Challenge Deploy your own app using what we have learned CN Challenge"},{"location":"labs/containers/","title":"Containers Lab","text":"PodmanDocker"},{"location":"labs/containers/#introduction","title":"Introduction","text":"<p>In this lab you will learn how to use podman.</p>"},{"location":"labs/containers/#introduction_1","title":"Introduction","text":"<p>In this lab, you will learn about how to use docker and how to run applications using docker. This lab will not explicitly give you the commands to progress through these exercises, but will show you a similar expected output.</p> <p>It's your goal to create the commands needed (shown as &lt; command &gt; at each step) to complete the lab.</p>"},{"location":"labs/containers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a Quay account. This account is needed to push images to a container registry. Follow the tutorial to get familiar with interacting with Quay</li> <li>You need to install Docker in your environment. Follow the instructions here to install it on Mac and here to install it on Windows.</li> </ul>"},{"location":"labs/containers/#working-with-docker","title":"Working with docker","text":"<p>Before proceeding, make sure docker is properly installed on your system.</p> <ol> <li>Please verify your Docker by looking up the version.</li> </ol> <p>If it is installed, you will see a version number something similar to below.</p> <pre><code>$ &lt;command&gt;\nDocker version 19.03.0-beta3, build c55e026\n</code></pre> <p>** Running a hello-world container **</p> <p>Let us start with a <code>hello-world</code> container.</p> <ol> <li>run a <code>hello-world</code> container.</li> </ol> <p>If it is successfully run, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n1b930d010525: Pull complete\nDigest: sha256:41a65640635299bab090f783209c1e3a3f11934cf7756b09cb2f1e02147c6ed8\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre> <p>Since, <code>hello-world</code> image is not existing locally, it is pulled from <code>library/hello-world</code>. But if it is already existing, docker will not pull it every time but rather use the existing one.</p> <p>This image is pulled from https://hub.docker.com/_/hello-world. Docker hub is a repository used to store docker images. Similarly, you can use your own registries to store images. For example, IBM Cloud provides you a container registry.</p> <p>Verifying the hello-world image</p> <ol> <li>Now verify if an image is existing in your system locally.</li> </ol> <p>You will then see something like below.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nhello-world         latest              fce289e99eb9        5 months ago        1.84kB\n</code></pre>"},{"location":"labs/containers/#get-the-sample-application","title":"Get the sample application","text":"<p>To get the sample application, you will need to clone it from github.</p> <pre><code># Clone the sample app\ngit clone https://github.com/ibm-cloud-architecture/cloudnative_sample_app.git\n\n# Go to the project's root folder\ncd cloudnative_sample_app/\n</code></pre>"},{"location":"labs/containers/#run-the-application-on-docker","title":"Run the application on Docker","text":""},{"location":"labs/containers/#build-the-docker-image","title":"Build the docker image","text":"<p>Let's take look at the docker file before building it.</p> <pre><code>FROM maven:3.3-jdk-8 as builder\n\nCOPY . .\nRUN mvn clean install\n\nFROM openliberty/open-liberty:springBoot2-ubi-min as staging\n\nCOPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\nRUN springBootUtility thin \\\n    --sourceAppPath=/config/app.jar \\\n    --targetThinAppPath=/config/dropins/spring/thinClinic.jar \\\n    --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n</code></pre> <ul> <li>Using the <code>FROM</code> instruction, we provide the name and tag of an image that should be used as our base. This must always be the first instruction in the Dockerfile.</li> <li>Using <code>COPY</code> instruction, we copy new contents from the source filesystem to the container filesystem.</li> <li><code>RUN</code> instruction executes the commands.</li> </ul> <p>This Dockerfile leverages multi-stage builds, which lets you create multiple stages in your Dockerfile to do certain tasks.</p> <p>In our case, we have two stages.</p> <ul> <li>The first one uses <code>maven:3.3-jdk-8</code> as its base image to download and build the project and its dependencies using Maven.</li> <li>The second stage uses <code>openliberty/open-liberty:springBoot2-ubi-min</code> as its base image to run the compiled code from the previous stage.</li> </ul> <p>The advantage of using the multi-stage builds approach is that the resulting image only uses the base image of the last stage. Meaning that in our case, we will only end up with the <code>openliberty/open-liberty:springBoot2-ubi-min</code> as our base image, which is much tinier than having an image that has both Maven and the JRE.</p> <p>By using the multi-stage builds approach when it makes sense to use it, you will end up with much lighter and easier to maintain images, which can save you space on your Docker Registry. Also, having tinier images usually means less resource consumption on your worker nodes, which can result cost-savings.</p> <p>Once, you have the docker file ready, the next step is to build it. The <code>build</code> command allows you to build a docker image which you can later run as a container.</p> <ol> <li>Build the docker file with the <code>image_name</code> of <code>greeting</code> and give it a <code>image_tag</code> of <code>v1.0.0</code> and build it using the current context.</li> </ol> <p>You will see something like below:</p> <pre><code>$ &lt;command&gt;\nSending build context to Docker daemon  22.17MB\nStep 1/6 : FROM maven:3.3-jdk-8 as builder\n---&gt; 9997d8483b2f\nStep 2/6 : COPY . .\n---&gt; c198e3e54023\nStep 3/6 : RUN mvn clean install\n---&gt; Running in 24378df7f87b\n[INFO] Scanning for projects...\n.\n.\n.\n[INFO] Installing /target/cloudnativesampleapp-1.0-SNAPSHOT.jar to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.jar\n[INFO] Installing /pom.xml to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.pom\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 44.619 s\n[INFO] Finished at: 2020-04-06T16:07:04+00:00\n[INFO] Final Memory: 38M/385M\n[INFO] ------------------------------------------------------------------------\nRemoving intermediate container 24378df7f87b\n---&gt; cc5620334e1b\nStep 4/6 : FROM openliberty/open-liberty:springBoot2-ubi-min as staging\n---&gt; 021530b0b3cb\nStep 5/6 : COPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\n---&gt; dbc81e5f4691\nStep 6/6 : RUN springBootUtility thin     --sourceAppPath=/config/app.jar     --targetThinAppPath=/config/dropins/spring/thinClinic.jar     --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n---&gt; Running in 8ea80b5863cb\nCreating a thin application from: /config/app.jar\nLibrary cache: /opt/ol/wlp/usr/shared/resources/lib.index.cache\nThin application: /config/dropins/spring/thinClinic.jar\nRemoving intermediate container 8ea80b5863cb\n---&gt; a935a129dcb2\nSuccessfully built a935a129dcb2\nSuccessfully tagged greeting:v1.0.0\n</code></pre> <ol> <li>Next, verify your newly built image</li> </ol> <p>The output will be as follows.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY                           TAG                   IMAGE ID            CREATED             SIZE\ngreeting                             v1.0.0                89bd7032fdee        51 seconds ago      537MB\nopenliberty/open-liberty             springBoot2-ubi-min   bcfcb2c5ce16        6 days ago          480MB\nhello-world                          latest                f9cad508cb4c        5 months ago        1.84kB\n</code></pre>"},{"location":"labs/containers/#run-the-docker-container","title":"Run the docker container","text":"<p>Now let's try running the docker container. Run it with the following parameters:</p> <ol> <li>Expose port <code>9080</code>. Run it in the background in detached mode. Give the container the name of <code>greeting</code>.</li> </ol> <p>Once done, you will have something like below.</p> <pre><code>$ &lt;command&gt;\nbc2dc95a6bd1f51a226b291999da9031f4443096c1462cb3fead3df36613b753\n</code></pre> <p>Also, docker cannot create two containers with the same name. If you try to run the same container having the same name again, you will see something like below.</p> <pre><code>$ &lt;command&gt;\ndocker: Error response from daemon: Conflict. The container name \"/greeting\" is already in use by container \"a74b91789b29af6e7be92b30d0e68eef852bfb24336a44ef1485bb58becbd664\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</code></pre> <p>It is a good practice to name your containers. Naming helps you to discover your service easily.</p> <ol> <li>List all the running containers.</li> </ol> <p>You will see something like below.</p> <pre><code>$ &lt;command&gt;\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                              NAMES\nbc2dc95a6bd1        greeting:v1.0.0     \"/opt/ol/helpers/run\u2026\"   18 minutes ago      Up 18 minutes       0.0.0.0:9080-&gt;9080/tcp, 9443/tcp   greeting\n</code></pre> <ol> <li>Let's inspect the running container.</li> </ol> <p>By inspecting the container, you can access detailed information about the container. By using this command, you get to know the details about network settings, volumes, configs, state etc.</p> <p>If we consider our container, it is as follows. You can see lot of information about the <code>greeting</code> container.</p> <pre><code>$ &lt;command&gt;\n[\n    {\n        \"Id\": \"bc2dc95a6bd1f51a226b291999da9031f4443096c1462cb3fead3df36613b753\",\n        \"Created\": \"2019-08-30T16:56:40.2081539Z\",\n        \"Path\": \"/opt/ol/helpers/runtime/docker-server.sh\",\n        \"Args\": [\n            \"/opt/ol/wlp/bin/server\",\n            \"run\",\n            \"defaultServer\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 27548,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2019-08-30T16:56:41.0927889Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        ..........\n        ..........\n        ..........\n    }\n]\n</code></pre> <ol> <li>Get the logs of the <code>greeting</code> container.</li> </ol> <p>It helps you to access the logs of your container. It allows you to debug the container if it fails. It also lets you to know what is happening with your application.</p> <p>At the end, you will see something like below.</p> <pre><code>.   ____          _            __ _ _\n/\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n\\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n'  |____| .__|_| |_|_| |_\\__, | / / / /\n=========|_|==============|___/=/_/_/_/\n:: Spring Boot ::        (v2.1.7.RELEASE)\n2019-08-30 16:57:01.494  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Starting SBApplication on bc2dc95a6bd1 with PID 1 (/opt/ol/wlp/usr/servers/defaultServer/dropins/spring/thinClinic.jar started by default in /opt/ol/wlp/output/defaultServer)\n2019-08-30 16:57:01.601  INFO 1 --- [ecutor-thread-5] application.SBApplication                : No active profile set, falling back to default profiles: default\n[AUDIT   ] CWWKT0016I: Web application available (default_host): http://bc2dc95a6bd1:9080/\n2019-08-30 16:57:09.641  INFO 1 --- [cutor-thread-25] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 7672 ms\n2019-08-30 16:57:12.279  INFO 1 --- [ecutor-thread-5] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'\n2019-08-30 16:57:12.974  INFO 1 --- [ecutor-thread-5] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\n2019-08-30 16:57:13.860  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Context refreshed\n2019-08-30 16:57:13.961  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Found 1 custom documentation plugin(s)\n2019-08-30 16:57:14.020  INFO 1 --- [ecutor-thread-5] s.d.s.w.s.ApiListingReferenceScanner     : Scanning for api listing references\n2019-08-30 16:57:14.504  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Started SBApplication in 17.584 seconds (JVM running for 33.368)\n[AUDIT   ] CWWKZ0001I: Application thinClinic started in 21.090 seconds.\n[AUDIT   ] CWWKF0012I: The server installed the following features: [el-3.0, jsp-2.3, servlet-4.0, springBoot-2.0, ssl-1.0, transportSecurity-1.0, websocket-1.1].\n[AUDIT   ] CWWKF0011I: The defaultServer server is ready to run a smarter planet. The defaultServer server started in 33.103 seconds.\n</code></pre> <p>This shows that the Spring Boot application is successfully started.</p>"},{"location":"labs/containers/#access-the-application","title":"Access the application","text":"<ul> <li>To access the application, open the browser and access http://localhost:9080/greeting?name=John.</li> </ul> <p>You will see something like below.</p> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, John :)\"}\n</code></pre> <p>Container Image Registry</p> <p>Container Image Registry is a place where you can store the container images. They can be public or private registries. They can be hosted by third party as well. In this lab, we are using Quay.</p>"},{"location":"labs/containers/#pushing-an-image-to-a-registry","title":"Pushing an image to a Registry","text":"<p>Let us now push the image to the Quay registry. Before pushing the image to the registry, one needs to login.</p> <ol> <li>Login to Quay using your credentials.</li> </ol> <p>Once logged in we need to take the image for the registry.</p> <ol> <li> <p>Tag your image for the image registry using the <code>same name and tag from before</code>. Be sure to include the host name of the target image registry in the destination tag (e.g. quay.io). NOTE: the tag command has both the source tag and repository destination tag in it.</p> </li> <li> <p>Now push the image to the registry. This allows you to share images to a registry.</p> </li> </ol> <p>If everything goes fine, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nThe push refers to repository [quay.io/&lt;repository_name&gt;/greeting]\n2e4d09cd03a2: Pushed\nd862b7819235: Pushed\na9212239031e: Pushed\n4be784548734: Pushed\na43c287826a1: Mounted from library/ibmjava\ne936f9f1df3e: Mounted from library/ibmjava\n92d3f22d44f3: Mounted from library/ibmjava\n10e46f329a25: Mounted from library/ibmjava\n24ab7de5faec: Mounted from library/ibmjava\n1ea5a27b0484: Mounted from library/ibmjava\nv1.0.0: digest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4 size: 2415\n</code></pre> <p>Once the push is successful, your image will be residing in the registry.</p>"},{"location":"labs/containers/#clean-up","title":"Clean Up","text":"<ol> <li> <p>Stop the <code>greeting</code> container.</p> </li> <li> <p>Remove the container.</p> </li> <li> <p>Remove the image. (NOTE: You will need the image_id to remove it.)</p> </li> </ol>"},{"location":"labs/containers/#pulling-an-image-from-the-registry","title":"Pulling an image from the registry","text":"<p>Sometimes, you may need the images that are residing on your registry. Or you may want to use some public images out there. Then, we need to pull the image from the registry.</p> <ol> <li>Pull the image <code>greeting</code> from the registry,</li> </ol> <p>If it successfully got pulled, we will see something like below.</p> <pre><code>ddcb5f219ce2: Pull complete\ne3371bbd24a0: Pull complete\n49d2efb3c01b: Pull complete\nDigest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4\nStatus: Downloaded newer image for &lt;repository_name&gt;/greeting:v1.0.0\ndocker.io/&lt;repository_name&gt;/greeting:v1.0.0\n</code></pre>"},{"location":"labs/containers/#conclusion","title":"Conclusion","text":"<p>You have successfully completed this lab! Let's take a look at what you learned and did today:</p> <ul> <li>Learned about Dockerfile.</li> <li>Learned about docker images.</li> <li>Learned about docker containers.</li> <li>Learned about multi-stage docker builds.</li> <li>Ran the Greetings service on Docker.</li> </ul> <p>Congratulations !!!</p>"},{"location":"labs/containers/container-registry/","title":"IBM Container Registries","text":"Quay.ioIBM Container RegistryDocker Hub <p>In this lab we are going to create a Container Image and store it in the IBM Cloud Container Registry</p>"},{"location":"labs/containers/container-registry/#introduction","title":"Introduction","text":"<p>In this lab you will learn how to use podman.</p>"},{"location":"labs/containers/container-registry/#introduction_1","title":"Introduction","text":""},{"location":"labs/containers/container-registry/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM Cloud Account</li> </ul>"},{"location":"labs/containers/container-registry/#login-into-ibm-cloud","title":"Login into IBM Cloud","text":""},{"location":"labs/containers/container-registry/#using-the-ibm-cloud-shell","title":"Using the IBM Cloud Shell","text":"<ol> <li>Login into IBM Cloud</li> <li>Select correct account from top right drop down if your IBM id is associated with multiple accounts</li> <li>Click the IBM Cloud Shell Icon on the top right corner of the IBM Cloud Console     </li> <li>This opens a new browser window with command linux terminal prompt.     </li> </ol>"},{"location":"labs/containers/container-registry/#create-a-new-container-registry-namespace","title":"Create a new Container Registry namespace","text":"<ol> <li>Ensure that you're targeting the correct IBM Cloud Container Registry region. For example for Dallas region use us-south <pre><code>ibmcloud cr region-set us-south\n</code></pre></li> <li>Choose a name for your first namespace, and create that namespace. Use this namespace for the rest of the Quick Start.Create a new Container Registry Namespace. This namespace is different from a Kubernetes/OpenShift namespace. The name needs to be all lowercase  and globaly unique within a region.     <pre><code>ibmcloud cr namespace-add &lt;my_namespace&gt;\n</code></pre>     Now set the environment <code>NAMESPACE</code> to be use for the rest of the lab     <pre><code>export NAMESPACE=&lt;my_namespace&gt;\n</code></pre></li> </ol>"},{"location":"labs/containers/container-registry/#building-and-pushing-a-container-image","title":"Building and Pushing a Container Image","text":"<ol> <li>Clone the following git repository and change directory to <code>1-containers</code> <pre><code>git clone --depth 1 https://github.com/csantanapr/think2020-nodejs.git my-app\ncd my-app/1-containers/\n</code></pre></li> <li>Inspect the file <code>Dockerfile</code> it contains a multistage build, first layer builds the application, the second copies only the built files.     <pre><code>cat Dockerfile\n</code></pre> <pre><code>FROM registry.access.redhat.com/ubi8/nodejs-12 as base\n\nFROM base as builder\n\nWORKDIR /opt/app-root/src\n\nCOPY package*.json ./\n\nRUN npm ci\n\nCOPY public public \nCOPY src src \n\nRUN npm run build\n\nFROM base\n\nWORKDIR /opt/app-root/src\n\nCOPY --from=builder  /opt/app-root/src/build build\n\nCOPY package*.json ./\n\nRUN npm ci --only=production\n\nCOPY --chown=1001:0 server server\nRUN chmod -R g=u server\n\nENV PORT=8080\n\nLABEL com.example.source=\"https://github.com/csantanapr/think2020-nodejs\"\nLABEL com.example.version=\"1.0\"\n\nARG ENV=production\nENV NODE_ENV $ENV\nENV NODE_VERSION $NODEJS_VERSION\nCMD npm run $NODE_ENV\n</code></pre></li> <li>Build and push the image, if not already set replace <code>$NAMESPACE</code> with the namespace you added previously, replace <code>us.icr.io</code> if using a different region.     <pre><code>ibmcloud cr build --tag us.icr.io/$NAMESPACE/my-app:1.0 ./\n</code></pre></li> </ol>"},{"location":"labs/containers/container-registry/#explore-the-container-registry-on-the-ibm-cloud-console","title":"Explore the Container Registry on the IBM Cloud Console","text":"<ol> <li>Explore the container image details using the IBM Cloud Console. Go to the Main Menu-&gt;Kubernetes-&gt;Registry you can use the tabs <code>Namespaces</code>, <code>Repository</code>, <code>Images</code> </li> </ol>"},{"location":"labs/containers/container-registry/#extra-credit-run-imge-on-kubernetes","title":"Extra Credit (Run Imge on Kubernetes)","text":"<p>If you have a Kubernetes Cluster you can run your application image</p> <ol> <li>Get the Access token for your Kubernetes cluster, command assumes your cluster name is <code>mycluster</code> <pre><code>ibmcloud ks cluster config -c mycluster\n</code></pre></li> <li>Run the following commands to create a deployment using the image we just build. If not already set replace <code>$NAMESPACE</code> with your IBM Container Registry Namespace we stored the image.     <pre><code>kubectl create deployment my-app --image us.icr.io/$NAMESPACE/my-app:1.0\nkubectl rollout status deployment/my-app\nkubectl port-forward deployment/my-app 8080:8080\n</code></pre>     If the app is connected you should see the following output     <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\n</code></pre></li> <li>Open a new Session and run the following command     <pre><code>curl localhost:8080 -I\n</code></pre>     You should see in the first line of output the following     <pre><code>HTTP/1.1 200 OK\n</code></pre></li> <li> <p>To access the app using a browser use the IBM Cloud Shell Web Preview. Click the Web Preview Icon and select port <code>8080</code> from the drop down. The application will open in a new browser window.      </p> </li> <li> <p>To stop the application on the terminal with the <code>kubectl port-forward</code> command quit by pressing Ctrl+C in *Session 1</p> </li> </ol>"},{"location":"labs/containers/container-registry/#delete-deployment-and-image","title":"Delete Deployment and Image","text":"<ol> <li>Delete the app deployment     <pre><code>kubectl delete deployment my-app\n</code></pre></li> <li>Delete the container image, if not already set replace <code>$NAMESPACE</code> with the registry namespace     <pre><code>ibmcloud cr image-rm us.icr.io/$NAMESPACE/my-app:1.0\n</code></pre></li> </ol>"},{"location":"labs/devops/argocd/","title":"ArgoCD Lab","text":"OpenShiftKubernetes"},{"location":"labs/devops/argocd/#openshift","title":"OpenShift","text":""},{"location":"labs/devops/argocd/#pre-requisites","title":"Pre-requisites","text":"<p>Make sure your environment is setup properly for the lab.</p> <p>Check the Environment Setup page for your setup.</p>"},{"location":"labs/devops/argocd/#argocd-installation","title":"ArgoCD Installation","text":"<ul> <li>Create the namespace <code>argocd</code> to install argocd     <pre><code>oc new-project argocd\n</code></pre></li> <li>Install ArgoCD as follows.     <pre><code>oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-operator.yaml\n</code></pre></li> <li>When installing the tutorial, make sure you wait until the argocd-operator is finished before installing the argocd-cr..or it will fail. You can do this:     <pre><code>oc get ClusterServiceVersion -n argocd\nNAME                                   DISPLAY                        VERSION   REPLACES   PHASE\nargocd-operator.v0.0.8                 Argo CD                        0.0.8                Succeeded\n</code></pre>     and wait for the \"succeeded\" to come up before proceeding.     <pre><code>oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-cr.yaml\n</code></pre>     and wait for the argocd server Pod to be running     <pre><code>oc get pods -n argocd -l app.kubernetes.io/name=example-argocd-server\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\nexample-argocd-server-57c4fd5c45-zf4q6   1/1     Running   0          115s\n</code></pre></li> <li>Install the <code>argocd</code> CLI, for example on OSX use brew     <pre><code>brew tap argoproj/tap\nbrew install argoproj/tap/argocd\n</code></pre></li> <li>Set an environment variable <code>ARGOCD_URL</code> using the <code>EXTERNAL-IP</code> <pre><code>export ARGOCD_NAMESPACE=\"argocd\"\nexport ARGOCD_SERVER=$(oc get route example-argocd-server -n $ARGOCD_NAMESPACE -o jsonpath='{.spec.host}')\nexport ARGOCD_URL=\"https://$ARGOCD_SERVER\"\necho ARGOCD_URL=$ARGOCD_URL\necho ARGOCD_SERVER=$ARGOCD_SERVER\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#deploying-the-app","title":"Deploying the app","text":"<ul> <li>Login into the UI.     <pre><code>open $ARGOCD_URL\n</code></pre></li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>oc get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d\n</code></pre>     For example the output is similar to this:     <pre><code>tyafMb7BNvO0kP9eizx3CojrK8pYJFQq\n</code></pre></li> </ul> <ul> <li>Now go back to the ArgoCD home and click on <code>NEW APP</code>.</li> <li>Add the below details:</li> <li>Application Name: <code>sample</code></li> <li>Project - <code>default</code></li> <li>SYNC POLICY: <code>Manual</code></li> <li>REPO URL: <code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>openshift</code></li> </ul> <ul> <li>Cluster - Select the default one <code>https://kubernetes.default.svc</code> to deploy in-cluster</li> <li>Namespace - <code>default</code></li> <li>Click Create to finish</li> </ul> <ul> <li>You will now see the available apps.</li> </ul> <ul> <li>Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying.</li> </ul> <p>To sync the application, click <code>SYNC</code> and then <code>SYNCHRONIZE</code>.</p> <p></p> <ul> <li>Wait till the app is deployed.</li> </ul> <p></p> <ul> <li>Once the app is deployed, click on it to see the details.</li> </ul> <p></p> <p></p>"},{"location":"labs/devops/argocd/#verifying-the-deployment","title":"Verifying the deployment","text":"<ul> <li>Access the app to verify if it is correctly deployed.</li> <li>List the cloudnativesampleapp-service route     <pre><code>oc get route\n</code></pre>     It should have an IP under <code>EXTERNAL-IP</code> column     <pre><code>NAME                 HOST/PORT                                     PATH   SERVICES                       PORT   TERMINATION   WILDCARD\ncloudnative-sample   cloudnative-sample-default.apps-crc.testing          cloudnativesampleapp-service   9080                 None\n</code></pre></li> <li>Set an environment variable <code>APP_URL</code> using the <code>EXTERNAL-IP</code> <pre><code>export APP_URL=\"http://$(oc get route cloudnative-sample -o jsonpath='{.status.ingress[0].host}')\"\necho ARGOCD_SERVER=$APP_URL\n</code></pre></li> <li>Access the url using <code>curl</code> <pre><code>curl \"$APP_URL/greeting?name=Carlos\"\n</code></pre> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#using-the-argocd-cli","title":"Using the ArgoCD CLI","text":"<ul> <li>Login using the cli.</li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>export ARGOCD_PASSWORD=$(oc get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d)\necho $ARGOCD_PASSWORD\n</code></pre></li> <li>Now login as follows.     <pre><code>argocd login --username admin --password $ARGOCD_PASSWORD $ARGOCD_SERVER\n</code></pre> <pre><code>WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain \nany IP SANs. Proceed insecurely (y/n)? y\n\n'admin' logged in successfully\nContext 'example-argocd-server-argocd.apps-crc.testing' updated\n</code></pre></li> <li>List the applications     <pre><code>argocd app list\n</code></pre> <pre><code>NAME    CLUSTER                         NAMESPACE  PROJECT  STATUS  HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                                     PATH   TARGET\nsample  https://kubernetes.default.svc  default    default  Synced  Healthy  &lt;none&gt;      &lt;none&gt;      https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy  openshift  HEAD\n</code></pre></li> <li>Get application details     <pre><code>argocd app get sample\n</code></pre> <pre><code>Name:               sample\nProject:            default\nServer:             https://kubernetes.default.svc\nNamespace:          default\nURL:                https://10.97.240.99/applications/sample\nRepo:               https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy\nTarget:             HEAD\nPath:               openshift\nSyncWindow:         Sync Allowed\nSync Policy:        &lt;none&gt;\nSync Status:        Synced to HEAD (9684037)\nHealth Status:      Healthy\n\nGROUP  KIND        NAMESPACE  NAME                             STATUS  HEALTH   HOOK  MESSAGE\n    Service     default    cloudnativesampleapp-service     Synced  Healthy        service/cloudnativesampleapp-service created\napps   Deployment  default    cloudnativesampleapp-deployment  Synced  Healthy        deployment.apps/cloudnativesampleapp-deployment created\n</code></pre></li> <li>Show application deployment history     <pre><code>argocd app history sample\n</code></pre> <pre><code>ID  DATE                           REVISION\n0   2020-02-12 21:10:32 -0500 EST  HEAD (9684037)\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#references","title":"References","text":"<ul> <li>ArgoCD</li> </ul>"},{"location":"labs/devops/argocd/#kubernetes","title":"Kubernetes","text":""},{"location":"labs/devops/argocd/#pre-requisites_1","title":"Pre-requisites","text":"<p>Make sure your environment is setup properly for the lab.</p> <p>Check the Environment Setup page for your setup.</p>"},{"location":"labs/devops/argocd/#argocd-installation_1","title":"ArgoCD Installation","text":"<ul> <li>Create the namespace <code>argocd</code> to install argocd     <pre><code>kubectl create namespace argocd\nexport ARGOCD_NAMESPACE=argocd\n</code></pre></li> <li> <p>Create RBAC resources     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/service_account.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/role.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/role_binding.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argo-clusteradmin.yaml\n</code></pre></p> </li> <li> <p>Install CRDs     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/argo-cd/argoproj.io_applications_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/argo-cd/argoproj.io_appprojects_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/crds/argoproj.io_argocdexports_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/crds/argoproj.io_argocds_crd.yaml\n</code></pre>     Verify CRDs     <pre><code>kubectl get crd -n argocd\n</code></pre> <pre><code>NAME                        CREATED AT\napplications.argoproj.io    2020-05-15T02:05:55Z\nappprojects.argoproj.io     2020-05-15T02:05:56Z\nargocdexports.argoproj.io   2020-05-15T02:08:21Z\nargocds.argoproj.io         2020-05-15T02:08:21Z\n</code></pre></p> </li> <li>Deploy Operator     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/operator.yaml\n</code></pre></li> <li>Deploy ArgoCD CO     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/examples/argocd-lb.yaml\n</code></pre>     Verify that ArgoCD Pods are running     <pre><code>kubectl get pods -n argocd\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\nargocd-operator-5f7d8cf7d8-486vn                         1/1     Running   0          3m46s\nexample-argocd-application-controller-7dc5fcb75d-xkk5h   1/1     Running   0          2m3s\nexample-argocd-dex-server-bb9df96cb-ndmhl                1/1     Running   0          2m3s\nexample-argocd-redis-756b6764-sb2gt                      1/1     Running   0          2m3s\nexample-argocd-repo-server-75944fcf87-zmh48              1/1     Running   0          2m3s\nexample-argocd-server-747b684c8c-xhgl9                   1/1     Running   0          2m3s\n</code></pre>     Verify that the other ArgoCD resources are created     <pre><code>kubectl get cm,secret,svc,deploy -n argocd\n</code></pre></li> <li> <p>List the argocd-server service     <pre><code>kubectl get svc example-argocd-server -n argocd\n</code></pre></p> </li> <li> <p>From the script, the Argo Server service has a <code>type</code> of <code>LoadBalancer</code>. If the <code>ExternalIP</code> is in a <code>pending</code> state, then there is no loadBalancer for your cluster, so we only need the the ArgoCD server's <code>NodePort</code>. Otherwise use the <code>ExternalIP</code> and <code>NodePort</code> to access Argo.     <pre><code>NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\nexample-argocd-server   LoadBalancer   10.105.73.245   &lt;pending&gt;   80:31138/TCP,443:31932/TCP   5m3s\n</code></pre></p> </li> <li> <p>To access the service we need the <code>Node's External IP</code> and the <code>NodePort</code>. Let's set an environment variable <code>ARGOCD_URL</code> with <code>NODE_EXTERNAL_IP</code>:<code>NodePort</code>.     <pre><code>export NODE_EXTERNAL_IP=\"$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}')\"\nexport ARGOCD_NODEPORT=\"$(kubectl get svc example-argocd-server -n $ARGOCD_NAMESPACE -o jsonpath='{.spec.ports[0].nodePort}')\"\nexport ARGOCD_URL=\"https://$NODE_EXTERNAL_IP:$ARGOCD_NODEPORT\"\necho ARGOCD_URL=$ARGOCD_URL\n</code></pre></p> </li> <li> <p>If you can't access the NodePort from your computer and only http/80 then edit the argocd-server and add the flag <code>--insecure</code> <pre><code>kubectl edit -n argocd deployment example-argocd-server\n</code></pre>     Use the kube api to proxy into the argocd server using <code>kubectl port-forward</code> <pre><code>kubectl port-forward service/example-argocd-server 8080:80 -n argocd\n</code></pre>     Then you can access the argocd server locally on port 8080 http://localhost:8080</p> </li> </ul>"},{"location":"labs/devops/argocd/#deploying-the-app_1","title":"Deploying the app","text":"<ul> <li>Login using the Browser into the UI using <code>$ARGOCD_URL</code> or <code>localhost:8080</code> if using port-forward</li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>kubectl get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d\n</code></pre>     For example the output is similar to this:     <pre><code>tyafMb7BNvO0kP9eizx3CojrK8pYJFQq\n</code></pre></li> </ul> <ul> <li>Now go back to the ArgoCD home and click on <code>NEW APP</code>.</li> <li>Add the below details:</li> <li>Application Name: <code>sample</code></li> <li>Project - <code>default</code></li> <li>SYNC POLICY: <code>Manual</code></li> <li>REPO URL: <code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>kubernetes</code></li> </ul> <ul> <li>Cluster - Select the default one <code>https://kubernetes.default.svc</code> to deploy in-cluster</li> <li>Namespace - <code>default</code></li> <li>Click Create to finish</li> </ul> <ul> <li>You will now see the available apps.</li> </ul> <ul> <li>Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying.</li> </ul> <p>To sync the application, click <code>SYNC</code> and then <code>SYNCHRONIZE</code>.</p> <p></p> <ul> <li>Wait till the app is deployed.</li> </ul> <p></p> <ul> <li>Once the app is deployed, click on it to see the details.</li> </ul> <p></p> <p></p>"},{"location":"labs/devops/argocd/#verifying-the-deployment_1","title":"Verifying the deployment","text":"<ul> <li>Access the app to verify if it is correctly deployed.</li> <li>List the cloudnativesampleapp-service service     <pre><code>kubectl get svc cloudnativesampleapp-service\n</code></pre>     It will have the <code>NodePort</code> for the application. In this case, it is <code>30499</code>.      <pre><code>NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\ncloudnativesampleapp-service   NodePort   172.21.118.165   &lt;none&gt;        9080:30499/TCP   20s\n</code></pre></li> <li>Set an environment variable <code>APP_URL</code> using the <code>Node's IP</code> and <code>NodePort</code> values     <pre><code>export APP_NODE_PORT=\"$(kubectl get svc cloudnativesampleapp-service -n default -o jsonpath='{.spec.ports[0].nodePort}')\"\nexport APP_URL=\"$NODE_EXTERNAL_IP:$APP_NODE_PORT\"\necho Application=$APP_URL\n</code></pre></li> <li>Access the url using <code>curl</code> <pre><code>curl \"$APP_URL/greeting?name=Carlos\"\n</code></pre> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#references_1","title":"References","text":"<ul> <li>ArgoCD</li> </ul>"},{"location":"labs/devops/ibm-toolchain/","title":"IBM Toolchain Lab","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"labs/devops/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     <pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.</p> </li> </ol>"},{"location":"labs/devops/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"labs/devops/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"labs/devops/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"labs/devops/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"labs/devops/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"labs/devops/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"labs/devops/jenkins/","title":"Jenkins Lab","text":"OpenShiftKubernetes"},{"location":"labs/devops/jenkins/#introduction","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using Jenkins to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - Jenkins, Leading open source automation server.</p>"},{"location":"labs/devops/jenkins/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need an IBM cloud account.</li> <li>Create kubernetes cluster using IBM Cloud Kubernetes Service. Here, you can choose an openshift cluster.</li> <li>Install oc command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"labs/devops/jenkins/#continuous-integration","title":"Continuous Integration","text":""},{"location":"labs/devops/jenkins/#install-jenkins","title":"Install Jenkins","text":"<ul> <li>Open the IBM Cloud Openshift cluster.</li> </ul> <ul> <li>Click on the <code>OpenShift web console</code> tab and this will take you to openshift UI.</li> </ul> <ul> <li>Create a new project.</li> </ul> <ul> <li>Search for <code>Jenkins</code>.</li> </ul> <ul> <li>Choose <code>Jenkins (Ephemeral)</code>.</li> </ul> <ul> <li>Install it.</li> </ul> <ul> <li>Wait till the Jenkins installs and the pods are ready.</li> </ul> <ul> <li>Once, it is ready you can access the Jenkins by clicking the link.</li> </ul> <p>Now, click on <code>Log in with OpenShift</code>.</p> <ul> <li>When you gets logged in, you will see the below screen. Click <code>Allow selected permissions</code>.</li> </ul> <p></p> <ul> <li>You will be able to access the Jenkins UI now.</li> </ul> <p></p>"},{"location":"labs/devops/jenkins/#get-the-sample-app","title":"Get the Sample App","text":"<ul> <li>Fork the below repository.</li> </ul> <pre><code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n</code></pre> <ul> <li>Clone the forked repository.</li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"labs/devops/jenkins/#jenkinsfile","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"labs/devops/jenkins/#jenkins-credentials","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p>image::Jenkins_add_creds.png[align=\"center\"] </p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: (Your container registry url, for eg., us.icr.io) ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"labs/devops/jenkins/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of the application, choose <code>Pipeline</code> and click <code>OK</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build Now</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"labs/devops/jenkins/#introduction_1","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using https://jenkins.io/[Jenkins] to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - https://jenkins.io/doc/[Jenkins, Leading open source automation server].</p>"},{"location":"labs/devops/jenkins/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>You need an https://cloud.ibm.com/login[IBM cloud account].</li> <li>Create kubernetes cluster using https://cloud.ibm.com/docs/containers?topic=containers-getting-started[IBM Cloud Kubernetes Service]. Here, you can choose a kubernetes cluster.</li> <li>Install https://kubernetes.io/docs/tasks/tools/install-kubectl/[kubectl] command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"labs/devops/jenkins/#continuous-integration_1","title":"Continuous Integration","text":""},{"location":"labs/devops/jenkins/#install-jenkins_1","title":"Install Jenkins","text":"<ul> <li>Initially log in into your ibm cloud account as follows.</li> </ul> <pre><code>$ ibmcloud login -a cloud.ibm.com -r (region) -g (cluster_name)\n</code></pre> <p>And then download the Kube config files as below.</p> <pre><code>$ ibmcloud ks cluster-config --cluster (cluster_name)\n</code></pre> <p>You can also get the <code>access</code> instructions in <code>IBM Cloud Dashboard -&gt; Kubernetes Clusters -&gt; Click on your Cluster -&gt; Click on Access Tab</code>.</p> <ul> <li>Install Jenkins using helm using the below command. We are not using persistence in this lab.</li> </ul> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\n</code></pre> <p>If it is successfully executed, you will see something like below.</p> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\nNAME:   cloudnative\nLAST DEPLOYED: Wed Aug  7 16:22:55 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==&gt; v1/ConfigMap\nNAME                       DATA  AGE\ncloudnative-jenkins        5     1s\ncloudnative-jenkins-tests  1     1s\n\n==&gt; v1/Deployment\nNAME                 READY  UP-TO-DATE  AVAILABLE  AGE\ncloudnative-jenkins  0/1    1           0          1s\n\n==&gt; v1/Pod(related)\nNAME                                  READY  STATUS    RESTARTS  AGE\ncloudnative-jenkins-57588c86c7-hxqmq  0/1    Init:0/1  0         0s\n\n==&gt; v1/Role\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/RoleBinding\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/Secret\nNAME                 TYPE    DATA  AGE\ncloudnative-jenkins  Opaque  2     1s\n\n==&gt; v1/Service\nNAME                       TYPE          CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE\ncloudnative-jenkins        LoadBalancer  172.21.143.35   169.63.132.124  8080:32172/TCP  1s\ncloudnative-jenkins-agent  ClusterIP     172.21.206.235  (none&gt;          50000/TCP       1s\n\n==&gt; v1/ServiceAccount\nNAME                 SECRETS  AGE\ncloudnative-jenkins  1        1s\n</code></pre> <p>Use the following steps to open Jenkins UI and login.</p> <pre><code>NOTES:\n1. Get your 'admin' user password by running:\nprintf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n2. Get the Jenkins URL to visit by running these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace default -w cloudnative-jenkins'\nexport SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\necho http://$SERVICE_IP:8080/login\n\n3. Login with the password from step 1 and the username: admin\n\n\nFor more information on running Jenkins on Kubernetes, visit:\nhttps://cloud.google.com/solutions/jenkins-on-container-engine\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Jenkins pod is terminated.                            #####\n#################################################################################\n</code></pre> <p>To get the url, run the below commands.</p> <pre><code>$ export SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\n$ echo http://$SERVICE_IP:8080/login\n</code></pre> <p>Once executed, you will see something like below.</p> <pre><code>$ echo http://$SERVICE_IP:8080/login\nhttp://169.63.132.124:8080/login\n</code></pre> <ul> <li>Now, let us login into the Jenkins.</li> </ul> <p></p> <p>The user name will be <code>admin</code> and to get the password, run the below command.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n</code></pre> <p>It returns you the password as follows.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\npassword\n</code></pre> <ul> <li>Once, successfully logged in you will see the Jenkins home page which is as follows.</li> </ul> <p></p>"},{"location":"labs/devops/jenkins/#get-the-sample-app_1","title":"Get the Sample App","text":"<ul> <li> <p>Fork the below repository.</p> <p>https://github.com/ibm-cloud-architecture/cloudnative_sample_app</p> </li> <li> <p>Clone the forked repository.</p> </li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"labs/devops/jenkins/#jenkinsfile_1","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"labs/devops/jenkins/#jenkins-credentials_1","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p></p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: Your container registry url, for eg., us.icr.io ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"labs/devops/jenkins/#jenkins-pipeline_1","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of your application, select <code>Pipeline</code> and then click <code>OK</code>.</li> </ul> <ul> <li>In <code>General</code>, check <code>This project is parameterized</code>. Create a string parameter with name <code>CLOUD</code> and Default value <code>kubernetes</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build with Parameters</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"labs/devops/tekton/","title":"Tekton Lab","text":"OpenShiftKubernetes"},{"location":"labs/devops/tekton/#prerequisites","title":"Prerequisites","text":"<p>Make sure your environment is properly setup.</p> <p>Follow the instructions here</p>"},{"location":"labs/devops/tekton/#setup","title":"SetUp","text":""},{"location":"labs/devops/tekton/#tekton-cli-installation","title":"Tekton CLI Installation","text":"<ul> <li> <p>Tekton CLI is command line utility used to interact with the Tekton resources.</p> </li> <li> <p>Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn</p> </li> <li> <p>For MacOS for example you can use brew     <pre><code>brew tap tektoncd/tools\nbrew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li>Verify the Tekton cli     <pre><code>tkn version\n</code></pre></li> <li>The command should show a result like:     <pre><code>$ tkn version\nClient version: 0.10.0\n</code></pre></li> <li>If you already have the <code>tkn</code> install you can upgrade running     <pre><code>brew upgrade tektoncd/tools/tektoncd-cli\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#tekton-pipelines-installation","title":"Tekton Pipelines Installation","text":"<ul> <li>To deploy the Tekton pipelines:     <code>oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/tekton-lab/tekton-operator.yaml</code></li> <li>Note: It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command:     <pre><code>oc get pods -n openshift-operators -w\n</code></pre>     You can use <code>Ctrl+c</code> to terminate the watch</li> <li>A successful deployment of Tekton pipelines will show the following pods:     <pre><code>NAME                                         READY   STATUS    RESTARTS   AGE\nopenshift-pipelines-operator-9cdbbb854-x9tvs   1/1     Running   0          25s\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#create-target-namespace","title":"Create Target Namespace","text":"<ul> <li>Set the environment variable <code>NAMESPACE</code> to <code>tekton-demo</code>, if you open a new terminal remember to set this environment again     <pre><code>export NAMESPACE=tekton-demo\n</code></pre></li> <li>Create a the namespace using the variable <code>NAMESPACE</code> <pre><code>oc new-project $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#tasks","title":"Tasks","text":""},{"location":"labs/devops/tekton/#task-creation","title":"Task Creation","text":"<ul> <li>Create the below yaml files.</li> <li>The following snippet shows what a Tekton Task YAML looks like:</li> <li> <p>Create the file task-test.yaml <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: java-test\nspec:\nparams:\n    - name: url\n    default: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n    - name: revision\n    default: master\nsteps:\n    - name: git-clone\n    image: alpine/git\n    script: |\n        git clone -b $(params.revision) --depth 1 $(params.url) /source\n    volumeMounts:\n        - name: source\n        mountPath: /source\n    - name: test\n    image: maven:3.3-jdk-8\n    workingdir: /source\n    script: |\n        mvn test\n        echo \"tests passed with rc=$?\"\n    volumeMounts:\n        - name: m2-repository\n        mountPath: /root/.m2\n        - name: source\n        mountPath: /source\nvolumes:\n    - name: m2-repository\n    emptyDir: {}\n    - name: source\n    emptyDir: {}\n</code></pre></p> </li> <li> <p>Each Task has the following:</p> </li> <li>name - the unique name using which the task can be referred<ul> <li>name - the name of the parameter</li> <li>description - the description of the parameter</li> <li>default - the default value of parameter</li> </ul> </li> <li> <p>Note: The <code>TaskRun</code> or <code>PipelineRun</code> could override the parameter values, if no parameter value is passed then the default value will be used.</p> </li> <li> <p>steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec</p> </li> <li>volumes - the task can also mount external volumes using the volumes attribute.</li> <li>The parameters that were part of the spec inputs params can be used in the steps using the notation <code>$(&lt;variable-name&gt;)</code>.</li> </ul>"},{"location":"labs/devops/tekton/#task-deploy","title":"Task Deploy","text":"<ul> <li> <p>The application test task could be created using the command:     <pre><code>oc apply -f task-test.yaml -n $NAMESPACE\n</code></pre></p> </li> <li> <p>We will use the Tekton cli to inspect the created resources     <pre><code>tkn task ls -n $NAMESPACE\n</code></pre></p> </li> <li> <p>The above command should list one Task as shown below:     <pre><code>NAME        AGE\njava-test   22 seconds ago\n</code></pre></p> </li> </ul>"},{"location":"labs/devops/tekton/#taskrun","title":"TaskRun","text":"<ul> <li>The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step</li> </ul>"},{"location":"labs/devops/tekton/#taskrun-creation","title":"TaskRun Creation","text":"<ul> <li>The following snippet shows what a Tekton TaskRun YAML looks like:</li> <li>Create the file taskrun-test.yaml <pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\ngenerateName: test-task-run-\nspec:\ntaskRef:\n    name: java-test\nparams:\n    - name: url\n    value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n</code></pre></li> <li>generateName - since the TaskRun can be run many times, in order to have unqiue name across the TaskRun ( helpful when checking the TaskRun history) we use this generateName instead of name. When Kubernetes sees generateName it will generate unquie set of characters and suffix the same to build-app-, similar to how pod names are generated</li> <li>taskRef - this is used to refer to the Task by its name that will be run as part of this TaskRun. In this example we use build-app Task.</li> <li>As described in the earlier section that the Task inputs and outputs could be overridden via TaskRun.</li> <li>params - this are the parameter values that are passed to the task</li> <li>The application test task(java-maven-test) could be run using the command:     <pre><code>kubectl create -f taskrun-test.yaml -n $NAMESPACE \n</code></pre></li> <li>Note - As tasks will use generated name, never use <code>oc apply -f taskrun-test.yaml</code></li> <li> <p>We will use the Tekton cli to inspect the created resources:     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     The above command should list one TaskRun as shown below:     <pre><code>NAME                       STARTED        DURATION   STATUS\ntest-task-run-q6s8c        1 minute ago   ---        Running(Pending)\n</code></pre> Note - It will take few seconds for the TaskRun to show status as Running as it needs to download the container images.</p> </li> <li> <p>To check the logs of the Task Run using the <code>tkn</code>: <pre><code>tkn tr logs -f --last -n $NAMESPACE\n</code></pre> Note - Each task step will be run within a container of its own. The -f or -a allows to tail the logs from all the containers of the task. For more options run <code>tkn tr logs --help</code></p> </li> <li>If you see the TaskRun status as Failed or Error use the following command to check the reason for error:     <pre><code>tkn tr describe --last -n $NAMESPACE\n</code></pre></li> <li>If it is successful, you will see something like below.     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     The above command should list one TaskRun as shown below:     <pre><code>NAME                  STARTED          DURATION     STATUS\ntest-task-run   47 seconds ago   34 seconds   Succeeded\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#creating-additional-tasks-and-deploying-them","title":"Creating additional tasks and deploying them","text":"<ul> <li>Create a Task to build a container image and push to the registry</li> <li>This task will be later used by the pipeline.</li> <li>Download the task file task-buildah.yaml to build the image, push the image to the registy:</li> <li>Create the <code>buildah</code> Task using the file and the command:     <pre><code>oc apply -f task-buildah.yaml -n $NAMESPACE\n</code></pre></li> <li>Use the Tekton cli to inspect the created resources     <pre><code>tkn task ls -n $NAMESPACE\n</code></pre></li> <li> <p>The above command should list one Task as shown below:     <pre><code>NAME              AGE\nbuildah            4 seconds ago\njava-test         46 minutes ago\n</code></pre></p> </li> <li> <p>Create an environment variable for location to push the image to be build. Replace <code>NAMESPACE</code> for the dockerhub username, or IBM CR Namespace     <pre><code>export REGISTRY_SERVER=image-registry.openshift-image-registry.svc:5000\nexport IMAGE_URL=${REGISTRY_SERVER}/${NAMESPACE}/cloudnative_sample_app\necho IMAGE_URL=${IMAGE_URL}\n</code></pre></p> </li> <li> <p>Lets create a Task Run for <code>buildah</code> Task using the <code>tkn</code> CLI passing the inputs, outputs and service account.     <pre><code>tkn task start buildah --showlog \\\n-p image=${IMAGE_URL} \\\n-p url=https://github.com/ibm-cloud-architecture/cloudnative_sample_app \\\n-s pipeline \\\n-n $NAMESPACE\n</code></pre>     The task will start and logs will start printing automatically     <pre><code>Taskrun started: buildah-run-vvrg2\nWaiting for logs to be available...\n</code></pre></p> </li> <li> <p>Verify the status of the Task Run     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     Output should look like this     <pre><code>NAME                  STARTED          DURATION     STATUS\nbuildah-run-zbsrv      2 minutes ago    1 minute     Succeeded\n</code></pre></p> </li> <li>To clean up all Pods associated with all Task Runs, delete all the task runs resources     <pre><code>oc delete taskrun --all -n $NAMESPACE\n</code></pre></li> <li>(Optional) Instead of starting the Task via <code>tkn task start</code> you could also use yaml TaskRun, create a file taskrun.yaml <pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\ngenerateName: buildah-task-run-\nspec:\nserviceAccountName: pipeline\ntaskRef:\n    name: buildah\nparams:\n    - name: url\n    value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n    - name: image\n    value: image-registry.openshift-image-registry.svc:5000/tekton-demo/cloudnative_sample_app\n</code></pre>     Then create the TaskRun with      <pre><code>oc create -f taskrun-buildah.yaml -n $NAMESPACE\n</code></pre>     Follow the logs with:     <pre><code>tkn tr logs -f -n $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#pipelines","title":"Pipelines","text":""},{"location":"labs/devops/tekton/#pipeline-creation","title":"Pipeline Creation","text":"<ul> <li> <p>Pipelines allows to start multiple Tasks, in parallel or in a certain order</p> </li> <li> <p>Create the file pipeline.yaml, the Pipeline contains two Tasks     <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: test-build\nspec:\nparams:\n    - name: repo-url\n    default: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n    - name: revision\n    default: master\n    - name: image-server\n    default: image-registry.openshift-image-registry.svc:5000\n    - name: image-namespace\n    default: tekton-demo\n    - name: image-repository\n    default: cloudnative_sample_app\ntasks:\n    - name: test\n    taskRef:\n        name: java-test\n    params:\n        - name: url\n        value: $(params.repo-url)\n        - name: revision\n        value: $(params.revision)\n    - name: build\n    runAfter: [test]\n    taskRef:\n        name: buildah\n    params:\n        - name: image\n        value: $(params.image-server)/$(params.image-namespace)/$(params.image-repository)\n        - name: url\n        value: $(params.repo-url)\n        - name: revision\n        value: $(params.revision)\n</code></pre></p> </li> <li> <p>Pipeline defines a list of Tasks to execute in order, while also indicating if any outputs should be used as inputs of a following Task by using the from field and also indicating the order of executing (using the runAfter and from fields). The same variable substitution you used in Tasks is also available in a Pipeline.</p> </li> <li>Create the Pipeline using the command: <pre><code>oc apply -f pipeline.yaml -n $NAMESPACE\n</code></pre></li> <li>Use the Tekton cli to inspect the created resources <pre><code>tkn pipeline ls -n $NAMESPACE\n</code></pre> The above command should list one Pipeline as shown below: <pre><code>NAME              AGE              LAST RUN   STARTED   DURATION   STATUS\ntest-build-push   31 seconds ago   ---        ---       ---        ---\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#pipelinerun","title":"PipelineRun","text":""},{"location":"labs/devops/tekton/#pipelinerun-creation","title":"PipelineRun Creation","text":"<ul> <li>To execute the Tasks in the Pipeline, you must create a PipelineRun. Creation of a PipelineRun will trigger the creation of TaskRuns for each Task in your pipeline.</li> <li>Create the file pipelinerun.yaml <pre><code>apiVersion: tekton.dev/v1alpha1\nkind: PipelineRun\nmetadata:\ngenerateName: test-build-run-\nspec:\nserviceAccountName: pipeline\npipelineRef:\n    name: test-build\nparams:\n    - name: image-server\n    value: image-registry.openshift-image-registry.svc:5000\n    - name: image-namespace\n    value: tekton-demo\n</code></pre> serviceAccount - it is always recommended to have a service account associated with PipelineRun, which can then be used to define fine grained roles.</li> <li>Create the PipelineRun using the command:     <pre><code>oc create -f pipelinerun.yaml -n $NAMESPACE\n</code></pre></li> <li> <p>We will use the Tekton cli to inspect the created resources     <pre><code>tkn pipelinerun ls -n $NAMESPACE\n</code></pre></p> </li> <li> <p>The above command should list one PipelineRun as shown below:     <pre><code>NAME                        STARTED         DURATION   STATUS\ntest-build-push-run-c7zgv   8 seconds ago   ---        Running\n</code></pre></p> </li> <li> <p>Wait for few minutes for your pipeline to complete all the tasks. If it is successful, you will see something like below.     <pre><code>tkn pipeline ls -n $NAMESPACE\n</code></pre> <pre><code>NAME              AGE              LAST RUN                    STARTED         DURATION    STATUS\ntest-build-push   33 minutes ago   test-build-push-run-c7zgv   2 minutes ago   2 minutes   Succeeded\n</code></pre></p> </li> <li> <p>Run again the pipeline ls command     <pre><code>tkn pipelinerun ls -n $NAMESPACE\n</code></pre> <pre><code>NAME                        STARTED         DURATION    STATUS\ntest-build-push-run-c7zgv   2 minutes ago   2 minutes   Succeeded\n</code></pre>     If it is successful, go to your container registry account and verify if you have the <code>cloudnative_sample_app</code> image pushed.</p> </li> <li> <p>(Optional) Run the pipeline again using the <code>tkn</code> CLI     <pre><code>tkn pipeline start test-build --showlog \\\n-s pipeline \\\n-n $NAMESPACE\n</code></pre></p> </li> <li>(Optional) Re-run the pipeline using last pipelinerun values     <pre><code>tkn pipeline start test-build-push --last -n $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#deploy-application","title":"Deploy Application","text":"<ul> <li>Create a deployment <pre><code>oc create deployment cloudnative --image=${IMAGE_URL} -n $NAMESPACE\n</code></pre></li> <li>Verify if the pods are running: <pre><code>oc get pods -l app=cloudnative -n $NAMESPACE\n</code></pre></li> <li>Expose the deployment as a service <pre><code>oc expose deployment cloudnative --port=9080 -n $NAMESPACE\n</code></pre></li> <li>Expose the service as a route <pre><code>oc expose service cloudnative -n $NAMESPACE\n</code></pre></li> <li>Now access the compose the URL of the App using IP and NodePort     <pre><code>export APP_URL=\"$(oc get route cloudnative --template 'http://{{.spec.host}}')/greeting?name=Carlos\"\necho APP_URL=$APP_URL\n</code></pre> <pre><code>http://cloudnative-tekton-demo.apps-crc.testing/greeting?name=Carlos\n</code></pre></li> <li>Now access the app from terminal or browser     <pre><code>curl $APP_URL\n</code></pre>     Output should be     <pre><code>{\"id\":4,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre> <pre><code>open $APP_URL\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#prerequisites_1","title":"Prerequisites","text":"<p>Make sure your environment is properly setup.</p> <p>Follow the instructions here</p>"},{"location":"labs/devops/tekton/#setup_1","title":"SetUp","text":""},{"location":"labs/devops/tekton/#tekton-cli-installation_1","title":"Tekton CLI Installation","text":"<ul> <li> <p>Tekton CLI is command line utility used to interact with the Tekton resources.</p> </li> <li> <p>Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn</p> </li> <li> <p>For MacOS for example you can use brew     <pre><code>brew install tektoncd-cli\n</code></pre></p> </li> <li>Verify the Tekton cli     <pre><code>tkn version\n</code></pre></li> <li>The command should show a result like:     <pre><code>$ tkn version\nClient version: 0.10.0\n</code></pre></li> <li>If you already have the <code>tkn</code> install you can upgrade running     <pre><code>brew upgrade tektoncd/tools/tektoncd-cli\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#tekton-pipelines-installation_1","title":"Tekton Pipelines Installation","text":"<ul> <li>To deploy the Tekton pipelines:     <pre><code>kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.13.2/release.yaml\n</code></pre></li> <li>Note: It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command:     <pre><code>kubectl get pods -n tekton-pipelines -w\n</code></pre>     You can use <code>Ctrl+c</code> to terminate the watch</li> <li>A successful deployment of Tekton pipelines will show the following pods:     <pre><code>NAME                                         READY   STATUS    RESTARTS   AGE\ntekton-pipelines-controller-9b8cccff-j6hvr   1/1     Running   0          2m33s\ntekton-pipelines-webhook-6fc9d4d9b6-kpkp7    1/1     Running   0          2m33s\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#tekton-dashboard-installation-optional","title":"Tekton Dashboard Installation (Optional)","text":"<ul> <li>To deploy the Tekton dashboard:     <pre><code>kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.7.0/tekton-dashboard-release.yaml\n</code></pre></li> <li>Note: It will take few mins for the Tekton dashboard components to be installed, you an watch the status using the command:     <pre><code>kubectl get pods -n tekton-pipelines -w\n</code></pre>     You can use <code>Ctrl+c</code> to terminate the watch</li> <li>A successful deployment of Tekton pipelines will show the following pods:     <pre><code>NAME                                           READY   STATUS    RESTARTS   AGE\ntekton-dashboard-59c7fbf49f-79f7q              1/1     Running   0          50s\ntekton-pipelines-controller-6b7f7cf7d8-r65ps   1/1     Running   0          15m\ntekton-pipelines-webhook-7bbd8fcc45-sfgxs      1/1     Running   0          15m\n</code></pre></li> <li>Access the dashboard as follows:     <pre><code>kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097\n</code></pre>     You can access the web UI at http://localhost:9097 .</li> </ul>"},{"location":"labs/devops/tekton/#create-target-namespace_1","title":"Create Target Namespace","text":"<ul> <li>Set the environment variable <code>NAMESPACE</code> to <code>tekton-demo</code>, if you open a new terminal remember to set this environment again     <pre><code>export NAMESPACE=tekton-demo\n</code></pre></li> <li>Create a the namespace using the variable <code>NAMESPACE</code> <pre><code>kubectl create namespace $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#tasks_1","title":"Tasks","text":""},{"location":"labs/devops/tekton/#task-creation_1","title":"Task Creation","text":"<ul> <li>Create the below yaml files.</li> <li>The following snippet shows what a Tekton Task YAML looks like:</li> <li> <p>Create the file task-test.yaml <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: java-test\nspec:\nparams:\n    - name: url\n    - name: revision\n    default: master\nsteps:\n    - name: git-clone\n    image: alpine/git\n    script: |\n        git clone -b $(params.revision) --depth 1 $(params.url) /source\n    volumeMounts:\n        - name: source\n        mountPath: /source\n    - name: test\n    image: maven:3.3-jdk-8\n    workingdir: /source\n    script: |\n        mvn test\n        echo \"tests passed with rc=$?\"\n    volumeMounts:\n        - name: m2-repository\n        mountPath: /root/.m2\n        - name: source\n        mountPath: /source\nvolumes:\n    - name: m2-repository\n    emptyDir: {}\n    - name: source\n    emptyDir: {}\n</code></pre></p> </li> <li> <p>Each Task has the following:</p> </li> <li>name - the unique name using which the task can be referred<ul> <li>name - the name of the parameter</li> <li>description - the description of the parameter</li> <li>default - the default value of parameter</li> </ul> </li> <li> <p>Note: The <code>TaskRun</code> or <code>PipelineRun</code> could override the parameter values, if no parameter value is passed then the default value will be used.</p> </li> <li> <p>steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec</p> </li> <li>volumes - the task can also mount external volumes using the volumes attribute.</li> <li>The parameters that were part of the spec inputs params can be used in the steps using the notation <code>$(&lt;variable-name&gt;)</code>.</li> </ul>"},{"location":"labs/devops/tekton/#task-deploy_1","title":"Task Deploy","text":"<ul> <li> <p>The application test task could be created using the command:     <pre><code>kubectl apply -f task-test.yaml -n $NAMESPACE\n</code></pre></p> </li> <li> <p>We will use the Tekton cli to inspect the created resources     <pre><code>tkn task ls -n $NAMESPACE\n</code></pre></p> </li> <li> <p>The above command should list one Task as shown below:     <pre><code>NAME        AGE\njava-test   22 seconds ago\n</code></pre></p> </li> </ul>"},{"location":"labs/devops/tekton/#taskrun_1","title":"TaskRun","text":"<ul> <li>The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step</li> </ul>"},{"location":"labs/devops/tekton/#taskrun-creation_1","title":"TaskRun Creation","text":"<ul> <li>The following snippet shows what a Tekton TaskRun YAML looks like:</li> <li>Create the file taskrun-test.yaml <pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\ngenerateName: test-task-run-\nspec:\ntaskRef:\n    name: java-test\nparams:\n    - name: url\n    value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n</code></pre></li> <li>generateName - since the TaskRun can be run many times, in order to have unqiue name across the TaskRun ( helpful when checking the TaskRun history) we use this generateName instead of name. When Kubernetes sees generateName it will generate unquie set of characters and suffix the same to build-app-, similar to how pod names are generated</li> <li>taskRef - this is used to refer to the Task by its name that will be run as part of this TaskRun. In this example we use build-app Task.</li> <li>As described in the earlier section that the Task inputs and outputs could be overridden via TaskRun.</li> <li>params - this are the parameter values that are passed to the task</li> <li>The application test task(java-maven-test) could be run using the command:     <pre><code>kubectl create -n $NAMESPACE -f taskrun-test.yaml\n</code></pre></li> <li>Note - As tasks will use generated name, never use <code>kubectl apply -f taskrun-test.yaml</code></li> <li> <p>We will use the Tekton cli to inspect the created resources:     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     The above command should list one TaskRun as shown below:     <pre><code>NAME                       STARTED        DURATION   STATUS\ntest-task-run-q6s8c        1 minute ago   ---        Running(Pending)\n</code></pre> Note - It will take few seconds for the TaskRun to show status as Running as it needs to download the container images.</p> </li> <li> <p>To check the logs of the Task Run using the <code>tkn</code>: <pre><code>tkn tr logs -f -a -n $NAMESPACE\n</code></pre> Note - Each task step will be run within a container of its own. The -f or -a allows to tail the logs from all the containers of the task. For more options run <code>tkn tr logs --help</code></p> </li> <li>If you see the TaskRun status as Failed or Error use the following command to check the reason for error:     <pre><code>tkn tr describe --last -n $NAMESPACE\n</code></pre></li> <li>If it is successful, you will see something like below.     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     The above command should list one TaskRun as shown below:     <pre><code>NAME                  STARTED          DURATION     STATUS\ntest-task-run-q6s8c   47 seconds ago   34 seconds   Succeeded\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#creating-additional-tasks-and-deploying-them_1","title":"Creating additional tasks and deploying them","text":"<ul> <li>Create a Task to build a container image and push to the registry</li> <li>This task will be later used by the pipeline.</li> <li>Download the task file task-buildah.yaml to build the image, push the image to the registy:</li> <li>Create task buildah</li> <li>Create the <code>buildah</code> Task using the file and the command:     <pre><code>kubectl apply -f task-buildah.yaml -n $NAMESPACE\n</code></pre></li> <li>Use the Tekton cli to inspect the created resources     <pre><code>tkn task ls -n $NAMESPACE\n</code></pre></li> <li> <p>The above command should list one Task as shown below:     <pre><code>NAME              AGE\nbuildah            4 seconds ago\njava-test         46 minutes ago\n</code></pre></p> </li> <li> <p>To access the container registry, create the required secret as follows.</p> </li> <li>If using IBM Container registry use <code>iamapikey</code> for <code>REGISTRY_USERNAME</code> and get a API Key for <code>REGISTRY_PASSWORD</code>, use the domain name for the region IBM CR service like <code>us.icr.io</code></li> <li> <p>Create the environment variables to be use, replace with real values and include the single quotes:     <pre><code>export REGISTRY_USERNAME='&lt;REGISTRY_USERNAME&gt;'\n</code></pre> <pre><code>export REGISTRY_PASSWORD='&lt;REGISTRY_PASSWORD&gt;'\n</code></pre> <pre><code>export REGISTRY_SERVER='docker.io'\n</code></pre></p> </li> <li> <p>Run the following command to create a secret <code>regcred</code> in the namespace <code>NAMESPACE</code> <pre><code>kubectl create secret docker-registry regcred \\\n--docker-server=${REGISTRY_SERVER} \\\n--docker-username=${REGISTRY_USERNAME} \\\n--docker-password=${REGISTRY_PASSWORD} \\\n-n ${NAMESPACE}\n</code></pre></p> <p> Before creating, replace the values as mentioned above. Note: If your docker password contains special characters in it, please enclose the password in double quotes or place an escape character before each special character. </p> <ul> <li>(Optional) Only if you have problems with the credentials you can recreate it, but you have to deleted first <pre><code>kubectl delete secret regcred -n $NAMESPACE\n</code></pre></li> </ul> </li> <li> <p>Before we run the Task using TaskRun let us create the Kubernetes service account and attach the needed permissions to the service account, the following Kubernetes resource defines a service account called <code>pipeline</code> in namespace <code>$NAMESPACE</code> who will have administrative role within the <code>$NAMESPACE</code> namespace.</p> </li> <li>Create the file sa.yaml <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: pipeline\nsecrets:\n- name: regcred\n</code></pre></li> <li> <p>Create sa role as follows:     <pre><code>kubectl create -n $NAMESPACE -f sa.yaml\n</code></pre></p> </li> <li> <p>Create an environment variable for location to push the image to be build. Replace <code>NAMESPACE</code> for the dockerhub username, or IBM CR Namespace     <pre><code>export NAMESPACE='&lt;REGISTRY_NAMESPACE&gt;'\nexport IMAGE_URL=${REGISTRY_SERVER}/${REGISTRY_NAMESPACE}/cloudnative_sample_app\n</code></pre></p> </li> <li> <p>Lets create a Task Run for <code>buildah</code> Task using the <code>tkn</code> CLI passing the inputs, outputs and service account.     <pre><code>tkn task start buildah --showlog \\\n-p url=https://github.com/ibm-cloud-architecture/cloudnative_sample_app \\\n-p image=${IMAGE_URL} \\\n-s pipeline \\\n-n $NAMESPACE\n</code></pre></p> <p>The task will start and logs will start printing automatically <pre><code>Taskrun started: buildah-run-vvrg2\nWaiting for logs to be available...\n</code></pre></p> </li> <li> <p>Verify the status of the Task Run     <pre><code>tkn tr ls -n $NAMESPACE\n</code></pre>     Output should look like this     <pre><code>NAME                  STARTED          DURATION     STATUS\nbuildah-run-zbsrv      2 minutes ago    1 minute     Succeeded\n</code></pre></p> </li> <li>To clean up all Pods associated with all Task Runs, delete all the task runs resources     <pre><code>kubectl delete taskrun --all -n $NAMESPACE\n</code></pre></li> <li>(Optional) Instead of starting the Task via <code>tkn task start</code> you could also use yaml TaskRun, create a file taskrun-buildah.yaml Make sure update value for parameter <code>image</code> with your registry info.     <pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\ngenerateName: buildah-task-run-\nspec:\nserviceAccountName: pipeline\ntaskRef:\n    name: buildah\nparams:\n    - name: url\n    value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n    - name: image\n    value: docker.io/csantanapr/cloudnative_sample_app\n</code></pre>     Then create the TaskRun with <code>generateName</code> <pre><code>kubectl create -f taskrun-buildah.yaml -n $NAMESPACE\n</code></pre>     Follow the logs with:     <pre><code>tkn tr logs --last -f -n $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#pipelines_1","title":"Pipelines","text":""},{"location":"labs/devops/tekton/#pipeline-creation_1","title":"Pipeline Creation","text":"<ul> <li> <p>Pipelines allows to start multiple Tasks, in parallel or in a certain order</p> </li> <li> <p>Create the file pipeline.yaml, the Pipeline contains two Tasks     <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: test-build\nspec:\nparams:\n    - name: repo-url\n    default: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n    - name: revision\n    default: master\n    - name: image-server\n    - name: image-namespace\n    - name: image-repository\n    default: cloudnative_sample_app\ntasks:\n    - name: test\n    taskRef:\n        name: java-test\n    params:\n        - name: url\n        value: $(params.repo-url)\n        - name: revision\n        value: $(params.revision)\n    - name: build\n    runAfter: [test]\n    taskRef:\n        name: buildah\n    params:\n        - name: image\n        value: $(params.image-server)/$(params.image-namespace)/$(params.image-repository)\n        - name: url\n        value: $(params.repo-url)\n        - name: revision\n        value: $(params.revision)\n</code></pre></p> </li> <li> <p>Pipeline defines a list of Tasks to execute in order, while also indicating if any outputs should be used as inputs of a following Task by using the from field and also indicating the order of executing (using the runAfter and from fields). The same variable substitution you used in Tasks is also available in a Pipeline.</p> </li> <li>Create the Pipeline using the command: <pre><code>kubectl apply -f pipeline.yaml -n $NAMESPACE\n</code></pre></li> <li>Use the Tekton cli to inspect the created resources <pre><code>tkn pipeline ls -n $NAMESPACE\n</code></pre> The above command should list one Pipeline as shown below: <pre><code>NAME              AGE              LAST RUN   STARTED   DURATION   STATUS\ntest-build-push   31 seconds ago   ---        ---       ---        ---\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#pipelinerun_1","title":"PipelineRun","text":""},{"location":"labs/devops/tekton/#pipelinerun-creation_1","title":"PipelineRun Creation","text":"<ul> <li>To execute the Tasks in the Pipeline, you must create a PipelineRun. Creation of a PipelineRun will trigger the creation of TaskRuns for each Task in your pipeline.</li> <li>Create the file pipelinerun.yaml replace the values for <code>image-server</code> and <code>image-namespace</code> with your own.     <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: test-build-run-\nspec:\nserviceAccountName: pipeline\npipelineRef:\n    name: test-build\nparams:\n    - name: image-server\n    value: us.icr.io\n    - name: image-namespace\n    value: student01-registry\n</code></pre> serviceAccount - it is always recommended to have a service account associated with PipelineRun, which can then be used to define fine grained roles.     Replace the values for <code>image-server</code> and <code>image-namespace</code></li> <li>Create the PipelineRun using the command:     <pre><code>kubectl create -f pipelinerun.yaml -n $NAMESPACE\n</code></pre></li> <li> <p>We will use the Tekton cli to inspect the created resources     <pre><code>tkn pipelinerun ls -n $NAMESPACE\n</code></pre></p> </li> <li> <p>The above command should list one PipelineRun as shown below:     <pre><code>NAME                        STARTED         DURATION   STATUS\ntest-build-push-run-c7zgv   8 seconds ago   ---        Running\n</code></pre></p> </li> <li> <p>Get the logs of the pipeline using the following command     <pre><code>tkn pipelinerun logs --last -f\n</code></pre></p> </li> <li>Wait for few minutes for your pipeline to complete all the tasks. If it is successful, you will see something like below.     <pre><code>tkn pipeline ls -n $NAMESPACE\n</code></pre> <pre><code>NAME              AGE              LAST RUN                    STARTED         DURATION    STATUS\ntest-build-push   33 minutes ago   test-build-push-run-c7zgv   2 minutes ago   2 minutes   Succeeded\n</code></pre></li> <li> <p>Run again the pipeline ls command     <pre><code>tkn pipelinerun ls -n $NAMESPACE\n</code></pre> <pre><code>NAME                        STARTED         DURATION    STATUS\ntest-build-push-run-c7zgv   2 minutes ago   2 minutes   Succeeded\n</code></pre>     If it is successful, go to your container registry account and verify if you have the <code>cloudnative_sample_app</code> image pushed.</p> </li> <li> <p>(Optional) Run the pipeline again using the <code>tkn</code> CLI     <pre><code>tkn pipeline start test-build --last -n $NAMESPACE\n</code></pre></p> </li> <li>(Optional) Re-run the pipeline using last pipelinerun values     <pre><code>tkn pipeline start test-build-push --last -f -n $NAMESPACE\n</code></pre></li> </ul>"},{"location":"labs/devops/tekton/#deploy-application_1","title":"Deploy Application","text":"<ul> <li>Add the <code>imagePullSecret</code> to the <code>default</code> Service Account <pre><code>kubectl patch sa default -p '\"imagePullSecrets\": [{\"name\": \"regcred\" }]' -n $NAMESPACE\n</code></pre></li> <li>Create a deployment <pre><code>kubectl create deployment cloudnative --image=${IMAGE_URL} -n $NAMESPACE\n</code></pre></li> <li>Verify if the pods are running: <pre><code>kubectl get pods -l app=cloudnative -n $NAMESPACE\n</code></pre></li> <li>Expose the deployment <pre><code>kubectl expose deployment cloudnative --type=NodePort --port=9080 -n $NAMESPACE\n</code></pre></li> <li>Now access the compose the URL of the App using IP and NodePort     <pre><code>export APP_EXTERNAL_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}')\nexport APP_NODEPORT=$(kubectl get svc cloudnative -n $NAMESPACE -o jsonpath='{.spec.ports[0].nodePort}')\nexport APP_URL=\"http://${APP_EXTERNAL_IP}:${APP_NODEPORT}/greeting?name=Carlos\"\necho APP_URL=$APP_URL\n</code></pre> <pre><code>http://192.168.64.30:30632//greeting?name=Carlos\n</code></pre></li> <li>Now access the app from terminal or browser     <pre><code>curl $APP_URL\n</code></pre>     Output should be     <pre><code>{\"id\":4,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre> <pre><code>open $APP_URL\n</code></pre></li> </ul>"},{"location":"labs/kubernetes/lab-solutions/","title":"Lab Solutions","text":"<ul> <li> <p>Lab 1</p> </li> <li> <p>Lab 2</p> </li> <li> <p>Lab 3</p> </li> <li> <p>Lab 4</p> </li> <li> <p>Lab 5</p> </li> <li> <p>Lab 6</p> </li> <li> <p>Lab 7</p> </li> <li> <p>Lab 8</p> </li> <li> <p>Lab 9</p> </li> <li> <p>Lab 10</p> </li> </ul>"},{"location":"labs/kubernetes/ingress-iks/","title":"Kubernetes Lab Ingress Controller IBM Free Kubernetes cluster","text":"<p>The IBM Kubernetes service free clusters consist of a single worker node with 2 CPU and 4 GB of memory for experimenting with Kubernetes. Unlike the fee-based service, these clusters do not include capabilities for application load balancing using ingress out-of-the-box. </p>"},{"location":"labs/kubernetes/ingress-iks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Free IBM Kubernetes Cluster (IKS) - upgrade your account from Lite plan to create one. In the example commands, we'll assume that this cluster is named <code>mycluster</code></li> <li>kubectl - match your cluster API version </li> <li>Log in to IBM Cloud and configure <code>kubectl</code> using the <code>ibmcloud ks cluster config --cluster mycluster</code> command</li> </ul>"},{"location":"labs/kubernetes/ingress-iks/#components","title":"Components","text":"<p>On the IKS cluster, you will install helm charts for a nginx ingress controller from NGINX. This lab already provides the templated yaml files so there is no need to use helm cli.</p>"},{"location":"labs/kubernetes/ingress-iks/#set-up-the-ingress-controller","title":"Set up the ingress controller","text":"<p>Only do this on a free IKS instance These steps assume facts that only apply to free IKS instances:</p> <ul> <li>a single worker where the cluster administrator can create pods that bind to host ports</li> <li>no pre-existing ingress controller or application load balancer</li> </ul> <p>Using the following steps with a paid instance can cause issues. See the IBM Cloud containers documentation for information on exposing applications with the ingress/alb services for paid clusters. You have been warned</p> <ol> <li> <p>Install the NGINX ingress controller with <code>helm</code> using a daemonset and no service resource (which will result in a single pod that binds to ports 80 and 443 on the worker node and will skip creation of a <code>ClusterIP, LoadBalancer, or NodePort</code> for the daemonset).     <pre><code>kubectl apply -f https://cloudnative101.dev/yamls/ingress-controller/iks-ingress-v1.7.1.yaml\n</code></pre></p> </li> <li> <p>You can use free domain <code>.nip.io</code> to get a domain name using one of the IP Address of your worker nodes. Run this command to set your DOMAIN     <pre><code>export DOMAIN=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}').nip.io\necho $DOMAIN\n</code></pre></p> </li> <li> <p>You can test the ingress controller using the <code>$DOMAIN</code>:</p> <p><pre><code>curl -I http://$DOMAIN\n</code></pre> <pre><code>HTTP/1.1 404 Not Found\nServer: nginx/1.17.10\n...\n</code></pre></p> <p>A 404 is expected at this point because unlike the kubernetes nginx ingress, the NGINX version of the ingress controller does not create a default backend deployment.</p> </li> <li> <p>To use the ingress controller deploy a sample application, expose a service.     <pre><code>kubectl create deployment web --image=bitnami/nginx\nkubectl expose deployment web --name=web --port 8080\n</code></pre></p> </li> <li> <p>Now create an Ingress resource     <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  rules:\n    - host: web.$DOMAIN\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\nEOF\necho \"Access your web app at http://web.$DOMAIN\"\n</code></pre></p> </li> <li> <p>List the created ingress     <pre><code>kubectl get ingress web\n</code></pre></p> </li> <li> <p>Access your web application    <pre><code>curl http://web.$DOMAIN\n</code></pre>    The output prints the html    <pre><code>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n</code></pre></p> </li> <li> <p>Delete all the resources created     <pre><code>kubectl delete deployment,svc,ingress -l app=web\n</code></pre></p> </li> </ol>"},{"location":"labs/kubernetes/lab1/","title":"Kubernetes Lab 1 - Pod Creation","text":""},{"location":"labs/kubernetes/lab1/#problem","title":"Problem","text":"<ul> <li>Write a pod definition named <code>yoda-service-pod.yml</code> Then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specificationsof this pod are as follows:</p> <ul> <li>Use the <code>bitnami/nginx</code> container image.</li> <li>The container needs a containerPort of <code>80</code>.</li> <li>Set the command to run as <code>nginx</code></li> <li>Pass in the <code>-g daemon off; -q</code> args to run nginx in quiet mode.</li> <li>Create the pod in the <code>web</code> namespace.</li> </ul>"},{"location":"labs/kubernetes/lab1/#verification","title":"Verification","text":"<p>When you have completed this lab, use the following commands to validate your solution. The 'get pods' command will</p> <p><code>kubectl get pods -n web</code> <code>kubectl describe pod nginx -n web</code></p>"},{"location":"labs/kubernetes/lab1/solution/","title":"Kubernetes Lab 1 - Pod Creation","text":""},{"location":"labs/kubernetes/lab1/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: web\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    command: [\"nginx\"]\n    args: [\"-g\", \"daemon off;\", \"-q\"]\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"labs/kubernetes/lab10/","title":"Kubernetes Lab 10 - Network Policies","text":""},{"location":"labs/kubernetes/lab10/#problem","title":"Problem","text":"<p>Setup minikube</p> <pre><code>minikube start --network-plugin=cni\nkubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml\nkubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true\nkubectl -n kube-system get pods | grep calico-node\n</code></pre> <p>Create secured pod</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-secure-pod\n  labels:\n    app: secure-app\nspec:\n  containers:\n    - name: nginx\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <p>Create client pod</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-client-pod\nspec:\n  containers:\n    - name: busybox\n      image: radial/busyboxplus:curl\n      command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"]\n</code></pre> <p>Create a policy to allow only client pods with label <code>allow-access: \"true\"</code> to access secure pod</p>"},{"location":"labs/kubernetes/lab10/solution/","title":"Kubernetes Lab 10 - Network Policies","text":""},{"location":"labs/kubernetes/lab10/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: my-network-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: secure-app\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          allow-access: \"true\"\n</code></pre>"},{"location":"labs/kubernetes/lab2/","title":"Kubernetes Lab 2 - Probes","text":""},{"location":"labs/kubernetes/lab2/#container-health-issues","title":"Container Health Issues","text":"<p>The first issue is caused by application instances entering an unhealthy state and responding to user requests with error messages. Unfortunately, this state does not cause the container to stop, so the Kubernetes cluster is not able to detect this state and restart the container. Luckily, the application has an internal endpoint that can be used to detect whether or not it is healthy. This endpoint is <code>/healthz</code> on port <code>8080</code>.</p> <ul> <li>Your first task will be to create a probe to check this endpoint periodically.</li> <li>If the endpoint returns an error or fails to respond, the probe will detect this and the cluster will restart the container.</li> </ul>"},{"location":"labs/kubernetes/lab2/#container-startup-issues","title":"Container Startup Issues","text":"<p>Another issue is caused by new pods when they are starting up. The application takes a few seconds after startup before it is ready to service requests. As a result, some users are getting error message during this brief time.</p> <ul> <li> <p>To fix this, you will need to create another probe. To detect whether the application is <code>ready</code>, the probe should simply make a request to the root endpoint, <code>/ready</code>, on port <code>8080</code>. If this request succeeds, then the application is ready.</p> </li> <li> <p>Also set a <code>initial delay</code> of <code>5 seconds</code> for the probes.</p> </li> </ul> <p>Here is the Pod yaml file, add the probes, then create the pod in the cluster to test it.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: energy-shield-service\nspec:\n  containers:\n    - name: energy-shield\n      image: ibmcase/energy-shield:1\n</code></pre>"},{"location":"labs/kubernetes/lab2/solution/","title":"Kubernetes Lab 2 - Probes","text":""},{"location":"labs/kubernetes/lab2/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: energy-shield-service\nspec:\n  containers:\n  - name: energy-shield\n    image: ibmcase/energy-shield:1\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 5\n</code></pre>"},{"location":"labs/kubernetes/lab3/","title":"Kubernetes Lab 3 - Debugging","text":""},{"location":"labs/kubernetes/lab3/#problem","title":"Problem","text":"<p>The Hyper Drive isn't working and we need to find out why. Let's debug the <code>hyper-drive</code> deployment so that we can reach light speed again.</p> <p>Here are some tips to help you solve the Hyper Drive:</p> <ul> <li>Check the description of the <code>deployment</code>.</li> <li>Get and save the logs of one of the broken <code>pods</code>.</li> <li>Are the correct <code>ports</code> assigned.</li> <li>Make sure your <code>labels</code> and <code>selectors</code> are correct.</li> <li>Check to see if the <code>Probes</code> are correctly working.</li> <li>To fix the deployment, save then modify the yaml file for redeployment.</li> </ul> <p>Reset the environment:</p> <pre><code>minikube delete\nminikube start\n</code></pre> <p>Setup the environment:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/lab-setup/lab-5-debug-k8s-setup.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab3/#validate","title":"Validate","text":"<p>Once you get the Hyper Drive working again. Verify it by checking the endpoints.</p> <pre><code>kubectl get ep hyper-drive\n</code></pre>"},{"location":"labs/kubernetes/lab3/solution/","title":"Kubernetes Lab 3 - Debugging","text":""},{"location":"labs/kubernetes/lab3/solution/#solution","title":"Solution","text":"<p>Check <code>STATUS</code> column for not Ready <pre><code>    kubectl get pods --all-namespaces\n</code></pre></p> <p>Check the description of the deployment     <pre><code>kubectl describe deployment hyper-drive\n</code></pre>    Save logs for a broken pod</p> <pre><code>kubectl logs &lt;pod name&gt; -n &lt;namespace&gt; &gt; /home/cloud_user/debug/broken-pod-logs.log\n</code></pre> <p>In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080.</p> <p>To fix probe, can't kubectl edit, need to delete and recreate the deployment     <pre><code>kubectl get deployment &lt;deployment name&gt; -n &lt;namespace&gt; -o yaml --export &gt; hyper-drive.yml\n</code></pre></p> <p>Delete pod     <pre><code>kubectl delete deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre>    Can also use <code>kubectl replace</code></p> <p>Edit yaml, and apply     <pre><code>kubectl apply -f hyper-drive.yml -n &lt;namespace&gt;\n</code></pre></p> <p>Verify     <pre><code>kubectl get deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"labs/kubernetes/lab4/","title":"Kubernetes Lab 4 - Manage Multiple Containers","text":""},{"location":"labs/kubernetes/lab4/#problem","title":"Problem","text":"<p>This service has already been packaged into a container image, but there is one special requirement:</p> <ul> <li>The legacy app is hard-coded to only serve content on port <code>8989</code>, but the team wants to be able to access the service using the standard port <code>80</code>.</li> </ul> <p>Your task is to build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port <code>80</code>.</p> <p>This setup will need to meet the following specifications:</p> <ul> <li>The pod should have the name <code>vader-service</code>.</li> <li>The <code>vader-service</code> pod should have a container that runs the legacy vader service image: <code>ibmcase/millennium-falcon:1</code>.</li> <li>The <code>vader-service</code> pod should have an ambassador container that runs the <code>haproxy:1.7</code> image and proxies incoming traffic on port <code>80</code> to the legacy service on port <code>8989</code> (the HAProxy configuration for this is provided below).</li> <li>Port <code>80</code> should be exposed as a <code>containerPort</code>.</li> </ul> <p> <p>Note: You do not need to expose port 8989</p> <p></p> <ul> <li>The HAProxy configuration should be stored in a ConfigMap called <code>vader-service-ambassador-config</code>.</li> <li>The HAProxy config should be provided to the ambassador container using a volume mount that places the data from the ConfigMap in a file at /usr/local/etc/haproxy/haproxy.cfg.   haproxy.cfg should contain the following configuration data:</li> </ul> <pre><code>global\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nlisten http-in\n    bind *:80\n    server server1 127.0.0.1:8989 maxconn 32\n</code></pre> <p>Once your pod is up and running, it's a good idea to test it to make sure you can access the service from within the cluster using port 80. In order to do this, you can create a busybox pod in the cluster, and then run a command to attempt to access the service from within the busybox pod.</p> <p>Create a descriptor for the busybox pod called <code>busybox.yml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n    - name: myapp-container\n      image: radial/busyboxplus:curl\n      command: [\"sh\", \"-c\", \"while true; do sleep 3600; done\"]\n</code></pre> <p>Create the busybox testing pod.</p> <pre><code>kubectl apply -f busybox.yml\n</code></pre> <p>Use this command to access <code>vader-service</code> using port 80 from within the busybox pod.</p> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:.status.podIP --no-headers):80\n</code></pre> <p>If the service is working, you should get a message that the hyper drive of the millennium falcon needs repair.</p> <p>Relevant Documentation:</p> <ul> <li>Kubernetes Sidecar Logging Agent</li> <li>Shared Volumes</li> <li>Distributed System Toolkit Patterns</li> </ul>"},{"location":"labs/kubernetes/lab4/solution/","title":"Kubernetes Lab 4 - Manage Multiple Containers","text":""},{"location":"labs/kubernetes/lab4/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vader-service-ambassador-config\ndata:\n  haproxy.cfg: |-\n    global\n        daemon\n        maxconn 256\n\n    defaults\n        mode http\n        timeout connect 5000ms\n        timeout client 50000ms\n        timeout server 50000ms\n\n    listen http-in\n        bind *:80\n        server server1 127.0.0.1:8775 maxconn 32\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: vader-service\nspec:\n  containers:\n  - name: millennium-falcon\n    image: ibmcase/millennium-falcon:1\n  - name: haproxy-ambassador\n    image: haproxy:1.7\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /usr/local/etc/haproxy\n  volumes:\n  - name: config-volume\n    configMap:\n      name: vader-service-ambassador-config\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: radial/busyboxplus:curl\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n</code></pre> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=jsonpath='{.status.podIP}'):80\n</code></pre>"},{"location":"labs/kubernetes/lab5/","title":"Kubernetes Lab 5 - Persistent Volumes","text":""},{"location":"labs/kubernetes/lab5/#problem","title":"Problem","text":"<p>The death star plans can't be lost no matter what happens so we need to make sure we protect them at all costs.</p> <p>In order to do that you will need to do the following:</p> <p>Create a <code>PersistentVolume</code>:</p> <ul> <li> <p>The PersistentVolume should be named <code>postgresql-pv</code>.</p> </li> <li> <p>The volume needs a capacity of <code>1Gi</code>.</p> </li> <li> <p>Use a storageClassName of <code>localdisk</code>.</p> </li> <li> <p>Use the accessMode <code>ReadWriteOnce</code>.</p> </li> <li> <p>Store the data locally on the node using a <code>hostPath</code> volume at the location <code>/mnt/data</code>.</p> </li> </ul> <p>Create a <code>PersistentVolumeClaim</code>:</p> <ul> <li> <p>The PersistentVolumeClaim should be named <code>postgresql-pv-claim</code>.</p> </li> <li> <p>Set a resource request on the claim for <code>500Mi</code> of storage.</p> </li> <li> <p>Use the same storageClassName and accessModes as the PersistentVolume so that this claim can bind to the PersistentVolume.</p> </li> </ul> <p>Create a <code>Postgresql</code> Pod configured to use the <code>PersistentVolumeClaim</code>:</p> <ul> <li> <p>The Pod should be named <code>postgresql-pod</code>.</p> </li> <li> <p>Use the image <code>bitnami/postgresql</code>.</p> </li> <li> <p>Expose the containerPort <code>5432</code>.</p> </li> <li> <p>Set an <code>environment variable</code> called <code>MYSQL_ROOT_PASSWORD</code> with the value <code>password</code>.</p> </li> <li> <p>Add the <code>PersistentVolumeClaim</code> as a volume and mount it to the container at the path <code>/bitnami/postgresql/</code>.</p> </li> </ul>"},{"location":"labs/kubernetes/lab5/solution/","title":"Kubernetes Lab 5 - Persistent Volumes","text":""},{"location":"labs/kubernetes/lab5/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgresql-pv\nspec:\n  storageClassName: localdisk\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgresql-pv-claim\nspec:\n  storageClassName: localdisk\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgresql-pod\nspec:\n  containers:\n  - name: postgresql\n    image: bitnami/postgresql\n    ports:\n    - containerPort: 5432\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: password\n    volumeMounts:\n    - name: sql-storage\n      mountPath: /bitnami/postgresql/\n  volumes:\n  - name: sql-storage\n    persistentVolumeClaim:\n      claimName: postgresql-pv-claim\n</code></pre> <p>verify via <code>ls /mnt/data</code> on node</p>"},{"location":"labs/kubernetes/lab6/","title":"Kubernetes Lab 6 - Pod Configuration","text":""},{"location":"labs/kubernetes/lab6/#problem","title":"Problem","text":"<ul> <li>Create a pod definition named <code>yoda-service-pod.yml</code>, and then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specifications are as follows:</p> <ul> <li>The current image for the container is <code>bitnami/nginx</code>. You do not need a custom command or args.</li> <li>There is some configuration data the container will need:</li> <li><code>yoda.baby.power=100000000</code></li> <li><code>yoda.strength=10</code></li> <li>It will expect to find this data in a file at <code>/etc/yoda-service/yoda.cfg</code>. Store the configuration data in a ConfigMap called <code>yoda-service-config</code> and provide it to the container as a mounted volume.</li> <li>The container should expect to use <code>64Mi</code> of memory and <code>250m</code> CPU (use resource requests).</li> <li>The container should be limited to <code>128Mi</code> of memory and <code>500m</code> CPU (use resource limits).</li> <li>The container needs access to a database password in order to authenticate with a backend database server. The password is <code>0penSh1ftRul3s!</code>. It should be stored as a Kubernetes secret called <code>yoda-db-password</code> and passed to the container as an environment variable called <code>DB_PASSWORD</code>.</li> <li>The container will need to access the Kubernetes API using the ServiceAccount <code>yoda-svc</code>. Create the service account if it doesn't already exist, and configure the pod to use it.</li> </ul>"},{"location":"labs/kubernetes/lab6/#verification","title":"Verification","text":"<p>To verify your setup is complete, check <code>/etc/yoda-service</code> for the <code>yoda.cfg</code> file and use the <code>cat</code> command to check it's contents.</p> <pre><code>kubectl exec -it yoda-service /bin/bash\ncd /etc/yoda-service\ncat yoda.cfg\n</code></pre>"},{"location":"labs/kubernetes/lab6/solution/","title":"Kubernetes Lab 6 - Pod Configuration","text":""},{"location":"labs/kubernetes/lab6/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: yoda-service-config\ndata:\n  yoda.cfg: |-\n    yoda.baby.power=100000000\n    yoda.strength=10\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: yoda-svc\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: yoda-db-password\nstringData:\n  password: 0penSh1ftRul3s!\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: yoda-service\nspec:\n  serviceAccountName: yoda-svc\n  containers:\n  - name: yoda-service\n    image: bitnami/nginx\n    volumeMounts:\n      - name: config-volume\n        mountPath: /etc/yoda-service\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: yoda-db-password\n          key: password\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n  volumes:\n  - name: config-volume\n    configMap:\n      name: yoda-service-config\n</code></pre>"},{"location":"labs/kubernetes/lab7/","title":"Kubernetes Lab 7 - Rolling Updates","text":""},{"location":"labs/kubernetes/lab7/#problem","title":"Problem","text":"<p>Your company's developers have just finished developing a new version of their jedi-themed mobile game. They are ready to update the backend services that are running in your Kubernetes cluster. There is a deployment in the cluster managing the replicas for this application. The deployment is called <code>jedi-deployment</code>. You have been asked to update the image for the container named <code>jedi-ws</code> in this deployment template to a new version, <code>bitnamy/nginx:1.18.1</code>.</p> <p>After you have updated the image using a rolling update, check on the status of the update to make sure it is working. If it is not working, perform a rollback to the previous state.</p> <p>Setup environment</p> <pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-6-rolling-updates-setup.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/","title":"Kubernetes Lab 7 - Rolling Updates","text":""},{"location":"labs/kubernetes/lab7/solution/#solution","title":"Solution","text":"<p>Update the deployment to the new version like so: <pre><code>kubectl set image deployment/jedi-deployment jedi-ws=bitnamy/nginx:1.18.1 --record\n</code></pre></p> <p>Check the progress of the rolling update: <pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre></p> <p>In another terminal window <pre><code>kubectl get pods -w\n</code></pre></p> <p>Get a list of previous revisions. <pre><code>kubectl rollout history deployment/jedi-deployment\n</code></pre></p> <p>Undo the last revision. <pre><code>kubectl rollout undo deployment/jedi-deployment\n</code></pre></p> <p>Check the status of the rollout. <pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre></p>"},{"location":"labs/kubernetes/lab8/","title":"Kubernetes Lab 8 - Cron Jobs","text":""},{"location":"labs/kubernetes/lab8/#problem","title":"Problem","text":"<p>Your commander has a simple data process that is run periodically to check status. They would like to stop doing this manually in order to save time, so you have been asked to implement a cron job in the Kubernetes cluster to run this process.</p> <ul> <li>Create a cron job called xwing-cronjob using the <code>ibmcase/xwing-status:1.0</code> image.</li> <li>Have the job run every second minute with the following cron expression: <code>*/2 * * * *</code>.</li> <li>Pass the argument <code>/usr/sbin/xwing-status.sh</code> to the container.</li> </ul>"},{"location":"labs/kubernetes/lab8/#verification","title":"Verification","text":"<ul> <li>Run <code>kubectl get cronjobs.batch</code> and <code>LAST-SCHEDULE</code> to see last time it ran</li> <li>From a bash shell, run the following to see the logs for all jobs:</li> </ul> <pre><code>jobs=( $(kubectl get jobs --no-headers -o custom-columns=\":metadata.name\") )\necho -e \"Job \\t\\t\\t\\t Pod \\t\\t\\t\\t\\tLog\"\nfor job in \"${jobs[@]}\"\ndo\n   pod=$(kubectl get pods -l job-name=$job --no-headers -o custom-columns=\":metadata.name\")\n   echo -en \"$job \\t $pod \\t\"\n   kubectl logs $pod\ndone\n</code></pre>"},{"location":"labs/kubernetes/lab8/solution/","title":"Kubernetes Lab 9 - Cron Jobs","text":""},{"location":"labs/kubernetes/lab8/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: xwing-cronjob\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: xwing-status\n            image: ibmcase/xwing-status:1.0\n            args:\n            - /usr/sbin/xwing-status.sh\n          restartPolicy: OnFailure\n</code></pre> <pre><code>kubectl get cronjob xwing-cronjob\n</code></pre>"},{"location":"labs/kubernetes/lab9/","title":"Kubernetes Lab 9 - Services","text":""},{"location":"labs/kubernetes/lab9/#problem","title":"Problem","text":"<p>We have a <code>jedi-deployment</code> and <code>yoda-deployment</code> that need to communicate with others. The <code>jedi</code> needs to talk to the world(outside the cluster), while <code>yoda</code> only needs to talk to jedi council(others in the cluster).</p>"},{"location":"labs/kubernetes/lab9/#your-task","title":"Your Task","text":"<ul> <li>Examine the two deployments, and create two services that meet the following criteria:</li> </ul> <p>jedi-svc</p> <ul> <li>The service name is <code>jedi-svc</code>.</li> <li>The service exposes the pod replicas managed by the deployment named <code>jedi-deployment</code>.</li> <li>The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.</li> <li>The service type is <code>NodePort</code>.</li> </ul> <p>yoda-svc</p> <ul> <li>The service name is <code>yoda-svc</code>.</li> <li>The service exposes the pod replicas managed by the deployment named <code>yoda-deployment</code>.</li> <li>The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.</li> <li>The service type is <code>ClusterIP</code>.</li> </ul>"},{"location":"labs/kubernetes/lab9/#setup-environment","title":"Setup environment:","text":"<pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-8-service-setup.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab9/solution/","title":"Kubernetes Lab 9 - Services","text":""},{"location":"labs/kubernetes/lab9/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jedi-svc\nspec:\n  type: NodePort\n  selector:\n    app: jedi\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: yoda-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: yoda\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"openshift/","title":"What is Container Orchestration?","text":""},{"location":"openshift/#introduction","title":"Introduction","text":"<p>Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications. Learn how Kubernetes enables cost-effective cloud native development.</p>"},{"location":"openshift/#what-is-openshift","title":"What is OpenShift?","text":"<p>Red Hat OpenShift is an open-source container platform that runs on the Red Hat enterprise Linux operating system and Kubernetes. It is typically referred to as a \"Platform as a Service\" (PaaS) due to its combination of services for enterprise businesses, including the Kubernetes platform and Docker container images. OpenShift offers additional features exclusive to its enterprise platform. It allows for deploying apps on highly available clusters and securing hybrid workloads with developer-focused tools and seamless integration of IBM, CloudPak, and Red Hat content.</p> <p>Red Hat OpenShift provides a uniform platform across public and private clouds for full portability, standardization, and ease of adoption. It offers various forms to meet customer needs, such as Red Hat OpenShift Container Platform (OCP), Red Hat OpenShift Dedicated (OSD), Microsoft Azure Red Hat OpenShift, and Red Hat OpenShift Online.</p> <p>In summary, Red Hat OpenShift is an enterprise-ready Kubernetes container platform with full-stack automated operations for managing hybrid cloud and multi-cloud deployments. It offers multiple offerings to cater to diverse customer requirements and ensures a consistent experience across public and private clouds.</p>"},{"location":"openshift/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes\u2014also known as \u2018k8s\u2019 or \u2018kube\u2019\u2014is a container orchestration platform for scheduling and automating the deployment, management, and scaling of containerized applications.</p> <p>Kubernetes was first developed by engineers at Google before being open sourced in 2014. It is a descendant of \u2018Borg,\u2019 a container orchestration platform used internally at Google. (Kubernetes is Greek for helmsman or pilot, hence the helm in the Kubernetes logo.)</p> <p>Today, Kubernetes and the broader container ecosystem are maturing into a general-purpose computing platform and ecosystem that rivals\u2014if not surpasses\u2014virtual machines (VMs) as the basic building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver a high-productivity Platform-as-a-Service (PaaS) that addresses multiple infrastructure- and operations-related tasks and issues surrounding cloud native development so that development teams can focus solely on coding and innovation.     </p> <ul> <li> <p> Learning Kubernetes</p> <p>Learning Kubernetes through IBM Learning</p> <p> Getting started</p> </li> </ul>"},{"location":"openshift/#presentations","title":"Presentations","text":"<p>Kubernetes Overview </p>"},{"location":"openshift/#predictable-demands-pattern","title":"Predictable Demands Pattern","text":"<p>An application's performance, efficiency, and behaviors are reliant upon it's ability to have the appropriate allocation of resources.  The Predictable Demands pattern is based on declaring the dependencies and resources needed by a given application.  The scheduler will prioritize an application with a defined set of resources and dependencies since it can better manage the workload across nodes in the cluster.  Each application has a different set of dependencies which we will touch on next.</p>"},{"location":"openshift/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>One of the most common runtime dependency's is the exposure of a container's specific port through hostPort.  Different applications can specify the same port through hostPort which reserves the port on each node in the cluster for the specific container.  This declaration restricts multiple continers with the same hostPort to be deployed on the same nodes in the cluster and restricts the scale of pods to the number of nodes you have in the cluster.  </p> <p>Another runtime dependency is file storage for saving the application state.  Kubernetes offers Pod-level storage utilities that are capable of surviving container restarts.  Applications needing to read or write to these storage mechanisms will require nodes that is provided the type of volume required by the application.  If there is no nodes available with the required volume type, then the pod will not be scheduled to be deployed at all.</p> <p>A different kind of dependency is configurations.  ConfigMaps are used by Kubernetes to strategically plan out how to consume it's settings through either environment variables or the filesystem.  Secrets are consumed the same was as a ConfigMap in Kubernetes.  Secrets are a more secure way to distribute environment-specific configurations to containers within the pod. </p>"},{"location":"openshift/#resource-profiles","title":"Resource Profiles","text":"<p>Resource Profiles are definitions for the compute resources required for a container.  Resources are categorized in two ways, compressible and incompressible.  Compressible resources include resources that can be throttled such as CPU or network bandwidth. Incompressible represents resouces that can't be throttled such as memory where there is no other way to release the allocated resource other than killing the container.  The difference between compressible and incompressible is very important when it comes to planning the deployment of pods and containers since the resource allocation can be affected by the limits of each.</p> <p>Every application needs to have a specified minimum and maximum amount of resources that are needed.  The minimum amount is called \"requests\" and the maximum is the \"limits\".  The scheduler uses the requests to determine the assignment of pods to nodes ensuring that the node will have enough capacity to accommodate the pod and all of it's containers required resources.  An example of defined resource limits is below:</p> <p>Different levels of Quality of Service (QoS) are offered based on the specified requests and limits.</p> <ol> <li>Quality of Service Levels Best Effort;;     Lowest priority pod with no requests or limits set for it's containers. These pods will be the first of any pods killed if resources run low. Burstable;;     Limits and requests are defined but they are not equal.  The pod will use the minimum amount of resources, but will consume more if needed up to the limit.  If the needed resources become scarce then these pods will be killed if no Best Effort pods are left. Guaranteed;;     Highest priority pods with an equal amount of requests and limits. These pods will be the last to be killed if resources run low and no Best Effort or Burstable pods are left. </li> </ol>"},{"location":"openshift/#pod-priority","title":"Pod Priority","text":"<p>The priority of pods can be defined through a PriorityClass object. The PriorityClass object allows developers to indicate the importance of a pod relative to the other pods in the cluster.  The higher the priority number then the higher the priority of the pod. The scheduler looks at a pods priorityClassName to populate the priority of new pods.  As pods are being placed in the scheduling queue for deployment, the scheduler orders them from highest to lowest.</p> <p>Another key feature for pod priority is the Preemption feature.  The Preemption feature occurs when there are no nodes with enough capacity to place a pod.  If this occurs the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority.  This effectively allows system administrators the ability to control which critical pods get top priority for resources in the cluster as well as controlling which critical workloads are able to be run on the cluster first. If a pod can not be scheduled due to constraints it will continue on with lower-priority nodes.</p> <p>Pod Priority should be used with caution for this gives users the ability to control over the kubernetes scheduler and ability to place or kill pods that may interrupt the cluster's critical functions.  New pods with higher priority than others can quickly evict pods with lower priority that may be critical to a container's performance.  ResourceQuota and PodDisruptionBudget are two tools that help combat this from happening read more here.</p>"},{"location":"openshift/#benefits-of-container-orchestration","title":"Benefits of Container Orchestration","text":"<p>...</p>"},{"location":"openshift/orchestrationConcepts/","title":"Concepts in Container Orchestration","text":""},{"location":"openshift/orchestrationConcepts/#declarative-deployment-pattern","title":"Declarative Deployment Pattern","text":"<p>With a growing number of microservices, reliance on an updating process for the services has become ever more important. Upgrading services is usually accompanied with some downtime for users or an increase in resource usage.  Both of these can lead to an error effecting the performance of the application making the release process a bottleneck.  </p> <p>A way to combat this issue in Kubernetes is through the use of Deployments.  There are different approaches to the updating process that we will cover below. Any of these approaches can be put to use in order to save time for developers during their release cycles which can last from a few minutes to a few months. </p>"},{"location":"openshift/orchestrationConcepts/#rolling-deployment","title":"Rolling Deployment","text":"<p>A Rolling Deployment ensures that there is no downtime during the update process.  Kubernetes creates a new ReplicaSet for the new version of the service to be rolled out.  From there Kubernetes creates set of pods of the new version while leaving the old pods running.  Once the new pods are all up and running they will replace the old pods and become the primary pods users access.</p> <p></p> <p>The upside to this approach is that there is no downtime and the deployment is handled by kubernetes through a deployment like the one below. The downside is with two sets of pods running at one time there is a higher usage of resources that may lead to performance issues for users. </p>"},{"location":"openshift/orchestrationConcepts/#fixed-deployment","title":"Fixed Deployment","text":"<p>A Fixed Deployment uses the Recreate strategy which sets the maxUnavailable setting to the number of declared replicas.  This in effect starts the versions of the pods as the old versions are being killed.  The starting and stopping of containers does create a little bit of downtime for customers while the starting and stopping is taking place, but the positive side is the users will only have to handle one version at a time.</p> <p></p>"},{"location":"openshift/orchestrationConcepts/#blue-green-release","title":"Blue-Green Release","text":"<p>A Blue-Green Release involves a manual process of creating a second deployment of pods with the newest version of the application running as well as keeping the old version of pods running in the cluster.  Once the new pods are up and running properly the administrator shifts the traffic over to the new pods. Below is a diagram showing both versions up and running with the traffic going to the newer (green) pods.</p> <p></p> <p>The downfall to this approach is the use of resources with two separate groups of pods running at the same time which could cause performance issues or complications. However, the advantage of this approach is users only experience one version at a time and it's easy to quickly switch back to the old version with no downtime if an issue arises with the newer version.</p>"},{"location":"openshift/orchestrationConcepts/#canary-release","title":"Canary Release","text":"<p>A Canary Release involves only standing up one pod of the new application code and shifting only a limited amount of new users traffic to that pod.  This approach reduces the number of people exposed to the new service allowing the administrator to see how the new version is performing.  Once the team feels comfortable with the performance of the new service then more pods can be stood up to replace the old pods.  An advantage to this approach is no downtime with any of the services as the new service is being scaled. </p> <p></p>"},{"location":"openshift/orchestrationConcepts/#health-probe-pattern","title":"Health Probe Pattern","text":"<p>The Health Probe pattern revolves the health of applications being communicated to Kubernetes. To be fully-automatable, cloud-applications must be highly observable in order for Kubernetes to know which applications are up and ready to receive traffic and which cannot. Kubernetes can use that information for traffic direction, self-healing, and to achieve the desired state of the application.</p>"},{"location":"openshift/orchestrationConcepts/#process-health-checks","title":"Process Health Checks","text":"<p>The simplest health check in kubernetes is the Process Health Check.  Kubernetes simply probes the application's processes to see if they are running or not. The process check tells kubernetes when a process for an application needs to be restarted or shut down in the case of a failure.</p>"},{"location":"openshift/orchestrationConcepts/#liveness-probes","title":"Liveness Probes","text":"<p>A Liveness Probe is performed by the Kubernetes Kubelet agent and asks the container to confirm it's health.  A simple process check can return that the container is healthy, but the container to users may not be performing correctly.  The liveness probe addresses this issue but asking the container for its health from outside of the container itself. If a failure is found it may require that the container be restarted to get back to normal health.  A liveness probe can perform the following actions to check health:</p> <ul> <li>HTTP GET and expects a success which is code 200-399.</li> <li>A TCP Socket Probe and expects a successful connection.</li> <li>A Exec Probe which executes a command and expects a successful exit code (0).</li> </ul> <p>The action chosen to be performed for testing depends on the nature of the application and which action fits best. Always keep in mind that a failing health check results in a restart of the container from Kubernetes, so make sure the right health check is in place if the underlying issue can't be fixed.</p>"},{"location":"openshift/orchestrationConcepts/#readiness-probes","title":"Readiness Probes","text":"<p>A Readiness Probe is very similar to a Liveness probe, but the resulting action to a failed Readiness probe is different.  When a liveness probe fails the container is restarted and, in some scenarios, a simple restart won't fix the issue, which is where a readiness probe comes in.  A failed readiness probe won't restart the container but will disconnect it from the traffic endpoint.  Removing a container from traffic allows it to get up and running smoothly before being tossed into service unready to handle requests from users.  Readiness probes give an application time to catch up and make itself ready again to handle more traffic versus shutting down completely and simply creating a new pod. In most cases, liveness and readiness probes are run together on the same application to make sure that the container has time to get up and running properly as well as stays healthy enough to handle the traffic. </p>"},{"location":"openshift/orchestrationConcepts/#managed-lifecycle-pattern","title":"Managed Lifecycle Pattern","text":"<p>The Managed Lifecycle pattern describes how containers need to adapt their lifecycles based on the events that are communicated from a managing platform such as Kubernetes.  Containers do not have control of their own lifecycles.  It's the managing platforms that allow them to live or die, get traffic or have none, etc.  This pattern covers how the different events can affect those lifecycle decisions.</p>"},{"location":"openshift/orchestrationConcepts/#sigterm","title":"SIGTERM","text":"<p>The SIGTERM is a signal that is sent from the managing platform to a container or pod that instructs the pod or container to shutdown or restart.  This signal can be sent due to a failed liveness test or a failure inside the container.  SIGKILL allows the container to cleaning and properly shut itself down versus SIGKILL, which we will get to next. Once received, the application will shutdown as quickly as it can, allowing other processes to stop properly and cleaning up other files.  Each application will have a different shutdown time based on the tasks needed to be done.</p>"},{"location":"openshift/orchestrationConcepts/#sigkill","title":"SIGKILL","text":"<p>SIGKILL is a signal sent to a container or pod forcing it to shutdown.  A SIGKILL is normally sent after the SIGTERM signal.  There is a default 30 second grace period between the time that SIGTERM is sent to the application and SIGKILL is sent.  The grace period can be adjusted for each pod using the .spec.terminationGracePeriodSeconds field. The overall goal for containerized applications should be aimed to have designed and implemented quick startup and shutdown operations.</p>"},{"location":"openshift/orchestrationConcepts/#poststart","title":"postStart","text":"<p>The postStart hook is a command that is run after the creation of a container and begins asynchronously with the container's primary process. PostStart is put in place in order to give the container time to warm up and check itself during startup.  During the postStart loop the container will be labeled in \"pending\" mode in kubernetes while running through it's initial processes.  If the postStart function errors out it will do so with a nonzero exit code and the container process will be killed by Kubernetes.  Careful planning must be done when deciding what logic goes into the postStart function because if it fails the container will also fail to start.  Both postStart and preStop have two handler types that they run:</p> <ul> <li> <p>exec: Runs a command directly in the container.</p> </li> <li> <p>httpGet: Executes an HTTP GET request against an opened port on the pod container.</p> </li> </ul>"},{"location":"openshift/orchestrationConcepts/#prestop","title":"preStop","text":"<p>The preStop hook is a call that blocks a container from terminating too quickly and makes sure the container has a graceful shutdown.  The preStop call must finish before the container is deleted by the container runtime.  The preStop signal does not stop the container from being deleted completely, it is only an alternative to a SIGTERM signal for a graceful shutdown. </p>"},{"location":"openshift/configuration/","title":"Container Configuration","text":""},{"location":"openshift/configuration/#command-and-argument","title":"Command and Argument","text":"<p>When you create a Pod, you can define a command and arguments for the containers that run in the Pod.</p> <p>The command and arguments that you define in the configuration file override the default command and arguments provided by the container image</p> <ul> <li>Dockerfile vs Kubernetes</li> <li>Dockerfile Entrypoint -&gt; k8s command</li> <li>Dockerfile CMD -&gt; k8s args</li> </ul>"},{"location":"openshift/configuration/#ports","title":"Ports","text":"<p>When you create a Pod, you can specify the port number the container exposes, as best practice is good to put a <code>name</code>, this way a service can specify targetport by name reference.</p>"},{"location":"openshift/configuration/#environment-variable","title":"Environment Variable","text":"<p>When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the container configuration</p> <p>A Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields</p>"},{"location":"openshift/configuration/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Container Commands </p> <p>Environment Variables </p> <p>Pod Exposing </p>"},{"location":"openshift/configuration/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cmd-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n  restartPolicy: Never\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-arg-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n      args: [\"Hello World\"]\n  restartPolicy: Never\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-port-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-env-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: c\n      image: busybox\n      env:\n        - name: DEMO_GREETING\n          value: \"Hello from the environment\"\n      command: [\"echo\"]\n      args: [\"$(DEMO_GREETING)\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-inter-pod\n  labels:\n    app: jedi\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n          name: http\n      env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n      command: [\"echo\"]\n      args: [\"$(MY_NODE_NAME) $(MY_POD_NAME) $(MY_POD_IP)\"]\n</code></pre>"},{"location":"openshift/configuration/#resource-requirements","title":"Resource Requirements","text":"<p>When you specify a Pod, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on.</p> <p>CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes.</p>"},{"location":"openshift/configuration/#resources_1","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Compute Resources </p> <p>Memory Management </p>"},{"location":"openshift/configuration/#references_1","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n</code></pre> <p>Namespaced defaults mem</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\nspec:\n  limits:\n    - default:\n        memory: 512Mi\n      defaultRequest:\n        memory: 256Mi\n      type: Container\n</code></pre> <p>Namespaced defaults mem</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-limit-range\nspec:\n  limits:\n    - default:\n        cpu: 1\n      defaultRequest:\n        cpu: 0.5\n      type: Container\n</code></pre>"},{"location":"openshift/configuration/#activities","title":"Activities","text":"Task Description Link Try It Yourself Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration"},{"location":"openshift/configuration/config-map/","title":"Config Maps","text":"<p>ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable.</p> <p>You can data from a ConfigMap in 3 different ways.</p> <ul> <li>As a single environment variable specific to a single key</li> <li>As a set of environment variables from all keys</li> <li>As a set of files, each key represented by a file on mounted volume</li> </ul>"},{"location":"openshift/configuration/config-map/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Mapping Volumes </p> <p>ConfigMaps </p>"},{"location":"openshift/configuration/config-map/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-cm\ndata:\n  color: blue\n  location: naboo\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"echo\"]\n      args: [\"color is $(MY_VAR)\"]\n      env:\n        - name: MY_VAR\n          valueFrom:\n            configMapKeyRef:\n              name: my-cm\n              key: color\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"ls -l /etc/config; echo located at $(cat /etc/config/location)\",\n        ]\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: my-cm\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"env | sort\"]\n      envFrom:\n        - configMapRef:\n            name: my-cm\n  restartPolicy: Never\n</code></pre>"},{"location":"openshift/configuration/limit-ranges/","title":"Limit Ranges","text":"<p>In an OpenShift Container Platform cluster, containers run with unlimited compute resources. By using limit ranges, you can restrict the amount of resources consumed for the following objects within a project.</p> <ul> <li> <p>Pods/Containers: You can set minimum and maximum requirements for CPU and memory</p> </li> <li> <p>Image Streams: You can set limits on the number of images and tags in an ImageStream object</p> </li> <li> <p>Images: You can limit the size of images that can be pushed to a registry</p> </li> <li> <p>Persistent Volume Claims (PVC): You can restrict the size of the PVCs that can be requested</p> </li> </ul> <p>A LimitRange object allows you to restrict the amount of resources that can be consumed in a project. Any request that is made to create or modify a resource will be validated against any LimitRange objects in the project. If any of the constraints listed in the LimitRange object are violated, then the resource request is rejected.</p>"},{"location":"openshift/configuration/limit-ranges/#creating-a-limit-range","title":"Creating a Limit Range","text":"<p>To create a LimitRange object you can follow the example below:</p> <pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\"\nspec:\n  limits:\n    - type: \"Pod\"\n      max:\n        cpu: \"2\"\n        memory: \"1Gi\"\n      min:\n        cpu: \"200m\"\n        memory: \"6Mi\"\n    - type: \"Container\"\n      max:\n        cpu: \"2\"\n        memory: \"1Gi\"\n      min:\n        cpu: \"100m\"\n        memory: \"4Mi\"\n      default:\n        cpu: \"300m\"\n        memory: \"200Mi\"\n      defaultRequest:\n        cpu: \"200m\"\n        memory: \"100Mi\"\n      maxLimitRequestRatio:\n        cpu: \"10\"\n    - type: openshift.io/Image\n      max:\n        storage: 1Gi\n    - type: openshift.io/ImageStream\n      max:\n        openshift.io/image-tags: 20\n        openshift.io/images: 30\n    - type: \"PersistentVolumeClaim\"\n      min:\n        storage: \"2Gi\"\n      max:\n        storage: \"50Gi\"\n</code></pre>"},{"location":"openshift/configuration/limit-ranges/#container-limits","title":"Container Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"Container\"\n      max:\n        cpu: \"2\" [2]\n        memory: \"1Gi\" [3]\n      min:\n        cpu: \"100m\" [4]\n        memory: \"4Mi\" [5]\n      default:\n        cpu: \"300m\" [6]\n        memory: \"200Mi\" [7]\n      defaultRequest:\n        cpu: \"200m\" [8]\n        memory: \"100Mi\" [9]\n      maxLimitRequestRatio:\n        cpu: \"10\" [10]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum amount of CPU that a single container in a pod can request</p> <p>[3] The maximum amount of memory that a single container in a pod can request</p> <p>[4] The minimum amount of CPU that a single container in a pod can request</p> <p>[5] The minimum amount of memory that a single container in a pod can request</p> <p>[6] The default amount of CPU that a container can use if not specified in the Pod spec</p> <p>[7] The default amount of memory that a container can use if not specified in teh Pod spec</p> <p>[8] The default amount of CPU that a contianer can request if not specified in the Pod spec</p> <p>[9] The default amount of memory that a container can request if not specified in the Pod spec</p> <p>[10] The maximum limit-to-request ratio for a container</p>"},{"location":"openshift/configuration/limit-ranges/#pod-limits","title":"Pod Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"Pod\"\n      max:\n        cpu: \"2\" [2]\n        memory: \"1Gi\" [3]\n      min:\n        cpu: \"200m\" [4]\n        memory: \"6Mi\" [5]\n      maxLimitRequestRatio:\n        cpu: \"10\" [6]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum amount of CPU that a pod can request across all containers</p> <p>[3] The maximum amount of memory that a pod can request across all containers</p> <p>[4] The minimum amount of CPU that a pod can request across all containers</p> <p>[5] The minimum amount of memory that a pod can request across all containers</p> <p>[6] The maximum limit-to-request ration for a container</p>"},{"location":"openshift/configuration/limit-ranges/#image-limits","title":"Image Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: openshift.io/Image\n      max:\n        storage: 1Gi [2]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum size of an image that can be pushed to a registry</p>"},{"location":"openshift/configuration/limit-ranges/#image-stream-limits","title":"Image Stream Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: openshift.io/ImageStream\n      max:\n        openshift.io/image-tags: 20 [2]\n        openshift.io/images: 30 [3]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum number of unique image tags in the imagestream.spec.tags parameter in imagestream spec</p> <p>[3] The maximum number of unique image regerenes in the imagestream.status.tags parameter in the imagestream spec</p>"},{"location":"openshift/configuration/limit-ranges/#persistent-volume-claim-limits","title":"Persistent Volume Claim Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"PersistentVolumeClaim\"\n      min:\n        storage: \"2Gi\" [2]\n      max:\n        storage: \"50Gi\" [3]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The minimum amount of storage that can be requested in a persistent volume claim</p> <p>[3] The maximum amount of storage that can be requested in a persistent volume claim</p>"},{"location":"openshift/configuration/secrets/","title":"Secrets","text":"<p>Kubernetes secret objects let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.</p> <p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.</p>"},{"location":"openshift/configuration/secrets/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Image Pull Secrets</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up   and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> </ul> <p>Image Pull Secrets </p> <p>Secret Commands </p> <p>Secrets </p> <p>Secret Distribution </p>"},{"location":"openshift/configuration/secrets/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\nstringData:\n  admin: administrator\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret-config\ntype: Opaque\nstringData:\n  config.yaml: |-\n    apiUrl: \"https://my.api.com/api/v1\"\n    username: token\n    password: thesecrettoken\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      env:\n        - name: SECRET_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: username\n      envFrom:\n        - secretRef:\n            name: mysecret\n      volumeMounts:\n        - name: config\n          mountPath: \"/etc/secrets\"\n  volumes:\n    - name: config\n      secret:\n        secretName: mysecret-config\n</code></pre> OpenShiftKubernetes <p>Create files needed for rest of example</p> <pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> <p>Creating Secret from files</p> <pre><code>oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> <p>Getting Secret</p> <pre><code>oc get secrets\n</code></pre> <p>Gets the Secret's Description</p> <pre><code>oc describe secrets/db-user-pass\n</code></pre> <p>Create files needed for rest of example <pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> Creates the Secret from the files <pre><code>kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> Gets the Secret <pre><code>kubectl get secrets\n</code></pre> Gets the Secret's Description <pre><code>kubectl describe secrets/db-user-pass\n</code></pre></p>"},{"location":"openshift/configuration/security-contexts/","title":"Security Contexts","text":"<p>A security context defines privilege and access control settings for a Pod or Container.</p> <p>To specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod.</p>"},{"location":"openshift/configuration/security-contexts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Managing Security Contexts </p> <p>Security Contexts </p>"},{"location":"openshift/configuration/security-contexts/#references","title":"References","text":"<p>Setup minikube VM with users</p> <pre><code>minikube ssh\n</code></pre> <pre><code>su -\n</code></pre> <pre><code>echo \"container-user-0:x:2000:2000:-:/home/container-user-0:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-user-1:x:2001:2001:-:/home/container-user-1:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-group-0:x:3000:\" &gt;&gt;/etc/group\necho \"container-group-1:x:3001:\" &gt;&gt;/etc/group\nmkdir -p /etc/message/\necho \"Hello, World!\" | sudo tee -a /etc/message/message.txt\nchown 2000:3000 /etc/message/message.txt\nchmod 640 /etc/message/message.txt\n</code></pre> <p>Using the this <code>securityContext</code> the container will be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2000\n    runAsGroup: 3000\n    fsGroup: 3000\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Using the this <code>securityContext</code> the container should NOT be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2001\n    runAsGroup: 3001\n    fsGroup: 3001\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Run to see the errors</p> OpenShiftKubernetes Get Pod Logs<pre><code>oc logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.text': Permission denied\n</code></pre> Get Pod Logs<pre><code>kubectl logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.txt': Permission denied\n</code></pre>"},{"location":"openshift/configuration/service-accounts/","title":"Service Accounts","text":"<p>A service account provides an identity for processes that run in a Pod.</p> <p>When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default).</p> <p>User accounts are for humans. Service accounts are for processes, which run in pods.</p> <p>User accounts are intended to be global. Names must be unique across all namespaces of a cluster, future user resource will not be namespaced. Service accounts are namespaced.</p>"},{"location":"openshift/configuration/service-accounts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Service Accounts </p> <p>Using Service Accounts </p> <p>Service Accounts </p> <p>Service Account Configuration </p>"},{"location":"openshift/configuration/service-accounts/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: my-service-account\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: build-robot-secret\n  annotations:\n    kubernetes.io/service-account.name: my-service-account\ntype: kubernetes.io/service-account-token\n</code></pre> OpenshiftKubernetes Create a Service Account<pre><code>oc create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>oc describe sa &lt;service_account_name&gt;\n</code></pre> Create a Service Account<pre><code>kubectl create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>kubectl describe sa &lt;service_account_name&gt;\n</code></pre>"},{"location":"openshift/core-concepts/","title":"Kubernetes API Primitives","text":"<p>Kubernetes API primitive, also known as Kubernetes objects, are the basic building blocks of any application running in Kubernetes</p> <p>Examples:</p> <ul> <li>Pod</li> <li>Node</li> <li>Service</li> <li>ServiceAccount</li> </ul> <p>Two primary members</p> <ul> <li>Spec, desired state</li> <li>Status, current state</li> </ul>"},{"location":"openshift/core-concepts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Pods </p> <p>Nodes </p> <p>Objects </p> <p>Kube Basics </p>"},{"location":"openshift/core-concepts/#references","title":"References","text":"OpenShiftKubernetes List API-Resources<pre><code>oc api-resources\n</code></pre> List API-Resources<pre><code>kubectl api-resources\n</code></pre>"},{"location":"openshift/core-concepts/namespaces-projects/","title":"Projects/Namespaces","text":"<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects.</p> <p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p> <p>Namespaces are a way to divide cluster resources between multiple users (via resource quota).</p> <p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace. In practice namespaces are used to deploy different versions based on stages of the CICD pipeline (dev, test, stage, prod)</p>"},{"location":"openshift/core-concepts/namespaces-projects/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Working with Projects </p> <p>Creating Projects </p> <p>Configure Project Creation </p> <p>Namespaces </p>"},{"location":"openshift/core-concepts/namespaces-projects/#references","title":"References","text":"Namespace YAML<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre> Pod YAML specifiying Namespace<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  namespace: dev\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes Getting all namespaces/projects<pre><code>oc projects\n</code></pre> Create a new Project<pre><code>oc new-project dev\n</code></pre> Viewing Current Project<pre><code>oc project\n</code></pre> Setting Namespace in Context<pre><code>oc project dev\n</code></pre> Viewing Project Status<pre><code>oc status\n</code></pre> Getting all namespaces<pre><code>kubectl get namespaces\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"openshift/deployments/","title":"Deployments","text":"<p>A Deployment provides declarative updates for Pods and ReplicaSets.</p> <p>You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p> <p>The following are typical use cases for Deployments:</p> <ul> <li>Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li> <li>Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li> <li>Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li> <li>Scale up the Deployment to facilitate more load.</li> <li>Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li> <li>Use the status of the Deployment as an indicator that a rollout has stuck.</li> <li>Clean up older ReplicaSets that you don\u2019t need anymore.</li> </ul>"},{"location":"openshift/deployments/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Deployments </p> <p>Managing Deployment Processes </p> <p>DeploymentConfig Strategies </p> <p>Route Based Deployment Strategies </p> <p>Deployments </p> <p>Scaling Deployments </p>"},{"location":"openshift/deployments/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenshiftKubernetes Create a Deployment<pre><code>oc apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>oc get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>oc describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>oc edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>oc scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>kubectl get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>kubectl describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>kubectl edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>kubectl scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>kubectl delete deployment my-deployment\n</code></pre>"},{"location":"openshift/deployments/#activities","title":"Activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application Rolling Updates"},{"location":"openshift/deployments/updates/","title":"Rolling Updates and Rollbacks","text":"<p>Updating a Deployment A Deployment\u2019s rollout is triggered if and only if the Deployment\u2019s Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</p> <p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0.</p> <p>Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.</p> <p>Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment\u2019s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).</p> <p>A Deployment\u2019s revision is created when a Deployment\u2019s rollout is triggered. This means that the new revision is created if and only if the Deployment\u2019s Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment\u2019s Pod template part is rolled back.</p>"},{"location":"openshift/deployments/updates/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Rollouts </p> <p>Rolling Back </p> <p>Updating a Deployment </p> <p>Rolling Back a Deployment </p>"},{"location":"openshift/deployments/updates/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenShiftKubernetes Get Deployments<pre><code>oc get deployments\n</code></pre> Sets new image for Deployment<pre><code>oc set image deployment/my-deployment nginx=bitnami/nginx:1.16.1 --record\n</code></pre> Check the status of a rollout<pre><code>oc rollout status deployment my-deployment\n</code></pre> Get Replicasets<pre><code>oc get rs\n</code></pre> Get Deployment Description<pre><code>oc describe deployment my-deployment\n</code></pre> Get Rollout History<pre><code>oc rollout history deployment my-deployment\n</code></pre> Undo Rollout<pre><code>oc rollback my-deployment\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"openshift/operators/","title":"What are OpenShift Operators","text":""},{"location":"openshift/operators/#overview","title":"Overview","text":"<p>Operators and Red Hat OpenShift Container Platform Red Hat\u00ae OpenShift\u00ae Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service.</p> <p>Red Hat OpenShift uses the power of Operators to run the entire platform in an autonomous fashion while exposing configuration natively through Kubernetes objects, allowing for quick installation and frequent, robust updates. In addition to the automation advantages of Operators for managing the platform, Red Hat OpenShift makes it easier to find, install, and manage Operators running on your clusters.</p> <p>Included in Red Hat OpenShift is the Embedded OperatorHub, a registry of certified Operators from software vendors and open source projects. Within the Embedded OperatorHub you can browse and install a library of Operators that have been verified to work with Red Hat OpenShift and that have been packaged for easy lifecycle management.</p>"},{"location":"openshift/operators/operatorCatalog/","title":"What is the Operator Catalog","text":""},{"location":"openshift/operators/operatorUsage/","title":"How to use Operators","text":""},{"location":"openshift/pods/","title":"Pods","text":"<p>A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster.</p> <p>A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.</p>"},{"location":"openshift/pods/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> About Pods</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Cluster Configuration for Pods</p> <p>Configure your cluster to work for your specific needs.</p> <p> Learn more</p> </li> <li> <p> Pod Autoscaling</p> <p>Use a horizontal pod autoscaler (HPA) to specify how OCP should automatically scale up or down your deployment.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Pod Overview</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Pod Lifecycle</p> <p>Read about the lifecycle process for pods and what each phase means.</p> <p> Learn more</p> </li> <li> <p> Pod Usage</p> <p>How do you use pods? Read about it here.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes <p>Create Pod using yaml file</p> <pre><code>oc apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>oc get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>oc get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>oc describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>oc logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>oc delete pod myapp-pod\n</code></pre> <p>Create Pod using yaml file</p> <pre><code>kubectl apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>kubectl get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>kubectl describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>kubectl logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>kubectl delete pod myapp-pod\n</code></pre>"},{"location":"openshift/pods/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Pods Create a Pod YAML file to meet certain parameters Pod Creation"},{"location":"openshift/pods/health-checks/","title":"Health and Monitoring","text":""},{"location":"openshift/pods/health-checks/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"<p>A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers:</p> <p>ExecAction: Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0.</p> <p>TCPSocketAction: Performs a TCP check against the Container\u2019s IP address on a specified port. The diagnostic is considered successful if the port is open.</p> <p>HTTPGetAction: Performs an HTTP Get request against the Container\u2019s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</p> <p>The kubelet can optionally perform and react to three kinds of probes on running Containers:</p> <p>livenessProbe: Indicates whether the Container is running. Runs for the lifetime of the Container.</p> <p>readinessProbe: Indicates whether the Container is ready to service requests. Only runs at start.</p>"},{"location":"openshift/pods/health-checks/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Application Health</p> <p>A health check periodically performs diagnostics on a running container using any combination of the readiness, liveness, and startup health checks.</p> <p> Learn more</p> </li> <li> <p> Virtual Machine Health</p> <p>Use readiness and liveness probes to detect and handle unhealthy virtual machines (VMs).</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Container Probes</p> <p>To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.</p> <p> Learn more</p> </li> <li> <p> Configure Probes</p> <p>Read about how to configure liveness, readiness and startup probes for containers.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: app\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello, Kubernetes! &amp;&amp; sleep 3600\"]\n      livenessProbe:\n        exec:\n          command: [\"echo\", \"alive\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      livenessProbe:\n        tcpSocket:\n          port: 8080\n        initialDelaySeconds: 10\n      readinessProbe:\n        httpGet:\n          path: /\n          port: 8080\n        periodSeconds: 10\n</code></pre>"},{"location":"openshift/pods/health-checks/#container-logging","title":"Container Logging","text":"<p>Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.</p> <p>Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.</p>"},{"location":"openshift/pods/health-checks/#resources_1","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Logs Command</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> <li> <p> Cluster Logging</p> <p>As a cluster administrator, you can deploy logging on an OpenShift Container Platform cluster, and use it to collect and aggregate node system audit logs, application container logs, and infrastructure logs.</p> <p> Learn more</p> </li> <li> <p> Logging Collector</p> <p>The collector collects log data from each node, transforms the data, and forwards it to configured outputs.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Logging</p> <p>Application logs can help you understand what is happening inside your application and are particularly useful for debugging problems and monitoring cluster activity.</p> <p> Getting started</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references_1","title":"References","text":"Pod Example<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n    - name: count\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done',\n        ]\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre>"},{"location":"openshift/pods/health-checks/#monitoring-applications","title":"Monitoring Applications","text":"<p>To scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application\u2019s resource usage at each of these levels. This information allows you to evaluate your application\u2019s performance and where bottlenecks can be removed to improve overall performance.</p> <p>Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.</p>"},{"location":"openshift/pods/health-checks/#resources_2","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Monitoring Application Health</p> <p>OpenShift Container Platform applications have a number of options to detect and handle unhealthy containers.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Monitoring Resource Usage</p> <p>You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster.</p> <p> Learn more</p> </li> <li> <p> Resource Metrics</p> <p>For Kubernetes, the Metrics API offers a basic set of metrics to support automatic scaling and similar use cases.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references_2","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 500m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 700m\n          memory: 128Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=500&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 200m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 300m\n          memory: 64Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=200&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> OpenShiftKubernetes <pre><code>oc get projects\noc api-resources -o wide\noc api-resources -o name\n\noc get nodes,ns,po,deploy,svc\n\noc describe node --all\n</code></pre> <p>Verify Metrics is enabled <pre><code>kubectl get --raw /apis/metrics.k8s.io/\n</code></pre></p> <p>Get Node Description <pre><code>kubectl describe node\n</code></pre></p> <p>Check Resource Usage <pre><code>kubectl top pods\nkubectl top nodes\n</code></pre></p> <p></p> <p></p>"},{"location":"openshift/pods/health-checks/#activities","title":"Activities","text":"Task Description Link Try It Yourself Probes Create some Health &amp; Startup Probes to find what's causing an issue. Probes"},{"location":"openshift/pods/jobs/","title":"Jobs and CronJobs","text":"<p>Jobs</p> <p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</p> <p>CronJobs</p> <p>One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format.</p> <p>All CronJob schedule: times are based on the timezone of the master where the job is initiated.</p>"},{"location":"openshift/pods/jobs/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Jobs </p> <p>CronJobs </p> <p>Jobs to Completion </p> <p>Cron Jobs </p> <p>Automated Tasks with Cron </p>"},{"location":"openshift/pods/jobs/#references","title":"References","text":"<p>It computes \u03c0 to 2000 places and prints it out</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>Running in parallel</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  parallelism: 2\n  completions: 3\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: hello\n              image: busybox\n              args:\n                - /bin/sh\n                - -c\n                - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre> OpenShiftKubernetes <p>Gets Jobs <pre><code>oc get jobs\n</code></pre> Gets Job Description <pre><code>oc describe job pi\n</code></pre> Gets Pods from the Job <pre><code>oc get pods\n</code></pre> Deletes Job <pre><code>oc delete job pi\n</code></pre> Gets CronJob <pre><code>oc get cronjobs\n</code></pre> Describes CronJob <pre><code>oc describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>oc get pods\n</code></pre> Deletes CronJob <pre><code>oc delete cronjobs pi\n</code></pre></p> <p>Gets Jobs <pre><code>kubectl get jobs\n</code></pre> Gets Job Description <pre><code>kubectl describe job pi\n</code></pre> Gets Pods from the Job <pre><code>kubectl get pods\n</code></pre> Deletes Job <pre><code>kubectl delete job pi\n</code></pre> Gets CronJob <pre><code>kubectl get cronjobs\n</code></pre> Describes CronJob <pre><code>kubectl describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>kubectl get pods\n</code></pre> Deletes CronJob <pre><code>kubectl delete cronjobs pi\n</code></pre></p>"},{"location":"openshift/pods/jobs/#activities","title":"Activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Using Tekton to test new versions of applications. Crons Jobs"},{"location":"openshift/pods/multi-container/","title":"Multi-Containers Pod","text":"<p>Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers. By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers.</p>"},{"location":"openshift/pods/multi-container/#resources","title":"Resources","text":"Kubernetes <ul> <li> <p> Sidecar Logging</p> <p>Application logs can help you understand what is happening inside your application.</p> <p> Learn more</p> </li> <li> <p> Shared Volume Communication</p> <p>Read about how to use a Volume to communicate between two Containers running in the same Pod.</p> <p> Learn more</p> </li> <li> <p> Toolkit Patterns</p> <p>Read Brendan Burns' blog post about \"The Distributed System ToolKit: Patterns for Composite Containers\".</p> <p> Learn more</p> </li> <li> <p> Brendan Burns Paper</p> <p>Read Brendan Burns' paper about design patterns for container-based distributed systems.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/multi-container/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n    - name: shared-data\n      emptyDir: {}\n  containers:\n    - name: app\n      image: bitnami/nginx\n      volumeMounts:\n        - name: shared-data\n          mountPath: /app\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      volumeMounts:\n        - name: shared-data\n          mountPath: /pod-data\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo Hello from the side container &gt; /pod-data/index.html &amp;&amp; sleep 3600\",\n        ]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      securityContext:\n        capabilities:\n          add:\n            - SYS_PTRACE\n      stdin: true\n      tty: true\n</code></pre> OpenShiftKubernetes <p>Attach Pods Together <pre><code>oc attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre></p> <p>Attach Pods Together <pre><code>kubectl attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre></p>"},{"location":"openshift/pods/multi-container/#activities","title":"Activities","text":"Task Description Link Try It Yourself Multiple Containers Build a container using legacy container image. Multiple Containers"},{"location":"openshift/pods/tagging/","title":"Labels, Selectors, and Annotations","text":"<p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.</p> <p>You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.</p>"},{"location":"openshift/pods/tagging/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> CLI Label Commands</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Labels</p> <p>Labels can be used to organize and to select subsets of objects.</p> <p> Learn more</p> </li> <li> <p> Annotations</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/tagging/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  labels:\n    app: foo\n    tier: frontend\n    env: dev\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"\n    gitrepo: \"https://github.com/csantanapr/knative\"\nspec:\n  containers:\n    - name: app\n      image: bitnami/nginx\n</code></pre> OpenShiftKubernetes <p>Change Labels on Objects <pre><code>oc label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get pods -L tier,env\n</code></pre> <pre><code>oc get pods -l app\n</code></pre> <pre><code>oc get pods -l tier=frontend\n</code></pre> <pre><code>oc get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>oc get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>oc get pods -l 'tier!=backend'\n</code></pre> <pre><code>oc get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>oc delete pod my-pod\n</code></pre></p> <p>Change Labels on Objects <pre><code>kubectl label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get pods -L tier,env\n</code></pre> <pre><code>kubectl get pods -l app\n</code></pre> <pre><code>kubectl get pods -l tier=frontend\n</code></pre> <pre><code>kubectl get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>kubectl get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>kubectl get pods -l 'tier!=backend'\n</code></pre> <pre><code>kubectl get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>kubectl delete pod my-pod\n</code></pre></p>"},{"location":"openshift/pods/troubleshooting/","title":"Debugging Applications","text":"<p>Kubernetes provides tools to help troubleshoot and debug problems with applications.</p> <p>Usually is getting familiar with how primitives objects interact with each other, checking the status of objects, and finally checking logs for any last resource clues.</p>"},{"location":"openshift/pods/troubleshooting/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Debugging ODO</p> <p>OpenShift Toolkit is an IDE plugin available on VS Code and JetBrains IDEs, that allows you to do all things that <code>odo</code> does, i.e. create, test, debug and deploy cloud-native applications on a cloud-native environment in simple steps.</p> <p> Getting started</p> </li> </ul> <ul> <li> <p> Debugging Applications</p> <p>Read about how to debug applications that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> <li> <p> Debugging Services</p> <p>You've run your Pods through a Deployment and created a Service, but you get no response when you try to access it. What do you do?</p> <p> Learn more</p> </li> <li> <p> Debugging Replication Controllers</p> <p>Read about how to debug replication controllers that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/troubleshooting/#references","title":"References","text":"OpenShiftKubernetes <p>MacOS/Linux/Windows command: <pre><code>oc apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre></p> <p>Expose the service using port-forward <pre><code>oc port-forward service/my-service 8080:80 -n debug\n</code></pre></p> <p>Try to access the service <pre><code>curl http://localhost:8080\n</code></pre></p> <p>Try Out these Commands to Debug <pre><code>oc get pods --all-namespaces\n</code></pre> <pre><code>oc project debug\n</code></pre> <pre><code>oc get deployments\n</code></pre> <pre><code>oc describe pod\n</code></pre> <pre><code>oc explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>oc explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>oc edit deployment\n</code></pre> <pre><code>oc logs\n</code></pre> <pre><code>oc get service\n</code></pre> <pre><code>oc get ep\n</code></pre> <pre><code>oc describe service\n</code></pre> <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get deployment --show-labels\n</code></pre></p> <p>MacOS/Linux/Windows command: <pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre></p> <p>Expose the service using port-forward <pre><code>kubectl port-forward service/my-service 8080:80 -n debug\n</code></pre></p> <p>Try to access the service <pre><code>curl http://localhost:8080\n</code></pre></p> <p>Try Out these Commands to Debug <pre><code>kubectl get pods --all-namespaces\n</code></pre> <pre><code>kubectl config set-context --current --namespace=debug\n</code></pre> <pre><code>kubectl get deployments\n</code></pre> <pre><code>kubectl describe pod\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>kubectl edit deployment\n</code></pre> <pre><code>kubectl logs\n</code></pre> <pre><code>kubectl get service\n</code></pre> <pre><code>kubectl get ep\n</code></pre> <pre><code>kubectl describe service\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get deployment --show-labels\n</code></pre></p>"},{"location":"openshift/pods/troubleshooting/#activities","title":"Activities","text":"<p>The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment.</p> Task Description Link Est. Time Try It Yourself Debugging Find which service is breaking in your cluster and find out why. Debugging 30 min"},{"location":"openshift/services-networking/","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"openshift/services-networking/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"openshift/services-networking/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"openshift/services-networking/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress"},{"location":"openshift/services-networking/ingress/","title":"Ingress","text":"<p>An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress can provide load balancing, SSL termination and name-based virtual hosting.</p> <p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.</p>"},{"location":"openshift/services-networking/ingress/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Ingress Operator </p> <p>Using Ingress Controllers </p> <p>Ingress </p> <p>Ingress Controllers </p> <p>Minikube Ingress </p>"},{"location":"openshift/services-networking/ingress/#references","title":"References","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\n</code></pre> OpenShiftKubernetes View Ingress Status<pre><code>oc describe clusteroperators/ingress\n</code></pre> Describe default Ingress Controller<pre><code>oc describe --namespace=openshift-ingress-operator ingresscontroller/default\n</code></pre> <p>Describe default Ingress Controller<pre><code>kubectl get pods -n kube-system | grep ingress\n</code></pre> <pre><code>kubectl create deployment web --image=bitnami/nginx\n</code></pre> <pre><code>kubectl expose deployment web --name=web --port 8080\n</code></pre> <pre><code>kubectl get svc web\n</code></pre> <pre><code>kubectl get ingress\n</code></pre> <pre><code>kubcetl describe ingress example-ingress\n</code></pre> <pre><code>curl hello-world.info --resolve hello-world.info:80:&lt;ADDRESS&gt;\n</code></pre></p>"},{"location":"openshift/services-networking/ingress/#activities","title":"Activities","text":"Task Description Link Try It Yourself IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress"},{"location":"openshift/services-networking/routes/","title":"Routes","text":"<p>OpenShift Only</p> <p>Routes are Openshift objects that expose services for external clients to reach them by name.</p> <p>Routes can insecured or secured on creation using certificates.</p> <p>The new route inherits the name from the service unless you specify one using the --name option.</p>"},{"location":"openshift/services-networking/routes/#resources","title":"Resources","text":"OpenShift <p>Routes </p> <p>Route Configuration </p> <p>Secured Routes </p>"},{"location":"openshift/services-networking/routes/#references","title":"References","text":"<p>Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n</code></pre> <p>Secured Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n  tls:\n    termination: edge\n</code></pre>"},{"location":"openshift/services-networking/routes/#commands","title":"Commands","text":"OpenShift Create Route from YAML<pre><code>oc apply -f route.yaml\n</code></pre> Get Route<pre><code>oc get route\n</code></pre> Describe Route<pre><code>oc get route &lt;route_name&gt;\n</code></pre> Get Route YAML<pre><code>oc get route &lt;route_name&gt; -o yaml\n</code></pre>"},{"location":"openshift/services-networking/services/","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"openshift/services-networking/services/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"openshift/services-networking/services/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"openshift/services-networking/services/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services"},{"location":"openshift/state-persistence/","title":"State Persistence","text":"<p>State persistence in the context of Kubernetes/OpenShift refers to the ability to maintain and retain the state or data of applications even when they are stopped, restarted, or moved between nodes.</p> <p>This is achieved through the use of volumes, persistent volumes (PVs), and persistent volume claims (PVCs). Volumes provide a way to store and access data in a container, while PVs serve as the underlying storage resources provisioned by the cluster. PVCs act as requests made by applications for specific storage resources from the available PVs. By utilizing PVs and PVCs, applications can ensure that their state is preserved and accessible across pod restarts and migrations, enabling reliable and consistent data storage and retrieval throughout the cluster.</p>"},{"location":"openshift/state-persistence/#resources","title":"Resources","text":"<p>Volumes </p> <p>Persistent Volumes </p> <p>Persistent Volume Claims </p>"},{"location":"openshift/state-persistence/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>In this example, we define a PVC named my-pvc with the following specifications:</p> <ul> <li>accessModes specify that the volume can be mounted as read-write by a single node at a time (\"ReadWriteOnce\")</li> <li>resources.requests.storage specifies the requested storage size for the PVC (\"1Gi\")</li> </ul>"},{"location":"openshift/state-persistence/#activities","title":"Activities","text":"Task Description Link Try It Yourself Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes"},{"location":"openshift/state-persistence/pv-pvc/","title":"PersistentVolumes and Claims","text":"<p>Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.</p> <p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.</p> <p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only).</p> <p>While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.</p> <p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.</p> <p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (ROX, RWX) is only possible within one namespace.</p>"},{"location":"openshift/state-persistence/pv-pvc/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Persistent Storage </p> <p>Persistent Volume Types </p> <p>Expanding Peristent Volumes </p> <p>Persistent Volumes </p> <p>Writing Portable Configurations </p> <p>Configuring Persistent Volume Storage </p>"},{"location":"openshift/state-persistence/pv-pvc/#references","title":"References","text":"<pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: my-pv\nspec:\n  storageClassName: local-storage\n  capacity:\n    storage: 128Mi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data-1\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: local-storage\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\n</code></pre> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: nginx\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo $(date):$HOSTNAME Hello Kubernetes! &gt;&gt; /mnt/data/message.txt &amp;&amp; sleep 3600\",\n        ]\n      volumeMounts:\n        - mountPath: \"/mnt/data\"\n          name: my-data\n  volumes:\n    - name: my-data\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre> OpenShiftKubernetes Get the Persistent Volumes in Project<pre><code>oc get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>oc get pvc\n</code></pre> Get a specific Persistent Volume<pre><code>oc get pv &lt;pv_claim&gt;\n</code></pre> Get the Persistent Volume<pre><code>kubectl get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>kubectl get pvc\n</code></pre>"},{"location":"openshift/state-persistence/volumes/","title":"Volumes","text":"<p>On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. First, when a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. Second, when running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems.</p> <p>Docker also has a concept of volumes, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another Container.</p> <p>A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.</p>"},{"location":"openshift/state-persistence/volumes/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Volume Lifecycle </p> <p>Volumes </p>"},{"location":"openshift/state-persistence/volumes/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n      name: busybox\n      volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n  volumes:\n    - name: cache-volume\n      emptyDir: {}\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n    - image: bitnami/nginx\n      name: test-container\n      volumeMounts:\n        - mountPath: /test-pd\n          name: test-volume\n  volumes:\n    - name: test-volume\n      hostPath:\n        # directory location on host\n        path: /data\n        # this field is optional\n        type: Directory\n</code></pre>"}]}