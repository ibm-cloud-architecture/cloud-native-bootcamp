{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"IBM Cloud Native Bootcamp","text":""},{"location":"#bootcamp-overview","title":"Bootcamp Overview","text":"<p>This Cloud Native Bootcamp has been created to teach and guide IBMers, Business Partners, and clients what it takes to move to the cloud. We want to provide a way for anyone using this site to come away with hands-on experience in each of the different technologies listed below.</p>"},{"location":"#concepts-covered","title":"Concepts Covered","text":"<ul> <li> <p> Cloud Native</p> <p>Moving to the cloud comes with new concepts and standards that should be understood before starting your journey to cloud. Learn about them by clicking the link below.</p> <p> Getting started</p> </li> <li> <p> Containers</p> <p>The first task when moving to the cloud is getting your applications running in containers. Get your hands on with containers by clicking the link below.</p> <p> Containerization</p> </li> <li> <p> Kubernetes/OpenShift</p> <p>Managing hundreds of containers is chaos to manage on your own. Learn how container orchestration can make it easy using Kubernetes or OpenShift.</p> <p> Container Orchestration</p> </li> <li> <p> DevOps/GitOps</p> <p>DevOps/GitOps adds standards and reliability to your development lifecycle. Learn how to use it by clicking below.</p> <p> DevOps</p> </li> </ul>"},{"location":"#how-to-approach-the-bootcamp","title":"How to approach the Bootcamp","text":"<p>This bootcamp has been designed to give the students a better hands-on experience than just copy/paste. The bootcamp uses the approach of \"Read, listen, watch, try it out\". We want students to take what they have heard and use their resources to solve the labs on their own without much instruction.</p>"},{"location":"agenda/","title":"Agenda","text":"<p>The following table lists the topics and coding activities for the week. Click on the name of the topic to open a pdf of the material. Click on the link to the solution code to view the solution.</p> In-PersonSelf Paced"},{"location":"agenda/#day-1","title":"Day 1","text":"Topic Type of Activity Kickoff Activity Introductions Activity Introduction Cloud Native Presentation Containers Presentation Container Activities Activity Lunch Activity Container Activities (Cont.) Activity Kubernetes Presentation Wrap up"},{"location":"agenda/#day-2","title":"Day 2","text":"Topic Type of Activity Recap and review from Monday; Q&amp;A Presentation Kubernetes Activities Activity Lunch Activity Kubernetes Presentation Wrap up"},{"location":"agenda/#day-3","title":"Day 3","text":"Topic Type of Activity Recap and review from Tuesday; Q&amp;A Presentation Kubernetes Activities Activity Continuous Integration Presentation Lunch Activity Continuous Integration Lab Activity Continuous Deployment Presentation Wrap up"},{"location":"agenda/#day-4","title":"Day 4","text":"Topic Type of Activity Recap and review from Wednesday; Q&amp;A Presentation Continuous Deployment Lab Activity Lunch Project Work Activity"},{"location":"agenda/#day-5","title":"Day 5","text":"Topic Type of Activity Recap and review from Thursday ; Q&amp;A Presentation Project Work Activity Retrospective Activity"},{"location":"agenda/#modules","title":"Modules","text":"Topic Type of Activity Duration Containers Presentation 1 Hour Container Activities Activity 30 mins Kubernetes Presentation 6 Hours Kubernetes Activities Activity 4 Hours Continuous Integration Presentation 1 Hour Continuous Integration Lab Activity 1 Hour Continuous Deployment Presentation 1 Hour Continuous Deployment Lab Activity 1 Hour Project Work Activity 2 Hours"},{"location":"cloudnative-challenge/","title":"Cloud Native Challenge","text":""},{"location":"cloudnative-challenge/#phase-1-local-develop","title":"Phase 1 - Local Develop","text":"<ul> <li>Start by creating a Github Repo for your application.</li> <li>Choose <code>NodeJS</code>, <code>Python</code>, or <code>React</code>.</li> <li>Site about one of the following:<ul> <li>Yourself</li> <li>Hobby</li> <li>Place you live</li> </ul> </li> <li>Must be able to run locally</li> </ul>"},{"location":"cloudnative-challenge/#application-requirements","title":"Application Requirements","text":"<ul> <li>Minimum of 3 webpages</li> <li>Minimum of 1 GET and POST method each.</li> <li>SwaggerUI Configured for API Testing.</li> <li>API's exposed through Swagger</li> <li>Custom CSS files for added formatting.</li> </ul>"},{"location":"cloudnative-challenge/#testing","title":"Testing","text":"<p>Setup each of the following tests that apply:</p> <ul> <li>Page tests</li> <li>API tests</li> <li>Connection Tests</li> </ul>"},{"location":"cloudnative-challenge/#phase-2-application-enhancements","title":"Phase 2 - Application Enhancements","text":""},{"location":"cloudnative-challenge/#database-connectivity-and-functionality","title":"Database Connectivity and Functionality","text":"<ul> <li>Add local or cloud DB to use for data collection.</li> <li>Use 3rd party API calls to get data.<ul> <li>Post Data to DB via API Call</li> <li>Retrieve Data from DB via API Call</li> <li>Delete Data from DB via API Call</li> </ul> </li> </ul>"},{"location":"cloudnative-challenge/#phase-3-containerize","title":"Phase 3 - Containerize","text":""},{"location":"cloudnative-challenge/#container-image","title":"Container Image","text":"<ul> <li>Create a DockerFile</li> <li>Build your docker image from the dockerfile</li> <li>Run it locally via Docker Desktop or another docker engine.</li> </ul>"},{"location":"cloudnative-challenge/#image-registries","title":"Image Registries","text":"<ul> <li>Once validation of working docker image, push the image up to a registry.</li> <li>Use one of the following registries:<ul> <li>Docker</li> <li>Quay.io</li> <li>IBM Container</li> </ul> </li> <li>Push the image up with the following name: <code>{DockerRegistry}/{yourusername}/techdemos-cn:v1</code></li> </ul>"},{"location":"cloudnative-challenge/#phase-4-kubernetes-ready","title":"Phase 4 - Kubernetes Ready","text":""},{"location":"cloudnative-challenge/#create-pod-and-deployment-files","title":"Create Pod and Deployment files","text":"<ul> <li>Create a <code>Pod</code> YAML to validate your image.</li> <li>Next, create a <code>deployment</code> yaml file with the setting of 3 replicas.</li> <li>Verify starting of deployment</li> <li>Push all YAML files to Github</li> </ul>"},{"location":"cloudnative-challenge/#application-exposing","title":"Application Exposing","text":"<ul> <li>Create a <code>Service</code> and <code>Route</code> yaml</li> <li>Save <code>Service</code> and <code>Route</code> yamls in Github</li> </ul>"},{"location":"cloudnative-challenge/#configuration-setup","title":"Configuration Setup","text":"<ul> <li>Create a <code>ConfigMap</code> for all site configuration.</li> <li>Setup <code>Secrets</code> for API keys or Passwords to 3rd parties.</li> <li>Add storage where needed to deployment.</li> </ul>"},{"location":"cloudnative-challenge/#phase-5-devopsgitops","title":"Phase 5 - Devops/Gitops","text":""},{"location":"cloudnative-challenge/#tekton-pipeline-setup","title":"Tekton Pipeline Setup","text":"<ul> <li>Create a Tekton pipeline to do the following:<ul> <li>Setup</li> <li>Test</li> <li>Build and Push Image</li> <li>GitOps Version Update</li> </ul> </li> <li>Make each of the above their own task.</li> <li>Setup triggers to respond to Github commits and PR's</li> </ul>"},{"location":"cloudnative-challenge/#gitsops-configuration","title":"GitsOps Configuration","text":"<ul> <li>Use ArgoCD to setup Deployment.</li> <li>Test your ArgoCD deployment</li> <li>Make a change to site and push them.</li> <li>Validate new image version.</li> </ul>"},{"location":"cloudnative-challenge/#extras","title":"Extras","text":""},{"location":"cloudnative-challenge/#chatbot-functions","title":"Chatbot Functions","text":"<ul> <li>Watson Assistant Integration</li> <li>Conversation about your sites topic.</li> <li>Have Chat window or page.</li> <li>Integrate Watson Assistant Actions.</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#required-skills","title":"Required skills","text":"<p>This activities contained here require you to be proficient in working from the command line with a linux shell (Bash, Zsh, etc.) Below is a partial list of activities you should be able to perform. </p> <ul> <li>Copy, move, and rename files</li> <li>Understand linux file permissions</li> <li>Edit text files (vi, vim, emacs, etc)</li> <li>Edit environment variables ($PATH)</li> </ul> <p>Here is a course for learning (or brushing up) on working from the linux command line Linux Command Line Basics</p>"},{"location":"prerequisites/#workstation-setup","title":"Workstation Setup","text":"Openshift (MacOS/Linux)Openshift (Windows)Kubernetes (MacOS/Linux)Kubernetes (Windows)"},{"location":"prerequisites/#create-accounts","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> <li> <p>RedHat Account: Needed for OpenShift Local.</p> </li> </ul>"},{"location":"prerequisites/#install-clis-and-tools","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 17 or 21 LTS: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"prerequisites/#create-accounts_1","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> <li> <p>RedHat Account: Needed for OpenShift Local.</p> </li> </ul>"},{"location":"prerequisites/#cloud-native-vm","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"prerequisites/#install-clis-and-tools_1","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.</p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 17 or 21 LTS: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>OpenShift Local: For running a local OpenShift cluster</p> </li> </ul> <p>Warning</p> <p>Make sure you have Cisco VPN turned off when using OpenShift Local.</p>"},{"location":"prerequisites/#create-accounts_2","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> </ul>"},{"location":"prerequisites/#install-clis-and-tools_2","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>!!! Note     If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 17 or 21 LTS: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>Minikube: Follow the instructions for your Operating System.</p> </li> </ul> <p>Warning</p> <p>Make sure you have Cisco VPN turned off when using minikube.</p>"},{"location":"prerequisites/#create-accounts_3","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>IBM Cloud Account: Create one if needed, make sure you can log in. </p> </li> <li> <p>O'Reilly Account: The account is free and easy to create.</p> </li> </ul>"},{"location":"prerequisites/#cloud-native-vm_1","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"prerequisites/#install-clis-and-tools_3","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud Account and management of your managed IBM Kubernetes and Red Hat OpenShift clusters</p> <ul> <li>Don't install just the IBM Cloud CLI, install the IBM Cloud CLI and Developer Tools <pre><code>curl -sL https://ibm.biz/idt-installer | bash\n</code></pre></li> </ul> </li> </ul> <p>Note</p> <p>If you log in to the web UI using SSO, you'll need to create an API key for logging into the CLI. </p> <ul> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Tekton CLI: Used to help control Tekton pipelines from the command line.     <pre><code>    brew tap tektoncd/tools\n    brew install tektoncd/tools/tektoncd-cli\n</code></pre></p> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 17 or 21 LTS: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> <li> <p>Minikube: Follow the instructions for your Operating System.</p> </li> </ul> <p>Warning</p> <p>Make sure you have Cisco VPN turned off when using minikube.</p>"},{"location":"prerequisites/#check-your-setup","title":"Check Your Setup","text":"<p>After installing the required CLIs and tools, you can run a system check script to verify which dependencies you have installed and which ones are missing.</p> <p>Download System Check Script </p> <p>Run the script after downloading:</p> <pre><code>chmod +x system-check.sh\n./system-check.sh\n</code></pre> <p>Tip</p> <p>The script checks for: IBM Cloud CLI, Git, OpenShift Local (crc), Minikube, Docker, Podman, kubectl, oc, Tekton CLI, and ArgoCD CLI. You don't need all of these - just the ones relevant to the labs you plan to complete.</p>"},{"location":"prerequisites/#environment-setup","title":"Environment Setup","text":"MiniKubeOpenShift LocalIKSOpenShift on IBM Cloud (4.x) <ul> <li>Verify your cluster has 4GB+ memory and Kubernetes 1.28+     <pre><code>minikube config view\n</code></pre></li> <li>Set the driver (use <code>docker</code> for most systems, or <code>qemu</code> for Apple Silicon Macs)     <pre><code>minikube config set driver docker\n</code></pre></li> <li>In case memory is not set, or need to increase set the memory and recreate the VM     <pre><code>minikube config set memory 4096\nminikube config set kubernetes-version v1.31.0\nminikube delete\nminikube start\n</code></pre></li> <li>Kubernetes should be v1.28+     <pre><code>kubectl version\n</code></pre></li> </ul> <p>Make sure OpenShift Local is installed. Check out the OpenShift Local Page.</p> <p>** Setup CRC ** <pre><code>crc setup\n</code></pre> ** Start CRC ** <pre><code>crc start\n</code></pre></p> <ul> <li> <p>Login to IBM Cloud with your IBM ID.</p> </li> <li> <p>Click \"Create Resource\" and search for \"kubernetes service\".</p> </li> <li> <p>Select the tile for \"Kubernetes Service\" and do the following:</p> </li> <li>Select the \"Free Cluster\" plan.</li> <li>Name your cluster.</li> <li> <p>Select \"Create\" at the bottom right of the screen.</p> </li> <li> <p>Once the Cluster is provisioned, Click on the \"Connect via CLI\" in the top right corner.</p> </li> <li> <p>Follow the instructions to connect and you are set to go.</p> </li> </ul> <ul> <li> <p>In this approach you share an OpenShift cluster on IBM Cloud with other bootcamp attendees.</p> </li> <li> <p>Considering 10-15 attendees we recommend a cluster with 3 worker nodes (each 8 vCPUs + 32GB RAM - b3c.8x32).</p> </li> <li> <p>Ask your IBM cloud account owner to provide access to an OpenShift cluster.</p> </li> <li> <p>In addition to the IBM Cloud CLI also install the OpenShift Origin CLI to be able to execute all commands.</p> </li> <li> <p>Open your OpenShift web console from within your IBM cloud account, select your profile and choose \"copy login command\" to retrieve an access token for the login.</p> </li> <li> <p>Login with your OpenShift Origin CLI.     <pre><code>oc login --token=&lt;token&gt; --server=&lt;server-url&gt;:&lt;server-port&gt;\n</code></pre></p> </li> <li> <p>Create your own project / namespace in OpenShift that you will leverage across all labs.     <pre><code>oc new-project &lt;dev-your_initials&gt;\n</code></pre></p> </li> <li> <p>Validate in the OpenShift web console that your project has been created (Administrator view -&gt; Home -&gt; Projects)</p> </li> </ul>"},{"location":"prerequisites/#next-steps","title":"Next Steps","text":"<p>Once Setup is complete, you can now begin reading about Cloud Native by clicking the link, or the <code>Next</code> button below.</p>"},{"location":"cloud/","title":"Journey to the Cloud","text":""},{"location":"cloud/#introduction","title":"Introduction","text":"<p>Cloud is everywhere. Today, many companies want to migrate their applications on to cloud. For this migration to be done, the applications must be re-architected in a way that they fully utilize the advantages of the cloud.</p> <p>Cloud computing has fundamentally changed how organizations build, deploy, and operate software. Rather than maintaining physical servers and infrastructure, businesses can now leverage on-demand computing resources that scale dynamically based on their needs.</p>"},{"location":"cloud/#what-is-cloud-computing","title":"What is Cloud Computing?","text":"<p>Cloud computing is the delivery of computing services\u2014including servers, storage, databases, networking, software, and analytics\u2014over the internet (\"the cloud\"). This model provides faster innovation, flexible resources, and economies of scale.</p>"},{"location":"cloud/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>On-demand self-service - Provision resources automatically without human interaction</li> <li>Broad network access - Services available over the network from any device</li> <li>Resource pooling - Computing resources are pooled to serve multiple consumers</li> <li>Rapid elasticity - Capabilities can be scaled up or down based on demand</li> <li>Measured service - Resource usage is monitored and charged based on consumption</li> </ul>"},{"location":"cloud/#cloud-service-models","title":"Cloud Service Models","text":"<p>There are three primary cloud service models, each offering different levels of control and responsibility:</p>"},{"location":"cloud/#infrastructure-as-a-service-iaas","title":"Infrastructure as a Service (IaaS)","text":"<p>IaaS provides virtualized computing resources over the internet. Users manage operating systems, middleware, and applications while the provider manages the physical infrastructure.</p> <p>Examples: IBM Cloud Virtual Servers, AWS EC2, Azure Virtual Machines</p> <p>Use Cases:</p> <ul> <li>Development and test environments</li> <li>High-performance computing</li> <li>Big data analysis</li> <li>Website hosting</li> </ul>"},{"location":"cloud/#platform-as-a-service-paas","title":"Platform as a Service (PaaS)","text":"<p>PaaS provides a platform for developing, running, and managing applications without the complexity of maintaining the underlying infrastructure.</p> <p>Examples: IBM Cloud Foundry, Red Hat OpenShift, Heroku, Google App Engine</p> <p>Use Cases:</p> <ul> <li>Application development</li> <li>API development and management</li> <li>Business analytics</li> <li>IoT applications</li> </ul>"},{"location":"cloud/#software-as-a-service-saas","title":"Software as a Service (SaaS)","text":"<p>SaaS delivers software applications over the internet on a subscription basis. The provider manages everything from infrastructure to application updates.</p> <p>Examples: IBM Watson, Salesforce, Microsoft 365, Slack</p> <p>Use Cases:</p> <ul> <li>Email and collaboration</li> <li>Customer relationship management</li> <li>Enterprise resource planning</li> <li>Human resources management</li> </ul>"},{"location":"cloud/#cloud-deployment-models","title":"Cloud Deployment Models","text":""},{"location":"cloud/#public-cloud","title":"Public Cloud","text":"<p>Resources are owned and operated by a third-party provider and shared across multiple organizations. This model offers cost efficiency and scalability.</p>"},{"location":"cloud/#private-cloud","title":"Private Cloud","text":"<p>Cloud infrastructure is used exclusively by a single organization. This provides more control and security but requires more management.</p>"},{"location":"cloud/#hybrid-cloud","title":"Hybrid Cloud","text":"<p>Combines public and private clouds, allowing data and applications to move between them. This offers flexibility and more deployment options.</p>"},{"location":"cloud/#multi-cloud","title":"Multi-Cloud","text":"<p>Uses services from multiple cloud providers to avoid vendor lock-in and leverage best-of-breed services from each provider.</p>"},{"location":"cloud/#why-migrate-to-the-cloud","title":"Why Migrate to the Cloud?","text":"<p>Organizations are moving to the cloud for several compelling reasons:</p>"},{"location":"cloud/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Reduce capital expenditure - No need to purchase and maintain hardware</li> <li>Pay-as-you-go pricing - Only pay for what you use</li> <li>Economies of scale - Benefit from the provider's large-scale infrastructure</li> </ul>"},{"location":"cloud/#agility-and-speed","title":"Agility and Speed","text":"<ul> <li>Rapid provisioning - Deploy new resources in minutes, not months</li> <li>Global reach - Deploy applications worldwide quickly</li> <li>Faster time to market - Focus on development rather than infrastructure</li> </ul>"},{"location":"cloud/#scalability","title":"Scalability","text":"<ul> <li>Elastic resources - Scale up or down based on demand</li> <li>Handle traffic spikes - Automatically accommodate increased load</li> <li>Global distribution - Serve users from locations closest to them</li> </ul>"},{"location":"cloud/#reliability","title":"Reliability","text":"<ul> <li>Built-in redundancy - Data replicated across multiple locations</li> <li>Disaster recovery - Quick recovery from failures</li> <li>High availability - SLAs guaranteeing uptime</li> </ul>"},{"location":"cloud/#innovation","title":"Innovation","text":"<ul> <li>Access to latest technology - AI, ML, IoT, and analytics services</li> <li>Experimentation - Low-cost environment for trying new ideas</li> <li>Continuous updates - Automatic access to new features</li> </ul>"},{"location":"cloud/#cloud-migration-strategies","title":"Cloud Migration Strategies","text":"<p>When migrating applications to the cloud, organizations typically follow one of these strategies (the \"6 R's\"):</p> Strategy Description When to Use Rehost \"Lift and shift\" - Move as-is to the cloud Quick migration, minimal changes Replatform Make minimal optimizations Leverage some cloud benefits Repurchase Move to a SaaS solution Replace with commercial product Refactor Re-architect for cloud-native Maximize cloud benefits Retain Keep on-premises Compliance or technical constraints Retire Decommission the application No longer needed"},{"location":"cloud/#cloud-native-applications","title":"Cloud-Native Applications","text":"<p>Cloud-native is an approach to building and running applications that fully exploit the advantages of the cloud computing model. Cloud-native applications are designed from the ground up for the cloud.</p>"},{"location":"cloud/#characteristics-of-cloud-native-applications","title":"Characteristics of Cloud-Native Applications","text":"<ul> <li>Microservices architecture - Loosely coupled, independently deployable services</li> <li>Containerized - Packaged with dependencies for consistent deployment</li> <li>Dynamically orchestrated - Managed by platforms like Kubernetes</li> <li>DevOps practices - Continuous integration and delivery</li> <li>API-driven - Services communicate through well-defined APIs</li> </ul>"},{"location":"cloud/#the-twelve-factor-app","title":"The Twelve-Factor App","text":"<p>The Twelve-Factor App methodology provides best practices for building cloud-native applications:</p> <ol> <li>Codebase - One codebase tracked in version control</li> <li>Dependencies - Explicitly declare and isolate dependencies</li> <li>Config - Store configuration in the environment</li> <li>Backing services - Treat backing services as attached resources</li> <li>Build, release, run - Strictly separate build and run stages</li> <li>Processes - Execute the app as stateless processes</li> <li>Port binding - Export services via port binding</li> <li>Concurrency - Scale out via the process model</li> <li>Disposability - Maximize robustness with fast startup and graceful shutdown</li> <li>Dev/prod parity - Keep development and production as similar as possible</li> <li>Logs - Treat logs as event streams</li> <li>Admin processes - Run admin tasks as one-off processes</li> </ol>"},{"location":"cloud/#getting-started","title":"Getting Started","text":"<p>This bootcamp will guide you through the key technologies and practices for cloud-native development:</p> <ol> <li>Containers - Package applications with their dependencies</li> <li>Kubernetes/OpenShift - Orchestrate containerized applications</li> <li>DevOps - Automate the software delivery pipeline</li> </ol> <p>By the end of this bootcamp, you will have hands-on experience building and deploying cloud-native applications on Kubernetes and OpenShift.</p>"},{"location":"cloud/benefits/","title":"Benefits of Cloud","text":""},{"location":"cloud/benefits/#cost-efficiency","title":"Cost Efficiency","text":"<p>Moving to the cloud can significantly reduce capital expenditure (CapEx) by eliminating the need for expensive data center infrastructure, hardware maintenance, and physical security. Cloud providers offer pay-as-you-go pricing models, allowing organizations to pay only for the resources they actually use. This flexibility helps optimize operational costs and converts fixed costs into variable expenses.</p>"},{"location":"cloud/benefits/#scalability-and-flexibility","title":"Scalability and Flexibility","text":"<p>Cloud platforms provide unprecedented scalability options. Applications can automatically scale up or down based on demand, ensuring optimal performance during peak times while reducing costs during slower periods. This elastic nature of cloud resources is impossible to achieve with traditional on-premises infrastructure without significant over provisioning.</p>"},{"location":"cloud/benefits/#global-reach-and-availability","title":"Global Reach and Availability","text":"<p>Cloud providers maintain data centers worldwide, enabling organizations to deploy applications closer to their users for better performance. This global infrastructure also provides built-in redundancy and disaster recovery capabilities, offering higher availability than most on-premises solutions can achieve cost-effectively.</p>"},{"location":"cloud/benefits/#security-and-compliance","title":"Security and Compliance","text":"<p>Leading cloud providers invest heavily in security measures that often exceed what individual organizations can implement. They offer advanced security features, regular security updates, and compliance certifications that help organizations meet various regulatory requirements. While security remains a shared responsibility, cloud providers handle much of the infrastructure security burden.</p>"},{"location":"cloud/benefits/#innovation-and-time-to-market","title":"Innovation and Time-to-Market","text":"<p>Cloud services provide access to cutting-edge technologies like artificial intelligence, machine learning, and IoT platforms without requiring significant investment in research and development. This accessibility accelerates innovation and reduces time-to-market for new features and products. Organizations can quickly experiment with new technologies and scale successful initiatives.</p>"},{"location":"cloud/benefits/#maintenance-and-updates","title":"Maintenance and Updates","text":"<p>Cloud providers handle infrastructure maintenance, hardware updates, and system patching, allowing IT teams to focus on business-critical tasks rather than routine maintenance. This automated maintenance ensures systems are always up-to-date and running on the latest hardware without service interruptions.</p>"},{"location":"cloud/benefits/#environmental-impact","title":"Environmental Impact","text":"<p>Cloud data centers typically operate more efficiently than individual company data centers, leading to reduced energy consumption and carbon footprint. Cloud providers often invest in renewable energy sources and implement advanced cooling technologies, making cloud computing a more environmentally sustainable choice.</p>"},{"location":"cloud/benefits/#business-continuity","title":"Business Continuity","text":"<p>Cloud platforms offer robust disaster recovery and backup solutions that can be implemented more easily and cost-effectively than traditional methods. Automatic data replication across multiple regions ensures business continuity even in the face of major disruptions, reducing downtime and data loss.</p>"},{"location":"cloud/cn-apps/","title":"Cloud Native Applications","text":""},{"location":"cloud/cn-apps/#what-is-cloud-native","title":"What is Cloud-Native?","text":"<p>Cloud-native is about how we build and run applications taking full advantage of cloud computing rather than worrying about where we deploy it.</p> <p>Cloud-native refers less to where an application resides and more to how it is built and deployed.</p> <ul> <li> <p>A cloud-native application consists of discrete, reusable components     known as microservices that are designed to integrate into any cloud     environment.</p> </li> <li> <p>These microservices act as building blocks and are often packaged in     containers.</p> </li> <li> <p>Microservices work together as a whole to comprise an application,     yet each can be independently scaled, continuously improved, and     quickly iterated through automation and orchestration processes.</p> </li> <li> <p>The flexibility of each microservice adds to the agility and     continuous improvement of cloud-native applications.</p> </li> </ul>"},{"location":"cloud/cn-apps/#why-cloud-native","title":"Why Cloud-Native?","text":"<p>Cloud-native applications are different from the traditional applications that run in your data centres. The applications that are designed in the traditional way are not built keeping cloud compatibility in mind. They may have strong ties with the internal systems. Also, they cannot take advantage of all the benefits of the cloud.</p> <p>So, we need a new architecture for our applications to utilize the benefits of cloud. There is a need to design the applications keeping cloud in mind and take advantage of several cloud services like storage, queuing, caching etc.</p> <ul> <li> <p>Speed, safety, and scalability comes with cloud-native applications.</p> </li> <li> <p>Helps you to quickly deliver the advancements.</p> </li> <li> <p>Allows you to have loose ties into the corporate IT where it most     certainly would destabilize legacy architectures.</p> </li> <li> <p>Helps you to continuously deliver your applications with zero     downtime.</p> </li> <li> <p>Infrastructure is less predictable.</p> </li> <li> <p>Service instances are all disposable.</p> </li> <li> <p>Deployments are immutable.</p> </li> <li> <p>To meet the expectations of the today\u2019s world customers, these     systems are architected for elastic scalability.</p> </li> </ul>"},{"location":"cloud/cn-concepts/","title":"Cloud Concepts","text":""},{"location":"cloud/cn-concepts/#cloud-native-concepts","title":"Cloud-native concepts","text":"<p>Some of the important characteristics of cloud-native applications are as follows.</p> <ul> <li> <p>Disposable Infrastructure</p> </li> <li> <p>Isolation</p> </li> <li> <p>Scalability</p> </li> <li> <p>Disposable architecture</p> </li> <li> <p>Value added cloud services</p> </li> <li> <p>Polyglot cloud</p> </li> <li> <p>Self-sufficient, full-stack teams</p> </li> <li> <p>Cultural Change</p> </li> </ul> <p>Disposable Infrastructure</p> <p>While creating applications on cloud, you need several cloud resources as part of it. We often hear how easy it is to create all these resources. But did you ever think how easy is it to dispose them. It is definitely not that easy to dispose them and that is why you don\u2019t hear a lot about it.</p> <p>In traditional or legacy applications, we have all these resources residing on machines. If these go down, we need to redo them again and most of this is handled by the operations team manually. So, when we are creating applications on cloud, we bring those resources like load balancers, databases, gateways, etc on to cloud as well along with machine images and containers.</p> <p>While creating these applications, you should always keep in mind that if you are creating a resource when required, you should also be able to destroy it when not required. Without this, we cannot achieve the factors speed, safety and scalability. If you want this to happen, we need automation.</p> <p>Automation allows you to</p> <ul> <li> <p>Deliver new features at any time.</p> </li> <li> <p>Deliver patches faster.</p> </li> <li> <p>Improves the system quality.</p> </li> <li> <p>Facilitates team scale and efficiency.</p> </li> </ul> <p>Now you know what we are talking about. Disposable infrastructure is nothing but <code>Infrastructure as Code</code>.</p> <p>Infrastructure as Code</p> <p>Here, you develop the code for automation exactly as same as the you do for the rest of the application using agile methodologies.</p> <ul> <li> <p>Automation code is driven by a story.</p> </li> <li> <p>Versioned in the same repository as rest of the code.</p> </li> <li> <p>Continuously tested as part of CI/CD pipeline.</p> </li> <li> <p>Test environments are created and destroyed along with test runs.</p> </li> </ul> <p>Thus, disposable infrastructure lays the ground work for scalability and elasticity.</p> <p>Isolation</p> <p>In traditional or legacy applications, the applications are monoliths. So, when there is bug or error in the application, you need to fix it. Once you changed the code, the entire application should be redeployed. Also, there may be side effects which you can never predict. New changes may break any components in the application as they are all inter related.</p> <p>In cloud-native applications, to avoid the above scenario, the system is decomposed into bounded isolated components. Each service will be defined as one component and they are all independent of each other. So, in this case, when there is a bug or error in the application, you know which component to fix and this also avoids any side effects as the components are all unrelated pieces of code.</p> <p>Thus, cloud-native systems must be resilient to man made errors. To achieve this we need isolation and this avoids a problem in one component affecting the entire system. Also, it helps you to introduce changes quickly in the application with confidence.</p> <p>Scalability</p> <p>Simply deploying your application on cloud does not make it cloud-native. To be cloud native it should be able to take full benefits of the cloud. One of the key features is Scalability.</p> <p>In today\u2019s world, once your business starts growing, the number of users keep increasing and they may be from different locations. Your application should be able to support more number of devices and it should also be able to maintain its responsiveness. Moreover, this should be efficient and cost-effective.</p> <p>To achieve this, cloud native application runs in multiple runtimes spread across multiple hosts. The applications should be designed and architected in a way that they support multi regional, active-active deployments. This helps you to increase the availability and avoids single point of failures.</p> <p>Disposable architecture</p> <p>Leveraging the disposable infrastructure and scaling isolated components is important for cloud native applications. Disposable architecture is based on this and it takes the idea of disposability and replacement to the next level.</p> <p>Most of us think in a monolithic way because we got used to traditional or legacy applications a lot. This may lead us to take decisions in monolithic way rather than in cloud native way. In monoliths, we tend to be safe and don\u2019t do a lot of experimentation. But Disposable architecture is exactly opposite to monolithic thinking. In this approach, we develop small pieces of the component and keep experimenting with it to find an optimal solution.</p> <p>When there is a breakthrough in the application, you can\u2019t simply take decisions based on the available information which may be incomplete or inaccurate. So, with disposable architecture, you start with small increments, and invest time to find the optimal solution. Sometimes, there may be a need to completely replace the component, but that initial work was just the cost of getting the information that caused the breakthrough. This helps you to minimize waste allowing you to use your resources on controlled experiments efficiently and get good value out of it in the end.</p> <p>Value added cloud services</p> <p>When you are defining an application, there are many things you need to care of. Each and every service will be associated with many things like databases, storage, redundancy, monitoring, etc. For your application, along with your components, you also need to scale the data. You can reduce the operational risk and also get all such things at greater velocity by leveraging the value-added services that are available on cloud. Sometimes, you may need third party services if they are not available on your cloud. You can externally hook them up with your application as needed.</p> <p>By using the value added services provided by your cloud provider, you will get to know all the available options on your cloud and you can also learn about all the new services. This will help you to take good long-termed decisions. You can definitely exit the service if you find something more suitable for your component and hook that up with your application based on the requirements.</p> <p>Polyglot cloud</p> <p>Most of you are familiar with Polyglot programming. For your application, based on the component, you can choose the programming languages that best suits it. You need not stick to a single programming language for the entire application. If you consider Polyglot persistence, the idea is choose the storage mechanism that suits better on a component by component basis. It allows a better global scale.</p> <p>Similarly, the next thing will be Polyglot cloud. Like above, here you choose a cloud provider that better suits on a component by component basis. For majority of your components, you may have a go to cloud provider. But, this does not stop you from choosing a different one if it suits well for any of your application components. So, you can run different components of your cloud native system on different cloud providers based on your requirements.</p> <p>Self-sufficient, full-stack teams</p> <p>In a traditional set up, many organizations have teams based on skill set like backend, user interface, database, operations etc. Such a structure will not allow you to build cloud native systems.</p> <p>In cloud native systems, the system is composed of bounded isolated components. They have their own resources. Each of such component must be owned by self-sufficient, full stack team. That team is entirely responsible for all the resources that belong to that particular component. In this set up, team tends to build quality up front in as they are the ones who deploy it and they will be taking care of it if the component is broken. It is more like you build it and then you run it. So, the team can continuously deliver advancements to the components at their own pace. Also, they are completely responsible for delivering it safely.</p> <p>Cultural Change</p> <p>Cloud native is different way of thinking. We need to first make up our minds, not just the systems, to utilize the full benefits of cloud. Compared to the traditional systems, there will be lots of things we do differently in cloud-native systems.</p> <p>To make that happen, cultural change is really important. To change the thinking at high level, we just to first prove that the low level practices can truly deliver and encourage lean thinking. With this practice, you can conduct experimentation. Based on the feedback from business, you can quickly and safely deliver your applications that can scale.</p>"},{"location":"cloud/cn-concepts/#presentations","title":"Presentations","text":"<p>Cloud-Native Presentation </p>"},{"location":"cloud/cn-concepts/#cloud-native-roadmap","title":"Cloud-native Roadmap","text":"<p>You can define your cloud native road map in many ways. You can get there by choosing different paths. Let us see the trail map defined by CNCF.</p> <p>CNCF defined the Cloud Native Trail Map providing an overview for enterprises starting their cloud native journey as follows.</p> <p>This cloud map gives us various steps that an engineering team may use while considering the cloud native technologies and exploring them. The most common ones among them are Containerization, CI/CD, and Orchestration. Next crucial pieces will be Observability &amp; Analysis and Service Mesh. And later comes the rest of them like Networking, Distributed Database, Messaging, Container runtime, and software distribution based on your requirements.</p> <p></p> <ul> <li> <p>With out Containerization, you cannot build cloud native     applications. This helps your application to run in any computing     environment. Basically, all your code and dependencies are packaged     up together in to a single unit here. Among different container     platforms available, Docker is a preferred one.</p> </li> <li> <p>To bring all the changes in the code to container automatically, it     is nice to set up a CI/CD pipeline which does that. There are many     tools available like jenkins, travis, etc.</p> </li> <li> <p>Since we have containers, we need container orchestration to manage     the container lifecycles. Currently, Kubernetes is one solution     which is popular.</p> </li> <li> <p>Monitoring and Observability plays a very important role. It is good     to set up some of them like logging, tracing, metrics etc.</p> </li> <li> <p>To enable more complex operational requirements, you can use a     service mesh. It helps you out with several things like service     discovery, health, routing, A/B testing etc. Istio is one of the     examples of service mesh.</p> </li> <li> <p>Networking plays a crucial role. You should define flexible     networking layers based on your requirements. For this, you can use     Calico, Weave Net etc.</p> </li> <li> <p>Sometimes, you may need distributed databases. Based on your     requirements, if you need more scalability and resiliency, these are     required.</p> </li> <li> <p>Messaging may be required sometimes too. Go with different messaging     queues like Kafka, RabbitMQ etc available when you need them.</p> </li> <li> <p>Container Registry helps you to store all your containers. You can     also enable image scanning and signing if required.</p> </li> <li> <p>As a part of your application, sometimes you may need a secure     software distribution.</p> </li> </ul> <p>Also, if you want to see the cloud native landscape, check it out here.</p>"},{"location":"cloud/cn-concepts/#summary","title":"Summary","text":"<p>In this, we covered the fundamentals of cloud native systems. You now know what cloud native is, why we need it and how it is important. Cloud native is not just deploying your application on cloud but it is more of taking full advantages of cloud. Also, from cloud-native roadmap, you will get an idea on how to design and architect your cloud-native system. You can also get the idea of different tools, frameworks, platforms etc from the cloud-native landscapes.</p> <p>Also, if you are interesting in knowing more, we have Cloud-Native: A Complete Guide. Feel free to check this out.</p>"},{"location":"cloud/cn-concepts/#references","title":"References","text":"<ul> <li> <p>John Gilbert, (2018). Cloud Native Development Patterns and Best     Practices. Publisher: Packt     Publishing</p> </li> <li> <p>CNCF landscape</p> </li> <li> <p>CNCF     Definition</p> </li> </ul>"},{"location":"containers/","title":"What are Containers?","text":"<p>You wanted to run your application on different computing environments. It may be your laptop, test environment, staging environment or production environment.</p> <p>So, when you run it on these different environments, will your application work reliably?</p> <p>What if some underlying software changes? What if the security policies are different? Or something else changes?</p> <p>To solve these problems, we need Containers.</p>"},{"location":"containers/#containers","title":"Containers","text":"<p>Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized.</p> <p>For example, Docker created a standard way to create images for Linux Containers.</p>"},{"location":"containers/#presentations","title":"Presentations","text":"<p>Container Basics </p>"},{"location":"containers/#why-containers","title":"Why Containers?","text":"<ul> <li>Run anywhere - Containers run consistently on any platform that supports a container runtime</li> <li>Lightweight - Share the host OS kernel, using fewer resources than virtual machines</li> <li>Fast startup - Start in seconds rather than minutes</li> <li>Isolation - Applications run in isolated environments without interfering with each other</li> <li>Scalable - Easily scale up or down based on demand</li> </ul>"},{"location":"containers/#how-containers-work","title":"How Containers Work","text":"<p>Containers leverage Linux kernel features to provide isolation and resource management:</p>"},{"location":"containers/#linux-namespaces","title":"Linux Namespaces","text":"<p>Namespaces provide isolation for system resources, making each container appear to have its own:</p> <ul> <li>PID namespace - Isolated process tree</li> <li>Network namespace - Isolated network stack (interfaces, routing tables, firewall rules)</li> <li>Mount namespace - Isolated filesystem mount points</li> <li>User namespace - Isolated user and group IDs</li> <li>UTS namespace - Isolated hostname and domain name</li> </ul>"},{"location":"containers/#control-groups-cgroups","title":"Control Groups (cgroups)","text":"<p>Cgroups limit and account for resource usage:</p> <ul> <li>CPU time and cores</li> <li>Memory limits</li> <li>Disk I/O bandwidth</li> <li>Network bandwidth</li> </ul>"},{"location":"containers/#union-filesystems","title":"Union Filesystems","text":"<p>Container images use layered filesystems (like OverlayFS) that allow:</p> <ul> <li>Efficient storage through shared base layers</li> <li>Copy-on-write for container modifications</li> <li>Fast image distribution</li> </ul>"},{"location":"containers/#container-runtimes-and-standards","title":"Container Runtimes and Standards","text":"<p>The container ecosystem has evolved around open standards managed by the Open Container Initiative (OCI), a Linux Foundation project.</p>"},{"location":"containers/#oci-specifications","title":"OCI Specifications","text":"Specification Purpose Runtime Spec Defines how to run a container (filesystem bundle, lifecycle, configuration) Image Spec Defines container image format (layers, manifests, configuration) Distribution Spec Defines how images are distributed via registries"},{"location":"containers/#container-runtimes","title":"Container Runtimes","text":"<p>Low-level runtimes (OCI-compliant):</p> <ul> <li>runc - The reference implementation, created by Docker and donated to OCI</li> <li>crun - A fast, lightweight runtime written in C (used by Podman)</li> <li>youki - A runtime written in Rust for improved safety</li> </ul> <p>High-level runtimes:</p> <ul> <li>containerd - Industry standard runtime used by Docker and Kubernetes, manages the complete container lifecycle</li> <li>CRI-O - Lightweight runtime built specifically for Kubernetes, implements the Container Runtime Interface (CRI)</li> </ul>"},{"location":"containers/#container-engines","title":"Container Engines","text":"<p>Container engines provide user-friendly tools for building, running, and managing containers:</p> Engine Description Docker The most widely adopted container platform, includes Docker Engine, CLI, and Desktop Podman Daemonless, rootless container engine, drop-in replacement for Docker CLI Buildah Specialized tool for building OCI-compliant container images nerdctl Docker-compatible CLI for containerd <p>Docker vs Podman</p> <p>Both Docker and Podman use OCI-compliant images and can run the same containers. Key differences:</p> <ul> <li>Docker uses a daemon (dockerd) that runs as root</li> <li>Podman is daemonless and runs rootless by default (more secure)</li> <li>Commands are nearly identical: <code>docker run</code> = <code>podman run</code></li> </ul>"},{"location":"containers/#docker","title":"Docker","text":"<p>Docker is the most popular containerization platform, providing tools to develop, deploy, and run applications inside containers.</p> <ul> <li>Open source project (Moby)</li> <li>Available on Linux, macOS, and Windows</li> <li>Extensive ecosystem and community</li> </ul>"},{"location":"containers/#docker-architecture","title":"Docker Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Docker Client                       \u2502\n\u2502                  (docker CLI commands)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 REST API\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Docker Daemon                        \u2502\n\u2502                       (dockerd)                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Images    \u2502  \u2502 Containers  \u2502  \u2502    Networks     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      containerd                          \u2502\n\u2502              (container runtime manager)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         runc                             \u2502\n\u2502                 (OCI container runtime)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"containers/#docker-image","title":"Docker Image","text":"<p>A read-only template containing instructions for creating a container. Images are built from a <code>Dockerfile</code> and stored in registries.</p> <p>Images are composed of layers:</p> <ul> <li>Each instruction in a Dockerfile creates a new layer</li> <li>Layers are cached and reused across images</li> <li>Only changed layers need to be transferred when pulling/pushing</li> </ul>"},{"location":"containers/#dockerfile","title":"Dockerfile","text":"<p>A text file containing instructions to build a Docker image.</p> <pre><code># Base image\nFROM node:20-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Copy dependency files first (for better caching)\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy application code\nCOPY . .\n\n# Create non-root user for security\nRUN addgroup -g 1001 appgroup &amp;&amp; \\\n    adduser -u 1001 -G appgroup -s /bin/sh -D appuser\nUSER appuser\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD wget --quiet --tries=1 --spider http://localhost:3000/health || exit 1\n\n# Start command\nCMD [\"node\", \"server.js\"]\n</code></pre>"},{"location":"containers/#multi-stage-builds","title":"Multi-stage Builds","text":"<p>Multi-stage builds reduce image size by separating build and runtime environments:</p> <pre><code># Build stage\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM node:20-alpine\nWORKDIR /app\n# Copy only production dependencies and built files\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nUSER node\nCMD [\"node\", \"dist/server.js\"]\n</code></pre>"},{"location":"containers/#docker-container","title":"Docker Container","text":"<p>A runnable instance of an image. Containers are isolated from each other and the host system.</p> <pre><code># Run a container\ndocker run -d --name myapp -p 8080:3000 myimage:latest\n\n# View running containers\ndocker ps\n\n# View logs\ndocker logs myapp\n\n# Execute command in running container\ndocker exec -it myapp /bin/sh\n\n# Stop and remove\ndocker stop myapp &amp;&amp; docker rm myapp\n</code></pre>"},{"location":"containers/#docker-registry","title":"Docker Registry","text":"<p>A service that stores and distributes container images:</p> <ul> <li>Docker Hub - Public registry with official and community images (hub.docker.com)</li> <li>Red Hat Quay - Enterprise registry with security scanning</li> <li>IBM Cloud Container Registry - IBM's managed registry service</li> <li>GitHub Container Registry - GitHub's package registry for containers</li> <li>Amazon ECR / Google GCR / Azure ACR - Cloud provider registries</li> </ul>"},{"location":"containers/#dockerfile-best-practices","title":"Dockerfile Best Practices","text":""},{"location":"containers/#1-use-minimal-base-images","title":"1. Use Minimal Base Images","text":"<p>Choose the smallest base image that meets your needs:</p> Image Type Size Use Case <code>scratch</code> 0 MB Statically compiled binaries (Go, Rust) <code>alpine</code> ~5 MB General purpose, includes shell <code>distroless</code> ~20 MB No shell, package manager, or unnecessary tools <code>slim</code> variants ~50-100 MB Reduced versions of full images <pre><code># Good - minimal image\nFROM node:20-alpine\n\n# Avoid - full image with unnecessary tools\nFROM node:20\n</code></pre>"},{"location":"containers/#2-run-as-non-root-user","title":"2. Run as Non-root User","text":"<p>Never run containers as root in production:</p> <pre><code># Create and switch to non-root user\nRUN addgroup -g 1001 appgroup &amp;&amp; \\\n    adduser -u 1001 -G appgroup -s /bin/sh -D appuser\nUSER appuser\n</code></pre>"},{"location":"containers/#3-optimize-layer-caching","title":"3. Optimize Layer Caching","text":"<p>Order instructions from least to most frequently changing:</p> <pre><code># Good - dependencies cached separately from code\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\n\n# Bad - cache invalidated on any code change\nCOPY . .\nRUN npm ci\n</code></pre>"},{"location":"containers/#4-use-dockerignore","title":"4. Use .dockerignore","text":"<p>Exclude unnecessary files from the build context:</p> <pre><code># .dockerignore\nnode_modules\n.git\n.env\n*.log\nDockerfile\n.dockerignore\n</code></pre>"},{"location":"containers/#5-pin-versions","title":"5. Pin Versions","text":"<p>Always pin base image and dependency versions:</p> <pre><code># Good - pinned version\nFROM node:20.10.0-alpine3.19\n\n# Avoid - unpredictable updates\nFROM node:latest\n</code></pre>"},{"location":"containers/#6-minimize-layers","title":"6. Minimize Layers","text":"<p>Combine related commands to reduce layers:</p> <pre><code># Good - single layer\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Bad - multiple layers, cached apt lists\nRUN apt-get update\nRUN apt-get install -y curl\n</code></pre>"},{"location":"containers/#7-add-health-checks","title":"7. Add Health Checks","text":"<p>Enable orchestrators to monitor container health:</p> <pre><code>HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:3000/health || exit 1\n</code></pre>"},{"location":"containers/#container-security","title":"Container Security","text":""},{"location":"containers/#image-security","title":"Image Security","text":"<ul> <li>Scan images for vulnerabilities - Use tools like Trivy, Grype, or Snyk</li> <li>Sign images - Use Cosign/Sigstore for image verification</li> <li>Use trusted base images - Prefer official images from verified publishers</li> <li>Keep images updated - Regularly rebuild with security patches</li> </ul> <pre><code># Scan image with Trivy\ntrivy image myapp:latest\n\n# Sign image with Cosign\ncosign sign myregistry.io/myapp:latest\n</code></pre>"},{"location":"containers/#runtime-security","title":"Runtime Security","text":"<ul> <li>Run as non-root - Never run containers as root</li> <li>Read-only filesystem - Use <code>--read-only</code> flag when possible</li> <li>Drop capabilities - Remove unnecessary Linux capabilities</li> <li>Resource limits - Set CPU and memory limits</li> </ul> <pre><code># Secure container run\ndocker run -d \\\n  --read-only \\\n  --user 1001:1001 \\\n  --cap-drop ALL \\\n  --memory 512m \\\n  --cpus 0.5 \\\n  myapp:latest\n</code></pre>"},{"location":"containers/#supply-chain-security","title":"Supply Chain Security","text":"<ul> <li>Generate SBOMs - Create Software Bill of Materials for images</li> <li>Verify signatures - Validate image authenticity before deployment</li> <li>Use private registries - Control access to your container images</li> </ul> <pre><code># Generate SBOM with Syft\nsyft myapp:latest -o spdx-json &gt; sbom.json\n</code></pre>"},{"location":"containers/#references","title":"References","text":"<ul> <li>Docker Documentation</li> <li>Podman Documentation</li> <li>Open Container Initiative (OCI)</li> <li>Cloud Native Computing Foundation (CNCF)</li> <li>Dockerfile Best Practices</li> <li>Container Security Guide - NIST</li> </ul>"},{"location":"containers/benefitsContainers/","title":"Benefits of Containers","text":""},{"location":"containers/benefitsContainers/#consistency-across-environments","title":"Consistency Across Environments","text":"<p>Containers package applications and their dependencies into a single unit, ensuring consistent behavior across different environments - from development to production. This \"build once, run anywhere\" approach eliminates the common \"it works on my machine\" problem and reduces deployment issues caused by environmental differences.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Development \u2502\u2500\u2500\u2500\u25b6\u2502   Testing   \u2502\u2500\u2500\u2500\u25b6\u2502   Staging   \u2502\u2500\u2500\u2500\u25b6\u2502 Production  \u2502\n\u2502             \u2502    \u2502             \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502  Same Image \u2502    \u2502  Same Image \u2502    \u2502  Same Image \u2502    \u2502  Same Image \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"containers/benefitsContainers/#lightweight-and-fast","title":"Lightweight and Fast","text":"<p>Unlike traditional virtual machines that include a full guest operating system, containers share the host OS kernel. This fundamental difference provides significant advantages:</p> Characteristic Virtual Machines Containers Startup time Minutes Seconds Image size Gigabytes Megabytes Memory overhead High (full OS) Low (shared kernel) Density per host 10-20 VMs 100s of containers"},{"location":"containers/benefitsContainers/#isolation-and-resource-efficiency","title":"Isolation and Resource Efficiency","text":"<p>Containers maintain isolation between applications while efficiently sharing system resources:</p> <ul> <li>Process isolation - Each container has its own process namespace</li> <li>Network isolation - Containers can have separate network stacks</li> <li>Filesystem isolation - Changes in one container don't affect others</li> <li>Resource limits - CPU, memory, and I/O can be constrained per container</li> </ul> <p>This isolation means multiple containers can run efficiently on a single host without interfering with each other.</p>"},{"location":"containers/benefitsContainers/#rapid-development-and-deployment","title":"Rapid Development and Deployment","text":"<p>Containers enable faster application development and deployment cycles:</p> <ul> <li>Standardized environments - Developers work in identical environments to production</li> <li>Quick iteration - Build, test, and deploy changes in minutes</li> <li>Version control - Container images can be versioned and rolled back easily</li> <li>Reproducible builds - Same Dockerfile always produces the same image</li> <li>CI/CD integration - Seamless integration with modern DevOps pipelines</li> </ul>"},{"location":"containers/benefitsContainers/#microservices-architecture-support","title":"Microservices Architecture Support","text":"<p>Containers are the foundation of microservices architecture, enabling:</p> <ul> <li>Independent deployment - Update services without affecting others</li> <li>Technology flexibility - Each service can use different languages/frameworks</li> <li>Team autonomy - Teams own and deploy their services independently</li> <li>Fault isolation - Failures in one service don't cascade to others</li> <li>Horizontal scaling - Scale individual services based on demand</li> </ul> Auth Service API Gateway Search Service Payment Service Email Service Container Container Container Container Container \u2195 Scale \u2195 Scale \u2195 Scale \u2195 Scale \u2195 Scale <p>Each service can be deployed, updated, and scaled independently.</p>"},{"location":"containers/benefitsContainers/#portability","title":"Portability","text":"<p>Container images are platform-independent and run on any system with a compatible container runtime:</p> <ul> <li>Developer laptops - macOS, Windows, Linux</li> <li>On-premises servers - Any Linux distribution</li> <li>Cloud providers - AWS, Azure, GCP, IBM Cloud</li> <li>Edge devices - IoT and embedded systems</li> </ul> <p>This portability eliminates vendor lock-in and provides flexibility in deployment choices.</p>"},{"location":"containers/benefitsContainers/#auto-scaling-and-high-availability","title":"Auto-scaling and High Availability","text":"<p>Container orchestration platforms like Kubernetes provide:</p> <ul> <li>Automatic scaling - Scale up or down based on CPU, memory, or custom metrics</li> <li>Self-healing - Automatically restart failed containers</li> <li>Load balancing - Distribute traffic across container instances</li> <li>Rolling updates - Deploy new versions with zero downtime</li> <li>Health checks - Continuously monitor container health</li> </ul>"},{"location":"containers/benefitsContainers/#resource-optimization","title":"Resource Optimization","text":"<p>Containers enable precise resource management:</p> <ul> <li>Fine-grained allocation - Set exact CPU and memory limits per container</li> <li>Bin packing - Orchestrators efficiently place containers on nodes</li> <li>Overcommitment - Run more containers than physical resources allow</li> <li>Right-sizing - Easily adjust resources based on actual usage</li> </ul> <p>This optimization reduces infrastructure costs and improves application performance.</p>"},{"location":"containers/benefitsContainers/#security-benefits","title":"Security Benefits","text":""},{"location":"containers/benefitsContainers/#runtime-security","title":"Runtime Security","text":"<ul> <li>Reduced attack surface - Minimal base images contain fewer vulnerabilities</li> <li>Isolation - Containers run in separate namespaces and cgroups</li> <li>Immutability - Container filesystems can be read-only</li> <li>Least privilege - Containers can run as non-root users</li> </ul>"},{"location":"containers/benefitsContainers/#rootless-containers","title":"Rootless Containers","text":"<p>Modern container runtimes support rootless operation:</p> <ul> <li>No root daemon - Podman runs entirely in user space</li> <li>User namespaces - Map container root to unprivileged host user</li> <li>Reduced blast radius - Container escape doesn't grant root access</li> </ul> <pre><code># Podman runs rootless by default\npodman run --rm alpine whoami\n# Output: root (inside container, but unprivileged on host)\n</code></pre>"},{"location":"containers/benefitsContainers/#image-security","title":"Image Security","text":"<ul> <li>Vulnerability scanning - Scan images for known CVEs before deployment</li> <li>Image signing - Verify image authenticity with digital signatures</li> <li>Base image updates - Quickly rebuild and redeploy when patches are available</li> <li>Minimal images - Use distroless or scratch images to reduce vulnerabilities</li> </ul>"},{"location":"containers/benefitsContainers/#supply-chain-security","title":"Supply Chain Security","text":"<p>Modern container workflows support software supply chain security:</p>"},{"location":"containers/benefitsContainers/#software-bill-of-materials-sbom","title":"Software Bill of Materials (SBOM)","text":"<p>Track all components in your container images:</p> <ul> <li>Dependency visibility - Know exactly what's in your images</li> <li>License compliance - Identify all software licenses</li> <li>Vulnerability tracking - Map CVEs to specific components</li> </ul>"},{"location":"containers/benefitsContainers/#image-provenance","title":"Image Provenance","text":"<p>Verify where images came from and how they were built:</p> <ul> <li>Build attestations - Cryptographic proof of build process</li> <li>Signature verification - Ensure images haven't been tampered with</li> <li>Policy enforcement - Only deploy signed images from trusted sources</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Build     \u2502\u2500\u2500\u2500\u25b6\u2502     Sign     \u2502\u2500\u2500\u2500\u25b6\u2502   Verify     \u2502\n\u2502   Image      \u2502    \u2502   (Cosign)   \u2502    \u2502  (Admission) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n   Generate            Attach              Enforce\n    SBOM             Attestation           Policy\n</code></pre>"},{"location":"containers/benefitsContainers/#developer-experience","title":"Developer Experience","text":"<p>Containers improve developer productivity:</p> <ul> <li>Onboarding - New developers start quickly with containerized dev environments</li> <li>Consistency - \"Works on my machine\" becomes \"works everywhere\"</li> <li>Local testing - Run the full stack locally with Docker Compose</li> <li>Debugging - Reproduce production issues in isolated containers</li> <li>Experimentation - Try new technologies without polluting the host system</li> </ul>"},{"location":"containers/benefitsContainers/#cost-efficiency","title":"Cost Efficiency","text":"<p>Containers provide both direct and indirect cost savings:</p> Area Benefit Infrastructure Higher density means fewer servers needed Operations Automation reduces manual intervention Development Faster cycles mean quicker time to market Maintenance Immutable infrastructure reduces configuration drift Scaling Pay only for resources actually used"},{"location":"containers/benefitsContainers/#industry-adoption","title":"Industry Adoption","text":"<p>Containers have become the standard for cloud-native applications:</p> <ul> <li>90%+ of organizations use containers in production</li> <li>Kubernetes is the de facto orchestration standard</li> <li>OCI standards ensure interoperability across tools</li> <li>Major clouds offer managed container services</li> <li>Ecosystem includes thousands of pre-built images</li> </ul>"},{"location":"containers/benefitsContainers/#summary","title":"Summary","text":"<p>Containers provide a comprehensive solution for modern application development and deployment:</p> Benefit Impact Consistency Eliminate environment-related issues Speed Deploy in seconds, not hours Efficiency Run more workloads on less infrastructure Portability Deploy anywhere without modification Security Isolate applications and reduce attack surface Scalability Scale automatically based on demand DevOps Enable CI/CD and infrastructure as code <p>These benefits make containers essential for organizations adopting cloud-native practices and microservices architectures.</p>"},{"location":"containers/imageregistry/","title":"What are Image Registries","text":"<p>A registry is a repository used to store and access container images. Container registries can support container-based application development, often as part of DevOps processes.</p> <p>Container registries save developers valuable time in the creation and delivery of cloud-native applications, acting as the intermediary for sharing container images between systems. They essentially act as a place for developers to store container images and share them out via a process of uploading (pushing) to the registry and downloading (pulling) into another system, like a Kubernetes cluster.</p>"},{"location":"containers/imageregistry/#popular-container-registries","title":"Popular Container Registries","text":"Registry Description Use Case Docker Hub The largest public registry with official and community images Public images, getting started Red Hat Quay Enterprise registry with security scanning and geo-replication Enterprise, OpenShift environments IBM Cloud Registry IBM's managed registry with vulnerability scanning IBM Cloud deployments GitHub Container Registry Integrated with GitHub for seamless CI/CD GitHub-based projects Amazon ECR AWS-native registry with IAM integration AWS deployments Google Artifact Registry GCP-native registry supporting multiple artifact types GCP deployments Azure Container Registry Azure-native with geo-replication and tasks Azure deployments Harbor Open-source enterprise registry with RBAC and scanning Self-hosted enterprise <p>Learn More </p>"},{"location":"containers/imageregistry/#registry-concepts","title":"Registry Concepts","text":""},{"location":"containers/imageregistry/#image-naming-convention","title":"Image Naming Convention","text":"<p>Container images follow a standard naming format:</p> <pre><code>[registry-host/][namespace/]repository[:tag|@digest]\n</code></pre> <p>Examples:</p> Image Reference Description <code>nginx</code> Docker Hub official image, latest tag implied <code>nginx:1.25</code> Docker Hub official image with specific tag <code>myuser/myapp:v1.0</code> Docker Hub user namespace <code>quay.io/myorg/myapp:latest</code> Quay.io registry <code>us.icr.io/mynamespace/myapp:v2</code> IBM Cloud Registry <code>ghcr.io/owner/repo:sha-abc123</code> GitHub Container Registry"},{"location":"containers/imageregistry/#tags-vs-digests","title":"Tags vs Digests","text":"<p>Tags are mutable labels that can be moved to different image versions:</p> <pre><code>myapp:latest    # Can change over time\nmyapp:v1.0      # Semantic version\nmyapp:dev       # Environment-specific\n</code></pre> <p>Digests are immutable content-addressable identifiers:</p> <pre><code>myapp@sha256:abc123...  # Always refers to exact same image\n</code></pre> <p>Production Best Practice</p> <p>Use digests or specific version tags in production. Avoid <code>latest</code> as it can change unexpectedly.</p>"},{"location":"containers/imageregistry/#image-layers-and-caching","title":"Image Layers and Caching","text":"<p>Images are composed of layers, each representing a Dockerfile instruction:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application Code (Layer 4) \u2502  \u2190 Changes frequently\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Dependencies (Layer 3)     \u2502  \u2190 Changes occasionally\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Runtime (Layer 2)          \u2502  \u2190 Rarely changes\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Base OS (Layer 1)          \u2502  \u2190 Rarely changes\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Registries store layers efficiently:</p> <ul> <li>Layers are deduplicated across images</li> <li>Only changed layers are transferred during push/pull</li> <li>Base layers are shared between many images</li> </ul>"},{"location":"containers/imageregistry/#registry-security","title":"Registry Security","text":""},{"location":"containers/imageregistry/#image-scanning","title":"Image Scanning","text":"<p>Most enterprise registries include vulnerability scanning:</p> <ul> <li>Automatic scanning on push</li> <li>CVE database integration</li> <li>Severity ratings (Critical, High, Medium, Low)</li> <li>Policy enforcement to block vulnerable images</li> </ul> <pre><code># Scan locally with Trivy before pushing\ntrivy image myapp:latest\n\n# Example output\nmyapp:latest (alpine 3.19.0)\nTotal: 2 (UNKNOWN: 0, LOW: 1, MEDIUM: 1, HIGH: 0, CRITICAL: 0)\n</code></pre>"},{"location":"containers/imageregistry/#image-signing","title":"Image Signing","text":"<p>Verify image authenticity and integrity using signatures:</p> <p>Cosign (Sigstore)</p> <pre><code># Sign an image\ncosign sign myregistry.io/myapp:v1.0\n\n# Verify signature before pulling\ncosign verify myregistry.io/myapp:v1.0\n</code></pre> <p>Docker Content Trust (DCT)</p> <pre><code># Enable content trust\nexport DOCKER_CONTENT_TRUST=1\n\n# Push signed image\ndocker push myregistry.io/myapp:v1.0\n</code></pre>"},{"location":"containers/imageregistry/#access-control","title":"Access Control","text":"<p>Registries provide various access control mechanisms:</p> Mechanism Description Authentication Username/password, tokens, or SSO Authorization Role-based access (read, write, admin) Namespaces Organizational separation of images Private repositories Restrict access to authorized users Robot accounts Service accounts for CI/CD automation"},{"location":"containers/imageregistry/#software-bill-of-materials-sbom","title":"Software Bill of Materials (SBOM)","text":"<p>Generate and attach SBOMs to track image contents:</p> <pre><code># Generate SBOM with Syft\nsyft myapp:latest -o spdx-json &gt; sbom.json\n\n# Attach SBOM to image with Cosign\ncosign attach sbom --sbom sbom.json myregistry.io/myapp:v1.0\n</code></pre>"},{"location":"containers/imageregistry/#registry-best-practices","title":"Registry Best Practices","text":""},{"location":"containers/imageregistry/#image-tagging-strategy","title":"Image Tagging Strategy","text":"<p>Use a consistent tagging strategy for your images:</p> Tag Type Example Use Case Semantic version <code>v1.2.3</code> Release versions Git SHA <code>sha-abc1234</code> Traceability to source Build number <code>build-456</code> CI/CD tracking Environment <code>staging</code>, <code>prod</code> Environment-specific Date <code>2025-01-15</code> Time-based releases <p>Recommended approach:</p> <pre><code># Tag with both version and git SHA\ndocker tag myapp:latest myregistry.io/myapp:v1.2.3\ndocker tag myapp:latest myregistry.io/myapp:sha-$(git rev-parse --short HEAD)\n</code></pre>"},{"location":"containers/imageregistry/#image-lifecycle-management","title":"Image Lifecycle Management","text":"<p>Keep your registry clean and costs manageable:</p> <ul> <li>Retention policies - Automatically delete old images</li> <li>Immutable tags - Prevent tag overwrites for release versions</li> <li>Garbage collection - Remove unreferenced layers</li> </ul> <pre><code># IBM Cloud Registry - delete old images\nibmcloud cr retention-policy-set --images 10 mynamespace\n\n# Quay - set up auto-pruning in repository settings\n</code></pre>"},{"location":"containers/imageregistry/#cicd-integration","title":"CI/CD Integration","text":"<p>Automate image building and pushing in your pipeline:</p> <pre><code># Example GitHub Actions workflow\nname: Build and Push\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Login to Registry\n        run: echo \"${{ secrets.REGISTRY_PASSWORD }}\" | docker login quay.io -u ${{ secrets.REGISTRY_USER }} --password-stdin\n\n      - name: Build and Push\n        run: |\n          docker build -t quay.io/myorg/myapp:${{ github.sha }} .\n          docker push quay.io/myorg/myapp:${{ github.sha }}\n\n      - name: Scan Image\n        run: trivy image quay.io/myorg/myapp:${{ github.sha }}\n</code></pre>"},{"location":"containers/imageregistry/#multi-architecture-images","title":"Multi-Architecture Images","text":"<p>Build images for multiple platforms (amd64, arm64):</p> <pre><code># Create and use buildx builder\ndocker buildx create --name mybuilder --use\n\n# Build multi-arch image\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  -t myregistry.io/myapp:v1.0 \\\n  --push .\n</code></pre>"},{"location":"containers/imageregistry/#next-steps","title":"Next Steps","text":"<p>Ready to start working with registries? Check out the hands-on tutorials:</p> <p>Registry Tutorials </p> <p>The tutorials cover step-by-step instructions for:</p> <ul> <li>Docker Hub</li> <li>Red Hat Quay</li> <li>IBM Cloud Container Registry</li> </ul>"},{"location":"containers/reference/","title":"Container References","text":""},{"location":"containers/reference/#basic-cli-commands","title":"Basic CLI Commands","text":"PodmanDocker Action Command Check Podman version <code>podman --version</code> Check host information <code>podman info</code> List images <code>podman images</code> Pull image <code>podman pull &lt;image-name&gt;:&lt;tag&gt;</code> Run image <code>podman run &lt;image-name&gt;:&lt;tag&gt;</code> List running containers <code>podman ps</code> List all containers <code>podman ps -a</code> Inspect a container <code>podman inspect &lt;container-id&gt;</code> View container logs <code>podman logs &lt;container-id&gt;</code> Stop a running container <code>podman stop &lt;container-id&gt;</code> Kill a running container <code>podman kill &lt;container-id&gt;</code> Build Podman image <code>podman build -t &lt;image_name&gt;:&lt;tag&gt; -f Containerfile .</code> Push image <code>podman push &lt;image_name&gt;:&lt;tag&gt;</code> Remove stopped containers <code>podman rm &lt;container-id&gt;</code> Remove image <code>podman rmi -f &lt;image-name&gt;</code> or <code>podman rmi -f &lt;image-id&gt;</code> Action Command Get Docker version <code>docker version</code> Run <code>hello-world</code> Container <code>docker run hello-world</code> List Running Containers <code>docker ps</code> List all containers <code>docker ps -a</code> Stop a container <code>docker stop &lt;container-name/container-id&gt;</code> List Docker Images <code>docker images</code> Login into registry <code>docker login</code> Build an image <code>docker build -t &lt;image_name&gt;:&lt;tag&gt; .</code> Inspect a docker object <code>docker inspect &lt;name/id&gt;</code> Inspect a docker image <code>docker inspect image &lt;name/id&gt;</code> View container logs <code>docker logs &lt;container-id&gt;</code> Pull an image <code>docker pull &lt;image_name&gt;:&lt;tag&gt;</code> Push an Image <code>docker push &lt;image_name&gt;:&lt;tag&gt;</code> Remove a container <code>docker rm &lt;container-name/container-id&gt;</code>"},{"location":"containers/reference/#running-containers","title":"Running Containers","text":"<p>Common options for running containers:</p> PodmanDocker <pre><code># Run in background (detached mode)\npodman run -d &lt;image&gt;\n\n# Run with interactive terminal\npodman run -it &lt;image&gt; /bin/sh\n\n# Run with port mapping (host:container)\npodman run -p 8080:80 &lt;image&gt;\n\n# Run with environment variables\npodman run -e MY_VAR=value &lt;image&gt;\n\n# Run with volume mount\npodman run -v /host/path:/container/path &lt;image&gt;\n\n# Run with container name\npodman run --name mycontainer &lt;image&gt;\n\n# Run with resource limits\npodman run --memory 512m --cpus 0.5 &lt;image&gt;\n\n# Run with automatic removal on exit\npodman run --rm &lt;image&gt;\n\n# Run as specific user\npodman run --user 1001:1001 &lt;image&gt;\n</code></pre> <pre><code># Run in background (detached mode)\ndocker run -d &lt;image&gt;\n\n# Run with interactive terminal\ndocker run -it &lt;image&gt; /bin/sh\n\n# Run with port mapping (host:container)\ndocker run -p 8080:80 &lt;image&gt;\n\n# Run with environment variables\ndocker run -e MY_VAR=value &lt;image&gt;\n\n# Run with volume mount\ndocker run -v /host/path:/container/path &lt;image&gt;\n\n# Run with container name\ndocker run --name mycontainer &lt;image&gt;\n\n# Run with resource limits\ndocker run --memory 512m --cpus 0.5 &lt;image&gt;\n\n# Run with automatic removal on exit\ndocker run --rm &lt;image&gt;\n\n# Run as specific user\ndocker run --user 1001:1001 &lt;image&gt;\n</code></pre>"},{"location":"containers/reference/#networking-commands","title":"Networking Commands","text":"PodmanDocker Action Command List networks <code>podman network ls</code> Create network <code>podman network create &lt;network-name&gt;</code> Inspect network <code>podman network inspect &lt;network-name&gt;</code> Connect container <code>podman network connect &lt;network&gt; &lt;container&gt;</code> Disconnect container <code>podman network disconnect &lt;network&gt; &lt;container&gt;</code> Remove network <code>podman network rm &lt;network-name&gt;</code> Run with network <code>podman run --network &lt;network-name&gt; &lt;image&gt;</code> Action Command List networks <code>docker network ls</code> Create network <code>docker network create &lt;network-name&gt;</code> Create bridge network <code>docker network create --driver bridge &lt;name&gt;</code> Inspect network <code>docker network inspect &lt;network-name&gt;</code> Connect container <code>docker network connect &lt;network&gt; &lt;container&gt;</code> Disconnect container <code>docker network disconnect &lt;network&gt; &lt;container&gt;</code> Remove network <code>docker network rm &lt;network-name&gt;</code> Run with network <code>docker run --network &lt;network-name&gt; &lt;image&gt;</code>"},{"location":"containers/reference/#network-types","title":"Network Types","text":"Type Description bridge Default network driver. Containers on the same bridge can communicate. host Remove network isolation, container uses host's network directly. none Disable all networking for the container. overlay Connect containers across multiple Docker hosts (Swarm mode)."},{"location":"containers/reference/#volume-commands","title":"Volume Commands","text":"PodmanDocker Action Command List volumes <code>podman volume ls</code> Create volume <code>podman volume create &lt;volume-name&gt;</code> Inspect volume <code>podman volume inspect &lt;volume-name&gt;</code> Remove volume <code>podman volume rm &lt;volume-name&gt;</code> Remove unused <code>podman volume prune</code> Run with volume <code>podman run -v &lt;volume&gt;:/path &lt;image&gt;</code> Run with bind mount <code>podman run -v /host/path:/container/path &lt;image&gt;</code> Action Command List volumes <code>docker volume ls</code> Create volume <code>docker volume create &lt;volume-name&gt;</code> Inspect volume <code>docker volume inspect &lt;volume-name&gt;</code> Remove volume <code>docker volume rm &lt;volume-name&gt;</code> Remove unused <code>docker volume prune</code> Run with volume <code>docker run -v &lt;volume&gt;:/path &lt;image&gt;</code> Run with bind mount <code>docker run -v /host/path:/container/path &lt;image&gt;</code>"},{"location":"containers/reference/#volume-types","title":"Volume Types","text":"Type Syntax Use Case Named volume <code>-v myvolume:/app/data</code> Persistent data managed by Docker/Podman Bind mount <code>-v /host/path:/container/path</code> Share files between host and container tmpfs mount <code>--tmpfs /app/temp</code> Temporary data in memory (not persisted)"},{"location":"containers/reference/#resource-limits","title":"Resource Limits","text":"<p>Control container resource usage:</p> PodmanDocker <pre><code># Limit memory\npodman run --memory 512m &lt;image&gt;\n\n# Limit memory with swap\npodman run --memory 512m --memory-swap 1g &lt;image&gt;\n\n# Limit CPU cores\npodman run --cpus 0.5 &lt;image&gt;\n\n# Limit CPU shares (relative weight)\npodman run --cpu-shares 512 &lt;image&gt;\n\n# Limit specific CPUs\npodman run --cpuset-cpus 0,1 &lt;image&gt;\n\n# Limit block I/O\npodman run --blkio-weight 500 &lt;image&gt;\n</code></pre> <pre><code># Limit memory\ndocker run --memory 512m &lt;image&gt;\n\n# Limit memory with swap\ndocker run --memory 512m --memory-swap 1g &lt;image&gt;\n\n# Limit CPU cores\ndocker run --cpus 0.5 &lt;image&gt;\n\n# Limit CPU shares (relative weight)\ndocker run --cpu-shares 512 &lt;image&gt;\n\n# Limit specific CPUs\ndocker run --cpuset-cpus 0,1 &lt;image&gt;\n\n# Limit block I/O\ndocker run --blkio-weight 500 &lt;image&gt;\n</code></pre>"},{"location":"containers/reference/#health-checks","title":"Health Checks","text":"<p>Monitor container health:</p> PodmanDocker <pre><code># Run with health check\npodman run \\\n  --health-cmd \"curl -f http://localhost/ || exit 1\" \\\n  --health-interval 30s \\\n  --health-timeout 10s \\\n  --health-retries 3 \\\n  --health-start-period 40s \\\n  &lt;image&gt;\n\n# Check container health status\npodman inspect --format='{{.State.Health.Status}}' &lt;container&gt;\n\n# View health check logs\npodman inspect --format='{{json .State.Health}}' &lt;container&gt; | jq\n</code></pre> <pre><code># Run with health check\ndocker run \\\n  --health-cmd \"curl -f http://localhost/ || exit 1\" \\\n  --health-interval 30s \\\n  --health-timeout 10s \\\n  --health-retries 3 \\\n  --health-start-period 40s \\\n  &lt;image&gt;\n\n# Check container health status\ndocker inspect --format='{{.State.Health.Status}}' &lt;container&gt;\n\n# View health check logs\ndocker inspect --format='{{json .State.Health}}' &lt;container&gt; | jq\n</code></pre>"},{"location":"containers/reference/#docker-compose-podman-compose","title":"Docker Compose / Podman Compose","text":"<p>Manage multi-container applications:</p> PodmanDocker Action Command Start services <code>podman-compose up</code> Start in background <code>podman-compose up -d</code> Stop services <code>podman-compose down</code> View logs <code>podman-compose logs</code> List running services <code>podman-compose ps</code> Build images <code>podman-compose build</code> Pull images <code>podman-compose pull</code> Restart services <code>podman-compose restart</code> Execute in service <code>podman-compose exec &lt;service&gt; &lt;cmd&gt;</code> <p>Podman Compose Installation</p> <pre><code>pip install podman-compose\n</code></pre> Action Command Start services <code>docker compose up</code> Start in background <code>docker compose up -d</code> Stop services <code>docker compose down</code> Stop and remove volumes <code>docker compose down -v</code> View logs <code>docker compose logs</code> Follow logs <code>docker compose logs -f</code> List running services <code>docker compose ps</code> Build images <code>docker compose build</code> Pull images <code>docker compose pull</code> Restart services <code>docker compose restart</code> Execute in service <code>docker compose exec &lt;service&gt; &lt;cmd&gt;</code>"},{"location":"containers/reference/#example-docker-composeyml","title":"Example docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:3000\"\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=postgres://db:5432/myapp\n    depends_on:\n      - db\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  db:\n    image: postgres:16-alpine\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_DB=myapp\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=password\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"containers/reference/#cleanup-commands","title":"Cleanup Commands","text":"PodmanDocker Action Command Remove stopped containers <code>podman container prune</code> Remove unused images <code>podman image prune</code> Remove all unused images <code>podman image prune -a</code> Remove unused volumes <code>podman volume prune</code> Remove unused networks <code>podman network prune</code> Remove all unused data <code>podman system prune</code> Remove everything <code>podman system prune -a --volumes</code> Action Command Remove stopped containers <code>docker container prune</code> Remove unused images <code>docker image prune</code> Remove all unused images <code>docker image prune -a</code> Remove unused volumes <code>docker volume prune</code> Remove unused networks <code>docker network prune</code> Remove all unused data <code>docker system prune</code> Remove everything <code>docker system prune -a --volumes</code>"},{"location":"containers/reference/#running-the-cli","title":"Running the CLI","text":"Local PodmanLocal DockerIBM Cloud <ol> <li> <p>a. RECOMMENDED: Download Podman Desktop here</p> <p>b. Alternative: Brew Installation <pre><code>brew install podman\n</code></pre></p> </li> <li> <p>Create and start your first Podman machine <pre><code>podman machine init\npodman machine start\n</code></pre></p> </li> <li> <p>Verify Podman installation <pre><code>podman info\n</code></pre></p> </li> <li> <p>Test it out: Podman Introduction</p> </li> </ol> <ol> <li> <p>Install Docker Desktop here</p> </li> <li> <p>Verify installation <pre><code>docker version\n</code></pre></p> </li> <li> <p>Test it out: Getting Started with Docker</p> </li> </ol> <ol> <li> <p>Install ibmcloud CLI <pre><code>curl -fsSL https://clis.cloud.ibm.com/install/osx | sh\n</code></pre></p> </li> <li> <p>Verify installation <pre><code>ibmcloud help\n</code></pre></p> </li> <li> <p>Configure environment. Go to cloud.ibm.com -&gt; click on your profile -&gt; Log into CLI and API and copy IBM Cloud CLI command. It will look something like this: <pre><code>ibmcloud login -a https://cloud.ibm.com -u passcode -p &lt;password&gt;\n</code></pre></p> </li> <li> <p>Log into container registry through IBM Cloud <pre><code>ibmcloud cr login --client docker\n# or\nibmcloud cr login --client podman\n</code></pre></p> </li> </ol>"},{"location":"containers/reference/#security-commands","title":"Security Commands","text":"PodmanDocker <pre><code># Run rootless (default in Podman)\npodman run &lt;image&gt;\n\n# Run with read-only filesystem\npodman run --read-only &lt;image&gt;\n\n# Drop all capabilities\npodman run --cap-drop ALL &lt;image&gt;\n\n# Add specific capability\npodman run --cap-add NET_BIND_SERVICE &lt;image&gt;\n\n# Run with no new privileges\npodman run --security-opt no-new-privileges &lt;image&gt;\n\n# Run with SELinux label\npodman run --security-opt label=level:s0:c100,c200 &lt;image&gt;\n</code></pre> <pre><code># Run with read-only filesystem\ndocker run --read-only &lt;image&gt;\n\n# Drop all capabilities\ndocker run --cap-drop ALL &lt;image&gt;\n\n# Add specific capability\ndocker run --cap-add NET_BIND_SERVICE &lt;image&gt;\n\n# Run with no new privileges\ndocker run --security-opt no-new-privileges &lt;image&gt;\n\n# Run as non-root user\ndocker run --user 1001:1001 &lt;image&gt;\n\n# Run with seccomp profile\ndocker run --security-opt seccomp=profile.json &lt;image&gt;\n</code></pre>"},{"location":"containers/reference/#activities","title":"Activities","text":"Task Description Link Time IBM Container Registry Build and Deploy Run using IBM Container Registry IBM Container Registry 30 min Docker Lab Running a Sample Application on Docker Docker Lab 30 min <p>Once you have completed these tasks, you should have a base understanding of containers and how to use Docker and Podman.</p>"},{"location":"containers/reference/#additional-resources","title":"Additional Resources","text":"<ul> <li>Docker CLI Reference</li> <li>Podman CLI Reference</li> <li>Docker Compose Reference</li> <li>Container Security Best Practices</li> </ul>"},{"location":"containers/registry-tutorials/","title":"Registry Tutorials","text":"<p>This page provides step-by-step tutorials for working with popular container registries. Select a registry below to get started.</p> IBM Cloud RegistryRed Hat QuayDocker Hub"},{"location":"containers/registry-tutorials/#ibm-cloud-container-registry","title":"IBM Cloud Container Registry","text":"<p>IBM Cloud Container Registry provides a multi-tenant, highly available, and scalable private image registry with integrated vulnerability scanning.</p>"},{"location":"containers/registry-tutorials/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker or Podman installed and running</li> <li>IBM Cloud account (Sign up here)</li> <li>IBM Cloud CLI installed</li> </ul>"},{"location":"containers/registry-tutorials/#tutorial","title":"Tutorial","text":"PodmanDocker <p>1. Install the IBM Cloud CLI</p> <pre><code>curl -fsSL https://clis.cloud.ibm.com/install/osx | sh\n</code></pre> <p>2. Install the Container Registry plugin</p> <pre><code>ibmcloud plugin install container-registry\n</code></pre> <p>3. Login to IBM Cloud</p> <pre><code>ibmcloud login\n</code></pre> <p>For federated accounts, use:</p> <pre><code>ibmcloud login --sso\n</code></pre> <p>4. Set up a namespace</p> <p>Create a namespace to organize your images:</p> <pre><code>ibmcloud cr namespace-add my_namespace\n</code></pre> <p>Namespace Names</p> <p>Namespace names must be unique across all IBM Cloud accounts in the same region. Choose a descriptive name like <code>mycompany-dev</code>.</p> <p>Verify the namespace was created:</p> <pre><code>ibmcloud cr namespace-list\n</code></pre> <p>5. Login to the registry</p> <pre><code>ibmcloud cr login --client podman\n</code></pre> <p>6. Pull a base image</p> <pre><code>podman pull docker.io/library/hello-world:latest\n</code></pre> <p>7. Tag for IBM Cloud Registry</p> <p>Tag the image with your region and namespace:</p> <pre><code>podman tag hello-world:latest us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>Regional Endpoints</p> Region Endpoint US South <code>us.icr.io</code> EU Central <code>de.icr.io</code> UK South <code>uk.icr.io</code> AP North <code>jp.icr.io</code> AP South <code>au.icr.io</code> <p>8. Push to IBM Cloud Registry</p> <pre><code>podman push us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>9. Verify your push</p> <pre><code>ibmcloud cr image-list\n</code></pre> <p>10. Scan for vulnerabilities</p> <p>IBM Cloud Registry includes Vulnerability Advisor:</p> <pre><code>ibmcloud cr va us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>You can also view your images in the IBM Cloud Console.</p> <p>1. Install the IBM Cloud CLI</p> <pre><code>curl -fsSL https://clis.cloud.ibm.com/install/osx | sh\n</code></pre> <p>2. Install the Container Registry plugin</p> <pre><code>ibmcloud plugin install container-registry\n</code></pre> <p>3. Login to IBM Cloud</p> <pre><code>ibmcloud login\n</code></pre> <p>For federated accounts, use:</p> <pre><code>ibmcloud login --sso\n</code></pre> <p>4. Set up a namespace</p> <p>Create a namespace to organize your images:</p> <pre><code>ibmcloud cr namespace-add my_namespace\n</code></pre> <p>Namespace Names</p> <p>Namespace names must be unique across all IBM Cloud accounts in the same region. Choose a descriptive name like <code>mycompany-dev</code>.</p> <p>Verify the namespace was created:</p> <pre><code>ibmcloud cr namespace-list\n</code></pre> <p>5. Login to the registry</p> <pre><code>ibmcloud cr login --client docker\n</code></pre> <p>6. Pull a base image</p> <pre><code>docker pull hello-world:latest\n</code></pre> <p>7. Tag for IBM Cloud Registry</p> <p>Tag the image with your region and namespace:</p> <pre><code>docker tag hello-world:latest us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>Regional Endpoints</p> Region Endpoint US South <code>us.icr.io</code> EU Central <code>de.icr.io</code> UK South <code>uk.icr.io</code> AP North <code>jp.icr.io</code> AP South <code>au.icr.io</code> <p>8. Push to IBM Cloud Registry</p> <pre><code>docker push us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>9. Verify your push</p> <pre><code>ibmcloud cr image-list\n</code></pre> <p>10. Scan for vulnerabilities</p> <p>IBM Cloud Registry includes Vulnerability Advisor:</p> <pre><code>ibmcloud cr va us.icr.io/my_namespace/hello-world:v1.0\n</code></pre> <p>You can also view your images in the IBM Cloud Console.</p>"},{"location":"containers/registry-tutorials/#ibm-cloud-registry-features","title":"IBM Cloud Registry Features","text":"Feature Description Vulnerability Advisor Automatic security scanning with detailed reports IAM Integration Fine-grained access control with IBM Cloud IAM Multi-Region Deploy images across multiple regions Retention Policies Automatic cleanup of old images Private Network Access via IBM Cloud private network"},{"location":"containers/registry-tutorials/#resources","title":"Resources","text":"<p>IBM Cloud Registry Documentation </p>"},{"location":"containers/registry-tutorials/#red-hat-quay","title":"Red Hat Quay","text":"<p>Red Hat Quay is an enterprise container registry that provides security scanning, geo-replication, and detailed access controls. It's commonly used in OpenShift environments.</p>"},{"location":"containers/registry-tutorials/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker or Podman installed and running</li> <li>A free Quay.io account (Sign up here)</li> </ul>"},{"location":"containers/registry-tutorials/#tutorial_1","title":"Tutorial","text":"PodmanDocker <p>1. Login to Quay.io</p> <pre><code>podman login quay.io\n</code></pre> <p>Enter your Quay.io username and password when prompted:</p> <pre><code>Username: your_username\nPassword: your_password\nLogin Succeeded!\n</code></pre> <p>2. Pull a base image</p> <p>Pull an image from Docker Hub to use as a starting point:</p> <pre><code>podman pull docker.io/library/alpine:latest\n</code></pre> <p>3. Create a simple container</p> <p>Run a container and make a small modification:</p> <pre><code>podman run -it --name my-alpine docker.io/library/alpine:latest /bin/sh\n</code></pre> <p>Inside the container, create a file:</p> <pre><code>echo \"Hello from Quay!\" &gt; /hello.txt\nexit\n</code></pre> <p>4. Commit the container to a new image</p> <pre><code>podman commit my-alpine my-quay-image:v1.0\n</code></pre> <p>5. Tag for Quay.io</p> <p>Tag the image with your Quay.io username and repository name:</p> <pre><code>podman tag my-quay-image:v1.0 quay.io/your_username/my-quay-image:v1.0\n</code></pre> <p>Replace <code>your_username</code></p> <p>Replace <code>your_username</code> with your actual Quay.io username.</p> <p>6. Push to Quay.io</p> <pre><code>podman push quay.io/your_username/my-quay-image:v1.0\n</code></pre> <p>7. Verify your push</p> <p>Visit Quay.io Repositories to see your pushed image.</p> <p>Repository Visibility</p> <p>By default, new repositories on Quay.io are private. You can change visibility in the repository settings.</p> <p>8. Clean up</p> <pre><code>podman rm my-alpine\npodman rmi my-quay-image:v1.0\n</code></pre> <p>1. Login to Quay.io</p> <pre><code>docker login quay.io\n</code></pre> <p>Enter your Quay.io username and password when prompted:</p> <pre><code>Username: your_username\nPassword: your_password\nLogin Succeeded!\n</code></pre> <p>2. Pull a base image</p> <p>Pull an image from Docker Hub to use as a starting point:</p> <pre><code>docker pull alpine:latest\n</code></pre> <p>3. Create a simple container</p> <p>Run a container and make a small modification:</p> <pre><code>docker run -it --name my-alpine alpine:latest /bin/sh\n</code></pre> <p>Inside the container, create a file:</p> <pre><code>echo \"Hello from Quay!\" &gt; /hello.txt\nexit\n</code></pre> <p>4. Commit the container to a new image</p> <pre><code>docker commit my-alpine my-quay-image:v1.0\n</code></pre> <p>5. Tag for Quay.io</p> <p>Tag the image with your Quay.io username and repository name:</p> <pre><code>docker tag my-quay-image:v1.0 quay.io/your_username/my-quay-image:v1.0\n</code></pre> <p>Replace <code>your_username</code></p> <p>Replace <code>your_username</code> with your actual Quay.io username.</p> <p>6. Push to Quay.io</p> <pre><code>docker push quay.io/your_username/my-quay-image:v1.0\n</code></pre> <p>7. Verify your push</p> <p>Visit Quay.io Repositories to see your pushed image.</p> <p>Repository Visibility</p> <p>By default, new repositories on Quay.io are private. You can change visibility in the repository settings.</p> <p>8. Clean up</p> <pre><code>docker rm my-alpine\ndocker rmi my-quay-image:v1.0\n</code></pre>"},{"location":"containers/registry-tutorials/#quayio-features","title":"Quay.io Features","text":"<p>Once your image is pushed, Quay.io automatically provides:</p> Feature Description Security Scanning Automatic vulnerability scanning of your images Build Triggers Automatically build images from Git repositories Robot Accounts Service accounts for CI/CD automation Teams &amp; Permissions Fine-grained access control Image Expiration Automatic cleanup of old tags"},{"location":"containers/registry-tutorials/#resources_1","title":"Resources","text":"<p>Quay.io Documentation </p>"},{"location":"containers/registry-tutorials/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub is the default registry for Docker and contains millions of public images. It's the easiest way to get started with container registries.</p>"},{"location":"containers/registry-tutorials/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Docker or Podman installed and running</li> <li>A free Docker Hub account (Sign up here)</li> </ul>"},{"location":"containers/registry-tutorials/#tutorial_2","title":"Tutorial","text":"PodmanDocker <p>1. Search for images</p> <pre><code>podman search docker.io/nginx\n</code></pre> <p>2. Pull an image</p> <pre><code>podman pull docker.io/library/nginx:1.25-alpine\n</code></pre> <p>3. Login to Docker Hub</p> <pre><code>podman login docker.io\n</code></pre> <p>Enter your Docker Hub username and password when prompted.</p> <p>4. Tag your image</p> <pre><code>podman tag myapp:latest docker.io/yourusername/myapp:v1.0\n</code></pre> <p>5. Push to Docker Hub</p> <pre><code>podman push docker.io/yourusername/myapp:v1.0\n</code></pre> <p>6. Verify your push</p> <p>Visit Docker Hub and navigate to your repositories to see your pushed image.</p> <p>1. Search for images</p> <pre><code>docker search nginx\n</code></pre> <p>2. Pull an image</p> <pre><code>docker pull nginx:1.25-alpine\n</code></pre> <p>3. Login to Docker Hub</p> <pre><code>docker login\n</code></pre> <p>Enter your Docker Hub username and password when prompted.</p> <p>4. Tag your image</p> <pre><code>docker tag myapp:latest yourusername/myapp:v1.0\n</code></pre> <p>5. Push to Docker Hub</p> <pre><code>docker push yourusername/myapp:v1.0\n</code></pre> <p>6. Verify your push</p> <p>Visit Docker Hub and navigate to your repositories to see your pushed image.</p>"},{"location":"containers/registry-tutorials/#resources_2","title":"Resources","text":"<p>Docker Hub Documentation </p>"},{"location":"devops/","title":"What is DevOps?","text":"<p>DevOps has recently become a popular buzzword in the Cloud World. It varies from business to business and it means a lot different things to different people. In traditional IT, organizations have separate teams for Development and Operations. The development team is responsible for coding and operations team is responsible for releasing it to production. When it comes to this two different teams, there will always be some sort of differences. It may be due to the usage of different system environments, software libraries etc. In order to level this up, DevOps came into play.</p> <p>\u201cDevOps is a philosophy, a cultural shift that merges operations with development and demands a linked toolchain of technologies to facilitate collaborative change. DevOps toolchains \u2026 can include dozens of non-collaborative tools, making the task of automation a technically complex and arduous one.\u201d - Gartner</p> <p></p> <p>These days every business has critical applications which can never go down. Some of the examples are as follows.</p> <p></p> <p>In order to make sure that these applications are up and running smoothly, we need DevOps.</p> <p>Adopting DevOps allows enterprises to create, maintain and improve their applications at a faster pace than the traditional methods. Today, most of the global organizations adopted DevOps.</p>"},{"location":"devops/#presentations","title":"Presentations","text":"<p>Tekton Overview </p> <p>GitOps Overview </p>"},{"location":"devops/#benefits-of-devops","title":"Benefits of DevOps","text":"<ul> <li>Continuous software delivery</li> <li>High quality software</li> <li>Increased speed and faster problem resolution</li> <li>Increased reliability</li> <li>Easier to manage the software</li> <li>Collaboration and enhanced team communication</li> <li>Customer satisfaction etc.</li> </ul>"},{"location":"devops/#understanding-devops","title":"Understanding DevOps","text":"<p>Like we mentioned before, often development teams and operation teams are in conflict with each other. Developers keeping changing the software to include new features where as operation engineers wants to keep the system stable.</p> <ul> <li>Their goals are different.</li> <li>They use different processes.</li> <li>They use different tools.</li> </ul> <p>All these may be different reasons for the gap between these two teams.</p> <p>To solve this gap between the two teams, we need DevOps. It closes the gap by aligning incentives and sharing approaches for tools and processes. It helps us to streamline the software delivery process. From the time we begin the project till its delivery, it helps us to improve the cycle time by emphasizing the learning by gathering feedback from production to development.</p> <p>It includes several aspects like the below.</p> <ul> <li>Automation - It is quite essential for DevOps. It helps us to gather quick feedback.</li> <li>Culture - Processes and tools are important. But, people are always more important.</li> <li>Measurement - Shared incentives are important. Quality is critical.</li> <li>Sharing - Need a Culture where people can share ideas, processes and tools.</li> </ul> <p> </p>"},{"location":"devops/#where-to-start","title":"Where to start ?","text":"<p>Understanding the eco system of your software is important. Identify all the environments like dev, test, prod etc. you have in your system and how the delivery happens from end to end.</p> <ul> <li>Define continuous delivery</li> <li>Establish proper collaboration between teams</li> <li>Make sure the teams are on same pace</li> <li>Identify the pain points in your system and start working on them.</li> </ul>"},{"location":"devops/#devops-best-practices","title":"DevOps Best Practices","text":"<p>These are some of the standard practices adopted in DevOps.</p> <ul> <li>Source Code Management</li> <li>Code Review</li> <li>Configuration Management</li> <li>Build Management</li> <li>Artifact Repository Management</li> <li>Release Management</li> <li>Test Automation</li> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Continuous Deployment</li> <li>Infrastructure As Code</li> <li>Automation</li> <li>Key Application Performance Monitoring/Indicators</li> </ul> <p>Source Code Management</p> <p>Source Code Management (SCM) systems helps to maintain the code base. It allows multiple developers to work on the code concurrently. It prevents them from overwriting the code and helps them to work in parallel from different locations.</p> <p>Collaboration is an important concept in devOps and SCM helps us to achieve it by coordination of services across the development team. It also tracks co-authoring, collaboration, and individual contributions. It helps the developers to audit the code changes. It also allows rollbacks if required. It also enables backup and allows recovery when required.</p> <p>Code Review</p> <p>Code reviews allows the developer to improve the quality of code. They help us to identify the problems in advance. By reviewing the code, we can fix some of the problems like memory leaks, buffer overflow, formatting errors etc.</p> <p>This process improves the collaboration across the team. Also, code defects are identified and removed before merging them with the main stream there by improving the quality of the code.</p> <p>Configuration Management</p> <p>Configuration Management is managing the configurations by identifying, verifying, and maintaining them. This is done for both software and hardware. The configuration management tools make sure that configurations are properly configured across different systems as per the requirements.</p> <p>This helps to analyze the impact on the systems due to configurations. It makes sure the provisioning is done correctly on different systems like dev, QA, prod etc. It simplifies the coordination between development and operations teams.</p> <p>Build Management</p> <p>Build Management helps to assemble the build environment by packaging all the required components such as the source code, dependencies, etc of the software application together in to a workable unit. Builds can be done manually, on-demand or automated.</p> <p>It ensures that the software is stable and it is reusable. It improves the quality of the software and makes sure it is reliable. It also increases the efficiency.</p> <p>Artifact Repository Management</p> <p>Artifact Repository Management system is used to manage the builds. It is dedicated server which is used to store all the binaries which were outputs of the successful builds.</p> <p>It manages the life cycles of different artifacts. It helps you to easily share the builds across the team. It controls access to the build artifacts by access control.</p> <p>Release Management</p> <p>Release management is a part of software development lifecycle which manages the release from development till deployment to support. Requests keep coming for the addition of the new features. Also, sometimes there may be need to change the existing functionality. This is when the cycle begins for the release management. Once, the new feature or change is approved, it is designed, built, tested, reviewed, and after acceptance, deployed to production. After this, it goes to maintainence and even at this point, there may be need for enhancement. If that is the case, it will be a new cycle again.</p> <p>It helps us to track all the phases and status of deployments in different environments.</p> <p>Test Automation</p> <p>Manual testing takes lots of time. We can automate some of the manual tests which are repetitive, time consuming, and have defined input by test automation.</p> <p>Automatic tests helps to improve the code quality, reduces the amount of time spent on testing, and improves the effectiveness of the overall testing life cycle.</p> <p>Continuous Integration</p> <p>Continuous integration allows the developers to continuously integrate the code they developed. Whenever a latest code change is made and committed to the source control system, the source code is rebuilt and this is then forwarded to testing.</p> <p>With this, the latest code is always available, the builds are faster and the tests are quick.</p> <p>Continuous Delivery</p> <p>Continuous Delivery is the next step to Continuous Integration. In the integration, the code is built and tested. Now in the delivery, this is taken to staging environment. This is done in small frequencies and it makes sure the functionality of the software is stable.</p> <p>It reduces the manual overhead. The code is continuously delivered and constantly reviewed.</p> <p>Continuous Deployment</p> <p>Continuous Deployment comes after Continuous Delivery. In the deployment stage, the code is deployed to the production environment. The entire process is automated in this stage.</p> <p>This allows faster software releases. Improves the collaboration across the teams. Enhances the code quality.</p> <p>Infrastructure As Code</p> <p>Infrastructure as Code is defining the infrastructure services as a software code. they are defines as configuration files. Traditionally, in on-premise application, these are run by system administrators but in cloud, the infrastructure is maintained like any other software code.</p> <p>Helps us to change the system configuration quickly. Tracking is easy and end to end testing is possible. Infrastructure availability is high.</p> <p>Automation</p> <p>Automation is key part to DevOps. Without automation, DevOps is not efficient.</p> <p>Automation comes into play whenever there is a repetitive task. Developers can automate infrastructure, applications, load balancers, etc.</p> <p>Key Application Performance Monitoring/Indicators</p> <p>DevOps is all about measuring the metrics and feedback, with continuous improvement processes. Collecting metrics and monitoring the software plays an important role. Different measures like uptime versus downtime, resolutions time lines etc. helps us to understand the performance of the system.</p>"},{"location":"devops/#devops-in-twelve-factor-apps","title":"Devops in Twelve factor apps","text":"<p>If you are new to Twelve factor methodology, have a look here.</p>"},{"location":"devops/#devops-reference-architecture","title":"DevOps Reference Architecture","text":"<ol> <li>Collaboration tools enable a culture of innovation. Developers, designers, operations teams, and managers must communicate constantly. Development and operations tools must be integrated to post updates and alerts as new builds are completed and deployed and as performance is monitored. The team can discuss the alerts as a group in the context of the tool.</li> <li>As the team brainstorms ideas, responds to feedback and metrics, and fixes defects, team members create work items and rank them in the backlog. The team work on items from the top of the backlog, delivering to production as they complete work.</li> <li>Developers write source code in a code editor to implement the architecture. They construct, change, and correct applications by using various coding models and tools.</li> <li>Developers manage the versions and configuration of assets, merge changes, and manage the integration of changes. The source control tool that a team uses should support social coding.</li> <li>Developers compile, package, and prepare software assets. They need tools that can assess the quality of the code that is being delivered to source control. Those assessments are done before delivery, are associated with automated build systems, and include practices such as code reviews, unit tests, code quality scans, and security scans.</li> <li>Binary files and other output from the build are sent to and managed in a build artifact repository.</li> <li>The release is scheduled. The team needs tools that support release communication and managing, preparing, and deploying releases.</li> <li>The team coordinates the manual and automated processes that are required for the solution to operate effectively. The team must strive towards continuous delivery with zero downtime. A/B deployments can help to gauge the effectiveness of new changes.</li> <li>The team must understand the application and the options for the application's runtime environment, security, management, and release requirements.</li> <li>Depending on the application requirements, some or all of the application stack must be considered, including middleware, the operating system, and virtual machines.</li> <li>The team must ensure that all aspects of the application and its supporting infrastructure are secured.</li> <li>The team plans, configures, monitors, defines criteria, and reports on application availability and performance. Predictive analytics can indicate problems before they occur.</li> <li>The right people on the team or systems are notified when issues occur.</li> <li>The team manages the process for responding to operations incidents, and delivers the changes to fix any incidents.</li> <li>The team uses analytics to learn how users interact with the application and measure success through metrics.</li> <li>When users interact with the application, they can provide feedback on their requirements and how the application is meeting them, which is captured by analytics as well.</li> <li>DevOps engineers manage the entire application lifecycle while they respond to feedback and analytics from the running application.</li> <li>The enterprise network is protected by a firewall and must be accessed through transformation and connectivity services and secure messaging services.</li> <li>The security team uses the user directory throughout the flow. The directory contains information about the user accounts for the enterprise.</li> </ol> <p>For a cloud native implementation, the reference architecture will be as follows.</p> <p></p>"},{"location":"devops/devopsTools/","title":"DevOps Tools","text":"<p>DevOps relies on a variety of tools to automate and streamline the software development lifecycle. These tools help teams collaborate more effectively, automate repetitive tasks, and deliver software faster and more reliably.</p>"},{"location":"devops/devopsTools/#categories-of-devops-tools","title":"Categories of DevOps Tools","text":"<p>DevOps tools can be organized into several categories based on the stage of the development lifecycle they support:</p>"},{"location":"devops/devopsTools/#source-code-management","title":"Source Code Management","text":"<p>Source code management (SCM) tools help teams track changes to code, collaborate on development, and maintain version history.</p> <ul> <li>Git - The most widely used distributed version control system</li> <li>GitHub - Cloud-based Git repository hosting with collaboration features</li> <li>GitLab - Complete DevOps platform with built-in CI/CD</li> <li>Bitbucket - Git repository management with Jira integration</li> </ul>"},{"location":"devops/devopsTools/#continuous-integration-continuous-delivery","title":"Continuous Integration / Continuous Delivery","text":"<p>CI/CD tools automate the building, testing, and deployment of applications.</p> <ul> <li>Tekton - Kubernetes-native CI/CD building blocks for creating pipelines</li> <li>Jenkins - Open-source automation server with extensive plugin ecosystem</li> <li>GitHub Actions - CI/CD integrated directly into GitHub repositories</li> <li>GitLab CI/CD - Built-in CI/CD capabilities within GitLab</li> <li>Travis CI - Cloud-based CI service for open source projects</li> <li>CircleCI - Cloud-native CI/CD platform</li> </ul>"},{"location":"devops/devopsTools/#container-orchestration","title":"Container Orchestration","text":"<p>These tools manage containerized applications at scale.</p> <ul> <li>Kubernetes - Industry-standard container orchestration platform</li> <li>Red Hat OpenShift - Enterprise Kubernetes platform with additional features</li> <li>Docker Swarm - Native clustering for Docker containers</li> </ul>"},{"location":"devops/devopsTools/#gitops-continuous-deployment","title":"GitOps &amp; Continuous Deployment","text":"<p>GitOps tools use Git as the source of truth for deployment automation.</p> <ul> <li>ArgoCD - Declarative GitOps continuous delivery for Kubernetes</li> <li>Flux - GitOps toolkit for Kubernetes</li> <li>Spinnaker - Multi-cloud continuous delivery platform</li> </ul>"},{"location":"devops/devopsTools/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>IaC tools allow infrastructure to be defined and managed through code.</p> <ul> <li>Terraform - Multi-cloud infrastructure provisioning tool</li> <li>Ansible - Agentless automation and configuration management</li> <li>Pulumi - Infrastructure as code using familiar programming languages</li> <li>AWS CloudFormation - Infrastructure as code for AWS resources</li> </ul>"},{"location":"devops/devopsTools/#configuration-management","title":"Configuration Management","text":"<p>These tools help maintain consistent configurations across environments.</p> <ul> <li>Ansible - Simple, agentless automation</li> <li>Chef - Infrastructure automation with Ruby-based DSL</li> <li>Puppet - Model-driven configuration management</li> </ul>"},{"location":"devops/devopsTools/#containerization","title":"Containerization","text":"<p>Container tools package applications with their dependencies for consistent deployment.</p> <ul> <li>Docker - Industry-standard containerization platform</li> <li>Podman - Daemonless container engine compatible with Docker</li> <li>Buildah - Tool for building OCI container images</li> </ul>"},{"location":"devops/devopsTools/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>Monitoring tools provide visibility into application and infrastructure health.</p> <ul> <li>Prometheus - Open-source monitoring and alerting toolkit</li> <li>Grafana - Visualization and analytics platform</li> <li>Datadog - Cloud monitoring and analytics</li> <li>Splunk - Log management and analysis</li> <li>ELK Stack - Elasticsearch, Logstash, and Kibana for log analysis</li> </ul>"},{"location":"devops/devopsTools/#artifact-management","title":"Artifact Management","text":"<p>Artifact repositories store and manage build artifacts and dependencies.</p> <ul> <li>JFrog Artifactory - Universal artifact repository</li> <li>Nexus Repository - Component and artifact management</li> <li>Docker Hub - Container image registry</li> <li>Quay.io - Enterprise container registry</li> </ul>"},{"location":"devops/devopsTools/#collaboration-communication","title":"Collaboration &amp; Communication","text":"<p>These tools facilitate team communication and collaboration.</p> <ul> <li>Slack - Team messaging and collaboration</li> <li>Microsoft Teams - Unified communication platform</li> <li>Jira - Project and issue tracking</li> <li>Confluence - Team collaboration and documentation</li> </ul>"},{"location":"devops/devopsTools/#choosing-the-right-tools","title":"Choosing the Right Tools","text":"<p>When selecting DevOps tools, consider these factors:</p> <ol> <li>Integration capabilities - How well does the tool integrate with your existing stack?</li> <li>Scalability - Can the tool grow with your organization?</li> <li>Community and support - Is there active community support and documentation?</li> <li>Learning curve - How quickly can your team become productive?</li> <li>Cost - What are the licensing and operational costs?</li> </ol>"},{"location":"devops/devopsTools/#tools-used-in-this-bootcamp","title":"Tools Used in This Bootcamp","text":"<p>Throughout this bootcamp, you will gain hands-on experience with several key DevOps tools:</p> Tool Purpose Lab Tekton CI/CD Pipelines Tekton Lab ArgoCD GitOps Deployment ArgoCD Lab Jenkins CI/CD Automation Jenkins Lab IBM Toolchain IBM Cloud DevOps IBM Toolchain Lab <p>These tools represent modern approaches to implementing CI/CD pipelines and GitOps workflows in cloud-native environments.</p>"},{"location":"devops/gitops/","title":"What is GitOps","text":"<p>GitOps is a modern approach to continuous deployment that uses Git as the single source of truth for declarative infrastructure and application configurations. It extends the principles that development teams already use for application source code\u2014version control, code review, and collaboration\u2014to infrastructure and operations.</p> <p>In a GitOps workflow, the desired state of your entire system is stored in Git repositories. Any changes to the system are made through pull requests, which provide an audit trail and enable collaboration. Automated processes then ensure that the actual state of the running system matches the desired state defined in Git.</p>"},{"location":"devops/gitops/#core-principles-of-gitops","title":"Core Principles of GitOps","text":"<p>GitOps is built on four key principles:</p> <ol> <li> <p>Declarative Configuration - The entire system, including infrastructure and applications, is described declaratively. This means you define what you want, not how to achieve it.</p> </li> <li> <p>Version Controlled - All configuration is stored in Git, providing a complete history of changes, the ability to rollback, and an audit trail for compliance.</p> </li> <li> <p>Automated Delivery - Approved changes are automatically applied to the system. Once a pull request is merged, the deployment happens without manual intervention.</p> </li> <li> <p>Software Agents - Software agents (like ArgoCD or Flux) continuously monitor the actual state and reconcile any drift from the desired state stored in Git.</p> </li> </ol>"},{"location":"devops/gitops/#how-gitops-works","title":"How GitOps Works","text":"<p>The typical GitOps workflow follows these steps:</p> <ol> <li>A developer makes changes to the application or infrastructure configuration</li> <li>The changes are submitted as a pull request to the Git repository</li> <li>The team reviews and approves the pull request</li> <li>Once merged, an automated agent detects the change</li> <li>The agent applies the changes to the target environment</li> <li>The agent continuously monitors and ensures the running state matches Git</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Developer   \u2502\u2500\u2500\u2500\u25b6\u2502     Git      \u2502\u2500\u2500\u2500\u25b6\u2502  GitOps      \u2502\n\u2502  makes       \u2502    \u2502  Repository  \u2502    \u2502  Agent       \u2502\n\u2502  changes     \u2502    \u2502  (PR/Merge)  \u2502    \u2502  (ArgoCD)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n                                               \u25bc\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502  Kubernetes  \u2502\n                                        \u2502  Cluster     \u2502\n                                        \u2502  (Deployed)  \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"devops/gitops/#gitops-vs-traditional-cicd","title":"GitOps vs Traditional CI/CD","text":"Aspect Traditional CI/CD GitOps Source of Truth CI/CD pipeline configuration Git repository Deployment Trigger Pipeline execution Git commit/merge State Management Imperative (push-based) Declarative (pull-based) Drift Detection Manual or scripted Automatic and continuous Rollback Re-run previous pipeline Git revert Audit Trail Pipeline logs Git history"},{"location":"devops/gitops/#benefits-of-gitops","title":"Benefits of GitOps","text":"<p>Adopting GitOps provides numerous advantages for development and operations teams:</p>"},{"location":"devops/gitops/#increased-reliability","title":"Increased Reliability","text":"<ul> <li>Consistent deployments - The same declarative configuration is applied every time, eliminating \"works on my machine\" issues</li> <li>Easy rollbacks - If something goes wrong, simply revert the Git commit to restore the previous working state</li> <li>Drift detection - Agents automatically detect and correct any unauthorized changes to the running system</li> </ul>"},{"location":"devops/gitops/#enhanced-security","title":"Enhanced Security","text":"<ul> <li>Reduced access requirements - Developers don't need direct access to clusters; they only need access to Git</li> <li>Complete audit trail - Every change is tracked in Git with who made it, when, and why</li> <li>Pull request reviews - All changes go through code review before being applied</li> </ul>"},{"location":"devops/gitops/#improved-developer-experience","title":"Improved Developer Experience","text":"<ul> <li>Familiar workflows - Developers use the same Git-based workflows they already know</li> <li>Self-service deployments - Teams can deploy by simply merging a pull request</li> <li>Faster onboarding - New team members can understand the system by reading the Git repository</li> </ul>"},{"location":"devops/gitops/#operational-benefits","title":"Operational Benefits","text":"<ul> <li>Disaster recovery - The entire system state can be recreated from Git</li> <li>Multi-cluster management - Easily manage configurations across multiple environments</li> <li>Compliance - Git history provides the documentation needed for audits</li> </ul>"},{"location":"devops/gitops/#common-gitops-tools","title":"Common GitOps Tools","text":"<p>Several tools implement GitOps principles:</p> <ul> <li>ArgoCD - A declarative, GitOps continuous delivery tool for Kubernetes</li> <li>Flux - A set of continuous delivery solutions for Kubernetes</li> <li>Jenkins X - CI/CD solution with built-in GitOps capabilities</li> <li>Tekton - Kubernetes-native CI/CD building blocks that can be used in GitOps workflows</li> </ul>"},{"location":"devops/gitops/#getting-started-with-gitops","title":"Getting Started with GitOps","text":"<p>To begin your GitOps journey:</p> <ol> <li>Store all your Kubernetes manifests or Helm charts in a Git repository</li> <li>Set up a GitOps agent (like ArgoCD) in your cluster</li> <li>Configure the agent to watch your Git repository</li> <li>Make changes through pull requests and let the agent handle deployments</li> </ol> <p>For hands-on experience with GitOps, check out the ArgoCD Lab in this bootcamp.</p>"},{"location":"devops/argocd/","title":"Continuous Deployment","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/argocd/#what-is-gitops","title":"What is GitOps?","text":"<p>GitOps in short is a set of practices to use Git pull requests to manage infrastructure and application configurations. Git repository in GitOps is considered the only source of truth and contains the entire state of the system so that the trail of changes to the system state are visible and auditable.</p> <ul> <li>Traceability of changes in GitOps is no novelty in itself as this approach is almost universally employed for the application source code. However GitOps advocates applying the same principles (reviews, pull requests, tagging, etc) to infrastructure and application configuration so that teams can benefit from the same assurance as they do for the application source code.</li> <li>Although there is no precise definition or agreed upon set of rules, the following principles are an approximation of what constitutes a GitOps practice:</li> <li>Declarative description of the system is stored in Git (configs, monitoring, etc)</li> <li>Changes to the state are made via pull requests</li> <li>Git push reconciled with the state of the running system with the state in the Git repository</li> </ul>"},{"location":"devops/argocd/#argocd-overview","title":"ArgoCD Overview","text":"<p>ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications by continuously monitoring Git repositories and synchronizing the desired application state with the live state in Kubernetes clusters.</p>"},{"location":"devops/argocd/#key-features","title":"Key Features","text":"<ul> <li>Declarative and version controlled - Application definitions, configurations, and environments are declarative and version controlled in Git</li> <li>Automated deployment - Automatically syncs application state from Git to Kubernetes</li> <li>Multi-cluster support - Manage deployments across multiple Kubernetes clusters</li> <li>SSO Integration - Integrates with OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, and Microsoft</li> <li>Rollback capabilities - Roll back to any application state committed in the Git repository</li> <li>Health status analysis - Real-time view of application deployment health</li> <li>Web UI and CLI - Visualize and manage applications through a web interface or command line</li> <li>Webhook integration - Trigger deployments automatically from Git events</li> </ul>"},{"location":"devops/argocd/#how-argocd-works","title":"How ArgoCD Works","text":"<ol> <li>You define your application's desired state in a Git repository (Kubernetes manifests, Helm charts, or Kustomize)</li> <li>ArgoCD continuously monitors the Git repository for changes</li> <li>When changes are detected, ArgoCD compares the desired state with the live state</li> <li>ArgoCD automatically or manually syncs the cluster to match the desired state</li> <li>You can visualize the sync status and health of your applications in real-time</li> </ol>"},{"location":"devops/argocd/#presentations","title":"Presentations","text":"<p>GitOps Overview </p>"},{"location":"devops/argocd/#activities","title":"Activities","text":"<p>These activities give you a chance to walkthrough building CD pipelines using ArgoCD.</p> <p>These tasks assume that you have:  - Reviewed the Continuous Deployment concept page.</p> Task Description Link Time Walkthroughs GitOps Introduction to GitOps with OpenShift Learn OpenShift GitOps 20 min Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD 30 min <p>Once you have completed these tasks, you will have created an ArgoCD deployment and have an understanding of Continuous Deployment.</p>"},{"location":"devops/ibm-toolchain/","title":"IBM ToolChain","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"devops/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     <pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.</p> </li> </ol>"},{"location":"devops/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"devops/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"devops/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"devops/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"devops/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"devops/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"devops/tekton/","title":"Continuous Integration","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/tekton/#tekton-overview","title":"Tekton Overview","text":"<p>Tekton is a cloud-native solution for building CI/CD systems. It consists of Tekton Pipelines, which provides the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete ecosystem.</p>"},{"location":"devops/tekton/#presentations","title":"Presentations","text":"<p>Tekton Overview  IBM Cloud DevOps with Tekton </p>"},{"location":"devops/tekton/#activities","title":"Activities","text":"<p>The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment.</p> <p>These tasks assume that you have:</p> <ul> <li>Reviewed the continuous integration concept page.</li> <li>Installed Tekton into your cluster.</li> </ul> Task Description Link Time Walkthroughs Deploying Applications From Source Using OpenShift 4 S2I 30 min Try It Yourself Tekton Lab Using Tekton to build container images Tekton 1 hour IBM Cloud DevOps Using IBM Cloud ToolChain with Tekton Tekton on IBM Cloud 1 hour Jenkins Lab Using Jenkins to build and deploy applications. Jenkins 1 hour <p>Once you have completed these tasks, you will have an understanding of continuous integration and how to use Tekton to build a pipeline.</p>"},{"location":"labs/","title":"Labs","text":""},{"location":"labs/#containers","title":"Containers","text":"Task Description Link Try It Yourself IBM Container Registry Build and Deploy Run using IBM Container Registry IBM Container Registry Docker Lab Running a Sample Application on Docker Docker Lab"},{"location":"labs/#kubernetes","title":"Kubernetes","text":"Task Description Link Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation Probes Create some Health &amp; Startup Probes to find what's causing an issue. Probes Debugging Find which service is breaking in your cluster and find out why. Debugging Multiple Containers Build a container using legacy container image. Multiple Containers Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Create a CronJob to run periodic tasks in your cluster. Cron Jobs Creating Services Create two services with certain requirements. Setting up Services Network Policies Create a policy to allow client pods with labels to access secure pod. Network Policies IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress Solutions Lab Solutions Solutions for the Kubernetes Labs Solutions"},{"location":"labs/#continuous-integration","title":"Continuous Integration","text":"Task Description Link Walkthroughs Deploying Applications From Source Using OpenShift 4 Source-to-Image S2I Try It Yourself Tekton Lab Using Tekton to test new versions of applications. Tekton IBM Cloud DevOps Using IBM Cloud ToolChain with Tekton Tekton on IBM Cloud Jenkins Lab Using Jenkins to test new versions of applications. Jenkins"},{"location":"labs/#continuous-deployment","title":"Continuous Deployment","text":"Task Description Link Walkthroughs GitOps Introduction to GitOps with OpenShift Learn OpenShift Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD"},{"location":"labs/#projects","title":"Projects","text":"Task Description Link Try It Yourself Cloud Native Challenge Deploy your own app using what we have learned CN Challenge"},{"location":"labs/containers/","title":"Containers Lab","text":"PodmanDocker"},{"location":"labs/containers/#introduction","title":"Introduction","text":"<p>In this lab you will learn how to use podman.</p>"},{"location":"labs/containers/#introduction_1","title":"Introduction","text":"<p>In this lab, you will learn about how to use docker and how to run applications using docker. This lab will not explicitly give you the commands to progress through these exercises, but will show you a similar expected output.</p> <p>It's your goal to create the commands needed (shown as &lt; command &gt; at each step) to complete the lab.</p>"},{"location":"labs/containers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a Quay account. This account is needed to push images to a container registry. Follow the tutorial to get familiar with interacting with Quay</li> <li>You need to install Docker in your environment. Follow the instructions here to install it on Mac and here to install it on Windows.</li> </ul>"},{"location":"labs/containers/#working-with-docker","title":"Working with docker","text":"<p>Before proceeding, make sure docker is properly installed on your system.</p> <ol> <li>Please verify your Docker by looking up the version.</li> </ol> <p>If it is installed, you will see a version number something similar to below.</p> <pre><code>$ &lt;command&gt;\nDocker version 19.03.0-beta3, build c55e026\n</code></pre> <p>** Running a hello-world container **</p> <p>Let us start with a <code>hello-world</code> container.</p> <ol> <li>run a <code>hello-world</code> container.</li> </ol> <p>If it is successfully run, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n1b930d010525: Pull complete\nDigest: sha256:41a65640635299bab090f783209c1e3a3f11934cf7756b09cb2f1e02147c6ed8\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre> <p>Since, <code>hello-world</code> image is not existing locally, it is pulled from <code>library/hello-world</code>. But if it is already existing, docker will not pull it every time but rather use the existing one.</p> <p>This image is pulled from https://hub.docker.com/_/hello-world. Docker hub is a repository used to store docker images. Similarly, you can use your own registries to store images. For example, IBM Cloud provides you a container registry.</p> <p>Verifying the hello-world image</p> <ol> <li>Now verify if an image is existing in your system locally.</li> </ol> <p>You will then see something like below.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nhello-world         latest              fce289e99eb9        5 months ago        1.84kB\n</code></pre>"},{"location":"labs/containers/#get-the-sample-application","title":"Get the sample application","text":"<p>To get the sample application, you will need to clone it from github.</p> <pre><code># Clone the sample app\ngit clone https://github.com/ibm-cloud-architecture/cloudnative_sample_app.git\n\n# Go to the project's root folder\ncd cloudnative_sample_app/\n</code></pre>"},{"location":"labs/containers/#run-the-application-on-docker","title":"Run the application on Docker","text":""},{"location":"labs/containers/#build-the-docker-image","title":"Build the docker image","text":"<p>Let's take look at the docker file before building it.</p> <pre><code>FROM maven:3.3-jdk-8 as builder\n\nCOPY . .\nRUN mvn clean install\n\nFROM openliberty/open-liberty:springBoot2-ubi-min as staging\n\nCOPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\nRUN springBootUtility thin \\\n    --sourceAppPath=/config/app.jar \\\n    --targetThinAppPath=/config/dropins/spring/thinClinic.jar \\\n    --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n</code></pre> <ul> <li>Using the <code>FROM</code> instruction, we provide the name and tag of an image that should be used as our base. This must always be the first instruction in the Dockerfile.</li> <li>Using <code>COPY</code> instruction, we copy new contents from the source filesystem to the container filesystem.</li> <li><code>RUN</code> instruction executes the commands.</li> </ul> <p>This Dockerfile leverages multi-stage builds, which lets you create multiple stages in your Dockerfile to do certain tasks.</p> <p>In our case, we have two stages.</p> <ul> <li>The first one uses <code>maven:3.3-jdk-8</code> as its base image to download and build the project and its dependencies using Maven.</li> <li>The second stage uses <code>openliberty/open-liberty:springBoot2-ubi-min</code> as its base image to run the compiled code from the previous stage.</li> </ul> <p>The advantage of using the multi-stage builds approach is that the resulting image only uses the base image of the last stage. Meaning that in our case, we will only end up with the <code>openliberty/open-liberty:springBoot2-ubi-min</code> as our base image, which is much tinier than having an image that has both Maven and the JRE.</p> <p>By using the multi-stage builds approach when it makes sense to use it, you will end up with much lighter and easier to maintain images, which can save you space on your Docker Registry. Also, having tinier images usually means less resource consumption on your worker nodes, which can result cost-savings.</p> <p>Once, you have the docker file ready, the next step is to build it. The <code>build</code> command allows you to build a docker image which you can later run as a container.</p> <ol> <li>Build the docker file with the <code>image_name</code> of <code>greeting</code> and give it a <code>image_tag</code> of <code>v1.0.0</code> and build it using the current context.</li> </ol> <p>You will see something like below:</p> <pre><code>$ &lt;command&gt;\nSending build context to Docker daemon  22.17MB\nStep 1/6 : FROM maven:3.3-jdk-8 as builder\n---&gt; 9997d8483b2f\nStep 2/6 : COPY . .\n---&gt; c198e3e54023\nStep 3/6 : RUN mvn clean install\n---&gt; Running in 24378df7f87b\n[INFO] Scanning for projects...\n.\n.\n.\n[INFO] Installing /target/cloudnativesampleapp-1.0-SNAPSHOT.jar to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.jar\n[INFO] Installing /pom.xml to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.pom\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 44.619 s\n[INFO] Finished at: 2020-04-06T16:07:04+00:00\n[INFO] Final Memory: 38M/385M\n[INFO] ------------------------------------------------------------------------\nRemoving intermediate container 24378df7f87b\n---&gt; cc5620334e1b\nStep 4/6 : FROM openliberty/open-liberty:springBoot2-ubi-min as staging\n---&gt; 021530b0b3cb\nStep 5/6 : COPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\n---&gt; dbc81e5f4691\nStep 6/6 : RUN springBootUtility thin     --sourceAppPath=/config/app.jar     --targetThinAppPath=/config/dropins/spring/thinClinic.jar     --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n---&gt; Running in 8ea80b5863cb\nCreating a thin application from: /config/app.jar\nLibrary cache: /opt/ol/wlp/usr/shared/resources/lib.index.cache\nThin application: /config/dropins/spring/thinClinic.jar\nRemoving intermediate container 8ea80b5863cb\n---&gt; a935a129dcb2\nSuccessfully built a935a129dcb2\nSuccessfully tagged greeting:v1.0.0\n</code></pre> <ol> <li>Next, verify your newly built image</li> </ol> <p>The output will be as follows.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY                           TAG                   IMAGE ID            CREATED             SIZE\ngreeting                             v1.0.0                89bd7032fdee        51 seconds ago      537MB\nopenliberty/open-liberty             springBoot2-ubi-min   bcfcb2c5ce16        6 days ago          480MB\nhello-world                          latest                f9cad508cb4c        5 months ago        1.84kB\n</code></pre>"},{"location":"labs/containers/#run-the-docker-container","title":"Run the docker container","text":"<p>Now let's try running the docker container. Run it with the following parameters:</p> <ol> <li>Expose port <code>9080</code>. Run it in the background in detached mode. Give the container the name of <code>greeting</code>.</li> </ol> <p>Once done, you will have something like below.</p> <pre><code>$ &lt;command&gt;\nbc2dc95a6bd1f51a226b291999da9031f4443096c1462cb3fead3df36613b753\n</code></pre> <p>Also, docker cannot create two containers with the same name. If you try to run the same container having the same name again, you will see something like below.</p> <pre><code>$ &lt;command&gt;\ndocker: Error response from daemon: Conflict. The container name \"/greeting\" is already in use by container \"a74b91789b29af6e7be92b30d0e68eef852bfb24336a44ef1485bb58becbd664\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</code></pre> <p>It is a good practice to name your containers. Naming helps you to discover your service easily.</p> <ol> <li>List all the running containers.</li> </ol> <p>You will see something like below.</p> <pre><code>$ &lt;command&gt;\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                              NAMES\nbc2dc95a6bd1        greeting:v1.0.0     \"/opt/ol/helpers/run\u2026\"   18 minutes ago      Up 18 minutes       0.0.0.0:9080-&gt;9080/tcp, 9443/tcp   greeting\n</code></pre> <ol> <li>Let's inspect the running container.</li> </ol> <p>By inspecting the container, you can access detailed information about the container. By using this command, you get to know the details about network settings, volumes, configs, state etc.</p> <p>If we consider our container, it is as follows. You can see lot of information about the <code>greeting</code> container.</p> <pre><code>$ &lt;command&gt;\n[\n    {\n        \"Id\": \"bc2dc95a6bd1f51a226b291999da9031f4443096c1462cb3fead3df36613b753\",\n        \"Created\": \"2019-08-30T16:56:40.2081539Z\",\n        \"Path\": \"/opt/ol/helpers/runtime/docker-server.sh\",\n        \"Args\": [\n            \"/opt/ol/wlp/bin/server\",\n            \"run\",\n            \"defaultServer\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 27548,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2019-08-30T16:56:41.0927889Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        ..........\n        ..........\n        ..........\n    }\n]\n</code></pre> <ol> <li>Get the logs of the <code>greeting</code> container.</li> </ol> <p>It helps you to access the logs of your container. It allows you to debug the container if it fails. It also lets you to know what is happening with your application.</p> <p>At the end, you will see something like below.</p> <pre><code>.   ____          _            __ _ _\n/\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n\\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n'  |____| .__|_| |_|_| |_\\__, | / / / /\n=========|_|==============|___/=/_/_/_/\n:: Spring Boot ::        (v2.1.7.RELEASE)\n2019-08-30 16:57:01.494  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Starting SBApplication on bc2dc95a6bd1 with PID 1 (/opt/ol/wlp/usr/servers/defaultServer/dropins/spring/thinClinic.jar started by default in /opt/ol/wlp/output/defaultServer)\n2019-08-30 16:57:01.601  INFO 1 --- [ecutor-thread-5] application.SBApplication                : No active profile set, falling back to default profiles: default\n[AUDIT   ] CWWKT0016I: Web application available (default_host): http://bc2dc95a6bd1:9080/\n2019-08-30 16:57:09.641  INFO 1 --- [cutor-thread-25] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 7672 ms\n2019-08-30 16:57:12.279  INFO 1 --- [ecutor-thread-5] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'\n2019-08-30 16:57:12.974  INFO 1 --- [ecutor-thread-5] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\n2019-08-30 16:57:13.860  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Context refreshed\n2019-08-30 16:57:13.961  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Found 1 custom documentation plugin(s)\n2019-08-30 16:57:14.020  INFO 1 --- [ecutor-thread-5] s.d.s.w.s.ApiListingReferenceScanner     : Scanning for api listing references\n2019-08-30 16:57:14.504  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Started SBApplication in 17.584 seconds (JVM running for 33.368)\n[AUDIT   ] CWWKZ0001I: Application thinClinic started in 21.090 seconds.\n[AUDIT   ] CWWKF0012I: The server installed the following features: [el-3.0, jsp-2.3, servlet-4.0, springBoot-2.0, ssl-1.0, transportSecurity-1.0, websocket-1.1].\n[AUDIT   ] CWWKF0011I: The defaultServer server is ready to run a smarter planet. The defaultServer server started in 33.103 seconds.\n</code></pre> <p>This shows that the Spring Boot application is successfully started.</p>"},{"location":"labs/containers/#access-the-application","title":"Access the application","text":"<ul> <li>To access the application, open the browser and access http://localhost:9080/greeting?name=John.</li> </ul> <p>You will see something like below.</p> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, John :)\"}\n</code></pre> <p>Container Image Registry</p> <p>Container Image Registry is a place where you can store the container images. They can be public or private registries. They can be hosted by third party as well. In this lab, we are using Quay.</p>"},{"location":"labs/containers/#pushing-an-image-to-a-registry","title":"Pushing an image to a Registry","text":"<p>Let us now push the image to the Quay registry. Before pushing the image to the registry, one needs to login.</p> <ol> <li>Login to Quay using your credentials.</li> </ol> <p>Once logged in we need to take the image for the registry.</p> <ol> <li> <p>Tag your image for the image registry using the <code>same name and tag from before</code>. Be sure to include the host name of the target image registry in the destination tag (e.g. quay.io). NOTE: the tag command has both the source tag and repository destination tag in it.</p> </li> <li> <p>Now push the image to the registry. This allows you to share images to a registry.</p> </li> </ol> <p>If everything goes fine, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nThe push refers to repository [quay.io/&lt;repository_name&gt;/greeting]\n2e4d09cd03a2: Pushed\nd862b7819235: Pushed\na9212239031e: Pushed\n4be784548734: Pushed\na43c287826a1: Mounted from library/ibmjava\ne936f9f1df3e: Mounted from library/ibmjava\n92d3f22d44f3: Mounted from library/ibmjava\n10e46f329a25: Mounted from library/ibmjava\n24ab7de5faec: Mounted from library/ibmjava\n1ea5a27b0484: Mounted from library/ibmjava\nv1.0.0: digest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4 size: 2415\n</code></pre> <p>Once the push is successful, your image will be residing in the registry.</p>"},{"location":"labs/containers/#clean-up","title":"Clean Up","text":"<ol> <li> <p>Stop the <code>greeting</code> container.</p> </li> <li> <p>Remove the container.</p> </li> <li> <p>Remove the image. (NOTE: You will need the image_id to remove it.)</p> </li> </ol>"},{"location":"labs/containers/#pulling-an-image-from-the-registry","title":"Pulling an image from the registry","text":"<p>Sometimes, you may need the images that are residing on your registry. Or you may want to use some public images out there. Then, we need to pull the image from the registry.</p> <ol> <li>Pull the image <code>greeting</code> from the registry,</li> </ol> <p>If it successfully got pulled, we will see something like below.</p> <pre><code>ddcb5f219ce2: Pull complete\ne3371bbd24a0: Pull complete\n49d2efb3c01b: Pull complete\nDigest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4\nStatus: Downloaded newer image for &lt;repository_name&gt;/greeting:v1.0.0\ndocker.io/&lt;repository_name&gt;/greeting:v1.0.0\n</code></pre>"},{"location":"labs/containers/#conclusion","title":"Conclusion","text":"<p>You have successfully completed this lab! Let's take a look at what you learned and did today:</p> <ul> <li>Learned about Dockerfile.</li> <li>Learned about docker images.</li> <li>Learned about docker containers.</li> <li>Learned about multi-stage docker builds.</li> <li>Ran the Greetings service on Docker.</li> </ul> <p>Congratulations !!!</p>"},{"location":"labs/containers/container-registry/","title":"IBM Container Registries","text":"Quay.ioIBM Container RegistryDocker Hub <p>In this lab we are going to create a Container Image and store it in the IBM Cloud Container Registry</p>"},{"location":"labs/containers/container-registry/#introduction","title":"Introduction","text":"<p>In this lab you will learn how to use podman.</p>"},{"location":"labs/containers/container-registry/#introduction_1","title":"Introduction","text":""},{"location":"labs/containers/container-registry/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM Cloud Account</li> </ul>"},{"location":"labs/containers/container-registry/#login-into-ibm-cloud","title":"Login into IBM Cloud","text":""},{"location":"labs/containers/container-registry/#using-the-ibm-cloud-shell","title":"Using the IBM Cloud Shell","text":"<ol> <li>Login into IBM Cloud</li> <li>Select correct account from top right drop down if your IBM id is associated with multiple accounts</li> <li>Click the IBM Cloud Shell Icon on the top right corner of the IBM Cloud Console     </li> <li>This opens a new browser window with command linux terminal prompt.     </li> </ol>"},{"location":"labs/containers/container-registry/#create-a-new-container-registry-namespace","title":"Create a new Container Registry namespace","text":"<ol> <li>Ensure that you're targeting the correct IBM Cloud Container Registry region. For example for Dallas region use us-south <pre><code>ibmcloud cr region-set us-south\n</code></pre></li> <li>Choose a name for your first namespace, and create that namespace. Use this namespace for the rest of the Quick Start.Create a new Container Registry Namespace. This namespace is different from a Kubernetes/OpenShift namespace. The name needs to be all lowercase  and globaly unique within a region.     <pre><code>ibmcloud cr namespace-add &lt;my_namespace&gt;\n</code></pre>     Now set the environment <code>NAMESPACE</code> to be use for the rest of the lab     <pre><code>export NAMESPACE=&lt;my_namespace&gt;\n</code></pre></li> </ol>"},{"location":"labs/containers/container-registry/#building-and-pushing-a-container-image","title":"Building and Pushing a Container Image","text":"<ol> <li>Clone the following git repository and change directory to <code>1-containers</code> <pre><code>git clone --depth 1 https://github.com/csantanapr/think2020-nodejs.git my-app\ncd my-app/1-containers/\n</code></pre></li> <li>Inspect the file <code>Dockerfile</code> it contains a multistage build, first layer builds the application, the second copies only the built files.     <pre><code>cat Dockerfile\n</code></pre> <pre><code>FROM registry.access.redhat.com/ubi8/nodejs-18 as base\n\nFROM base as builder\n\nWORKDIR /opt/app-root/src\n\nCOPY package*.json ./\n\nRUN npm ci\n\nCOPY public public \nCOPY src src \n\nRUN npm run build\n\nFROM base\n\nWORKDIR /opt/app-root/src\n\nCOPY --from=builder  /opt/app-root/src/build build\n\nCOPY package*.json ./\n\nRUN npm ci --only=production\n\nCOPY --chown=1001:0 server server\nRUN chmod -R g=u server\n\nENV PORT=8080\n\nLABEL com.example.source=\"https://github.com/csantanapr/think2020-nodejs\"\nLABEL com.example.version=\"1.0\"\n\nARG ENV=production\nENV NODE_ENV $ENV\nENV NODE_VERSION $NODEJS_VERSION\nCMD npm run $NODE_ENV\n</code></pre></li> <li>Build and push the image, if not already set replace <code>$NAMESPACE</code> with the namespace you added previously, replace <code>us.icr.io</code> if using a different region.     <pre><code>ibmcloud cr build --tag us.icr.io/$NAMESPACE/my-app:1.0 ./\n</code></pre></li> </ol>"},{"location":"labs/containers/container-registry/#explore-the-container-registry-on-the-ibm-cloud-console","title":"Explore the Container Registry on the IBM Cloud Console","text":"<ol> <li>Explore the container image details using the IBM Cloud Console. Go to the Main Menu-&gt;Kubernetes-&gt;Registry you can use the tabs <code>Namespaces</code>, <code>Repository</code>, <code>Images</code> </li> </ol>"},{"location":"labs/containers/container-registry/#extra-credit-run-imge-on-kubernetes","title":"Extra Credit (Run Imge on Kubernetes)","text":"<p>If you have a Kubernetes Cluster you can run your application image</p> <ol> <li>Get the Access token for your Kubernetes cluster, command assumes your cluster name is <code>mycluster</code> <pre><code>ibmcloud ks cluster config -c mycluster\n</code></pre></li> <li>Run the following commands to create a deployment using the image we just build. If not already set replace <code>$NAMESPACE</code> with your IBM Container Registry Namespace we stored the image.     <pre><code>kubectl create deployment my-app --image us.icr.io/$NAMESPACE/my-app:1.0\nkubectl rollout status deployment/my-app\nkubectl port-forward deployment/my-app 8080:8080\n</code></pre>     If the app is connected you should see the following output     <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\n</code></pre></li> <li>Open a new Session and run the following command     <pre><code>curl localhost:8080 -I\n</code></pre>     You should see in the first line of output the following     <pre><code>HTTP/1.1 200 OK\n</code></pre></li> <li> <p>To access the app using a browser use the IBM Cloud Shell Web Preview. Click the Web Preview Icon and select port <code>8080</code> from the drop down. The application will open in a new browser window.      </p> </li> <li> <p>To stop the application on the terminal with the <code>kubectl port-forward</code> command quit by pressing Ctrl+C in *Session 1</p> </li> </ol>"},{"location":"labs/containers/container-registry/#delete-deployment-and-image","title":"Delete Deployment and Image","text":"<ol> <li>Delete the app deployment     <pre><code>kubectl delete deployment my-app\n</code></pre></li> <li>Delete the container image, if not already set replace <code>$NAMESPACE</code> with the registry namespace     <pre><code>ibmcloud cr image-rm us.icr.io/$NAMESPACE/my-app:1.0\n</code></pre></li> </ol>"},{"location":"labs/devops/argocd/","title":"ArgoCD Lab","text":"OpenShiftKubernetes"},{"location":"labs/devops/argocd/#openshift","title":"OpenShift","text":""},{"location":"labs/devops/argocd/#pre-requisites","title":"Pre-requisites","text":"<p>Make sure your environment is setup properly for the lab.</p> <p>Check the Environment Setup page for your setup.</p>"},{"location":"labs/devops/argocd/#argocd-installation","title":"ArgoCD Installation","text":"<ul> <li>Create the namespace <code>argocd</code> to install argocd     <pre><code>oc new-project argocd\n</code></pre></li> <li>Install ArgoCD as follows.     <pre><code>oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-operator.yaml\n</code></pre></li> <li>When installing the tutorial, make sure you wait until the argocd-operator is finished before installing the argocd-cr..or it will fail. You can do this:     <pre><code>oc get ClusterServiceVersion -n argocd\nNAME                                   DISPLAY                        VERSION   REPLACES   PHASE\nargocd-operator.v0.0.8                 Argo CD                        0.0.8                Succeeded\n</code></pre>     and wait for the \"succeeded\" to come up before proceeding.     <pre><code>oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-cr.yaml\n</code></pre>     and wait for the argocd server Pod to be running     <pre><code>oc get pods -n argocd -l app.kubernetes.io/name=example-argocd-server\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\nexample-argocd-server-57c4fd5c45-zf4q6   1/1     Running   0          115s\n</code></pre></li> <li>Install the <code>argocd</code> CLI, for example on OSX use brew     <pre><code>brew tap argoproj/tap\nbrew install argoproj/tap/argocd\n</code></pre></li> <li>Set an environment variable <code>ARGOCD_URL</code> using the <code>EXTERNAL-IP</code> <pre><code>export ARGOCD_NAMESPACE=\"argocd\"\nexport ARGOCD_SERVER=$(oc get route example-argocd-server -n $ARGOCD_NAMESPACE -o jsonpath='{.spec.host}')\nexport ARGOCD_URL=\"https://$ARGOCD_SERVER\"\necho ARGOCD_URL=$ARGOCD_URL\necho ARGOCD_SERVER=$ARGOCD_SERVER\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#deploying-the-app","title":"Deploying the app","text":"<ul> <li>Login into the UI.     <pre><code>open $ARGOCD_URL\n</code></pre></li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>oc get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d\n</code></pre>     For example the output is similar to this:     <pre><code>tyafMb7BNvO0kP9eizx3CojrK8pYJFQq\n</code></pre></li> </ul> <ul> <li>Now go back to the ArgoCD home and click on <code>NEW APP</code>.</li> <li>Add the below details:</li> <li>Application Name: <code>sample</code></li> <li>Project - <code>default</code></li> <li>SYNC POLICY: <code>Manual</code></li> <li>REPO URL: <code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>openshift</code></li> </ul> <ul> <li>Cluster - Select the default one <code>https://kubernetes.default.svc</code> to deploy in-cluster</li> <li>Namespace - <code>default</code></li> <li>Click Create to finish</li> </ul> <ul> <li>You will now see the available apps.</li> </ul> <ul> <li>Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying.</li> </ul> <p>To sync the application, click <code>SYNC</code> and then <code>SYNCHRONIZE</code>.</p> <p></p> <ul> <li>Wait till the app is deployed.</li> </ul> <p></p> <ul> <li>Once the app is deployed, click on it to see the details.</li> </ul> <p></p> <p></p>"},{"location":"labs/devops/argocd/#verifying-the-deployment","title":"Verifying the deployment","text":"<ul> <li>Access the app to verify if it is correctly deployed.</li> <li>List the cloudnativesampleapp-service route     <pre><code>oc get route\n</code></pre>     It should have an IP under <code>EXTERNAL-IP</code> column     <pre><code>NAME                 HOST/PORT                                     PATH   SERVICES                       PORT   TERMINATION   WILDCARD\ncloudnative-sample   cloudnative-sample-default.apps-crc.testing          cloudnativesampleapp-service   9080                 None\n</code></pre></li> <li>Set an environment variable <code>APP_URL</code> using the <code>EXTERNAL-IP</code> <pre><code>export APP_URL=\"http://$(oc get route cloudnative-sample -o jsonpath='{.status.ingress[0].host}')\"\necho ARGOCD_SERVER=$APP_URL\n</code></pre></li> <li>Access the url using <code>curl</code> <pre><code>curl \"$APP_URL/greeting?name=Carlos\"\n</code></pre> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#using-the-argocd-cli","title":"Using the ArgoCD CLI","text":"<ul> <li>Login using the cli.</li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>export ARGOCD_PASSWORD=$(oc get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d)\necho $ARGOCD_PASSWORD\n</code></pre></li> <li>Now login as follows.     <pre><code>argocd login --username admin --password $ARGOCD_PASSWORD $ARGOCD_SERVER\n</code></pre> <pre><code>WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain \nany IP SANs. Proceed insecurely (y/n)? y\n\n'admin' logged in successfully\nContext 'example-argocd-server-argocd.apps-crc.testing' updated\n</code></pre></li> <li>List the applications     <pre><code>argocd app list\n</code></pre> <pre><code>NAME    CLUSTER                         NAMESPACE  PROJECT  STATUS  HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                                     PATH   TARGET\nsample  https://kubernetes.default.svc  default    default  Synced  Healthy  &lt;none&gt;      &lt;none&gt;      https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy  openshift  HEAD\n</code></pre></li> <li>Get application details     <pre><code>argocd app get sample\n</code></pre> <pre><code>Name:               sample\nProject:            default\nServer:             https://kubernetes.default.svc\nNamespace:          default\nURL:                https://10.97.240.99/applications/sample\nRepo:               https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy\nTarget:             HEAD\nPath:               openshift\nSyncWindow:         Sync Allowed\nSync Policy:        &lt;none&gt;\nSync Status:        Synced to HEAD (9684037)\nHealth Status:      Healthy\n\nGROUP  KIND        NAMESPACE  NAME                             STATUS  HEALTH   HOOK  MESSAGE\n    Service     default    cloudnativesampleapp-service     Synced  Healthy        service/cloudnativesampleapp-service created\napps   Deployment  default    cloudnativesampleapp-deployment  Synced  Healthy        deployment.apps/cloudnativesampleapp-deployment created\n</code></pre></li> <li>Show application deployment history     <pre><code>argocd app history sample\n</code></pre> <pre><code>ID  DATE                           REVISION\n0   2020-02-12 21:10:32 -0500 EST  HEAD (9684037)\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#references","title":"References","text":"<ul> <li>ArgoCD</li> </ul>"},{"location":"labs/devops/argocd/#kubernetes","title":"Kubernetes","text":""},{"location":"labs/devops/argocd/#pre-requisites_1","title":"Pre-requisites","text":"<p>Make sure your environment is setup properly for the lab.</p> <p>Check the Environment Setup page for your setup.</p>"},{"location":"labs/devops/argocd/#argocd-installation_1","title":"ArgoCD Installation","text":"<ul> <li>Create the namespace <code>argocd</code> to install argocd     <pre><code>kubectl create namespace argocd\nexport ARGOCD_NAMESPACE=argocd\n</code></pre></li> <li> <p>Create RBAC resources     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/service_account.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/role.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/role_binding.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argo-clusteradmin.yaml\n</code></pre></p> </li> <li> <p>Install CRDs     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/argo-cd/argoproj.io_applications_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/argo-cd/argoproj.io_appprojects_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/crds/argoproj.io_argocdexports_crd.yaml\nkubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/crds/argoproj.io_argocds_crd.yaml\n</code></pre>     Verify CRDs     <pre><code>kubectl get crd -n argocd\n</code></pre> <pre><code>NAME                        CREATED AT\napplications.argoproj.io    2020-05-15T02:05:55Z\nappprojects.argoproj.io     2020-05-15T02:05:56Z\nargocdexports.argoproj.io   2020-05-15T02:08:21Z\nargocds.argoproj.io         2020-05-15T02:08:21Z\n</code></pre></p> </li> <li>Deploy Operator     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/deploy/operator.yaml\n</code></pre></li> <li>Deploy ArgoCD CO     <pre><code>kubectl create -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-operator/v0.0.8/examples/argocd-lb.yaml\n</code></pre>     Verify that ArgoCD Pods are running     <pre><code>kubectl get pods -n argocd\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\nargocd-operator-5f7d8cf7d8-486vn                         1/1     Running   0          3m46s\nexample-argocd-application-controller-7dc5fcb75d-xkk5h   1/1     Running   0          2m3s\nexample-argocd-dex-server-bb9df96cb-ndmhl                1/1     Running   0          2m3s\nexample-argocd-redis-756b6764-sb2gt                      1/1     Running   0          2m3s\nexample-argocd-repo-server-75944fcf87-zmh48              1/1     Running   0          2m3s\nexample-argocd-server-747b684c8c-xhgl9                   1/1     Running   0          2m3s\n</code></pre>     Verify that the other ArgoCD resources are created     <pre><code>kubectl get cm,secret,svc,deploy -n argocd\n</code></pre></li> <li> <p>List the argocd-server service     <pre><code>kubectl get svc example-argocd-server -n argocd\n</code></pre></p> </li> <li> <p>From the script, the Argo Server service has a <code>type</code> of <code>LoadBalancer</code>. If the <code>ExternalIP</code> is in a <code>pending</code> state, then there is no loadBalancer for your cluster, so we only need the the ArgoCD server's <code>NodePort</code>. Otherwise use the <code>ExternalIP</code> and <code>NodePort</code> to access Argo.     <pre><code>NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\nexample-argocd-server   LoadBalancer   10.105.73.245   &lt;pending&gt;   80:31138/TCP,443:31932/TCP   5m3s\n</code></pre></p> </li> <li> <p>To access the service we need the <code>Node's External IP</code> and the <code>NodePort</code>. Let's set an environment variable <code>ARGOCD_URL</code> with <code>NODE_EXTERNAL_IP</code>:<code>NodePort</code>.     <pre><code>export NODE_EXTERNAL_IP=\"$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}')\"\nexport ARGOCD_NODEPORT=\"$(kubectl get svc example-argocd-server -n $ARGOCD_NAMESPACE -o jsonpath='{.spec.ports[0].nodePort}')\"\nexport ARGOCD_URL=\"https://$NODE_EXTERNAL_IP:$ARGOCD_NODEPORT\"\necho ARGOCD_URL=$ARGOCD_URL\n</code></pre></p> </li> <li> <p>If you can't access the NodePort from your computer and only http/80 then edit the argocd-server and add the flag <code>--insecure</code> <pre><code>kubectl edit -n argocd deployment example-argocd-server\n</code></pre>     Use the kube api to proxy into the argocd server using <code>kubectl port-forward</code> <pre><code>kubectl port-forward service/example-argocd-server 8080:80 -n argocd\n</code></pre>     Then you can access the argocd server locally on port 8080 http://localhost:8080</p> </li> </ul>"},{"location":"labs/devops/argocd/#deploying-the-app_1","title":"Deploying the app","text":"<ul> <li>Login using the Browser into the UI using <code>$ARGOCD_URL</code> or <code>localhost:8080</code> if using port-forward</li> <li>Use <code>admin</code> as the username and get the password with the following command     <pre><code>kubectl get secret example-argocd-cluster -n $ARGOCD_NAMESPACE -o jsonpath='{.data.admin\\.password}' | base64 -d\n</code></pre>     For example the output is similar to this:     <pre><code>tyafMb7BNvO0kP9eizx3CojrK8pYJFQq\n</code></pre></li> </ul> <ul> <li>Now go back to the ArgoCD home and click on <code>NEW APP</code>.</li> <li>Add the below details:</li> <li>Application Name: <code>sample</code></li> <li>Project - <code>default</code></li> <li>SYNC POLICY: <code>Manual</code></li> <li>REPO URL: <code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>kubernetes</code></li> </ul> <ul> <li>Cluster - Select the default one <code>https://kubernetes.default.svc</code> to deploy in-cluster</li> <li>Namespace - <code>default</code></li> <li>Click Create to finish</li> </ul> <ul> <li>You will now see the available apps.</li> </ul> <ul> <li>Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying.</li> </ul> <p>To sync the application, click <code>SYNC</code> and then <code>SYNCHRONIZE</code>.</p> <p></p> <ul> <li>Wait till the app is deployed.</li> </ul> <p></p> <ul> <li>Once the app is deployed, click on it to see the details.</li> </ul> <p></p> <p></p>"},{"location":"labs/devops/argocd/#verifying-the-deployment_1","title":"Verifying the deployment","text":"<ul> <li>Access the app to verify if it is correctly deployed.</li> <li>List the cloudnativesampleapp-service service     <pre><code>kubectl get svc cloudnativesampleapp-service\n</code></pre>     It will have the <code>NodePort</code> for the application. In this case, it is <code>30499</code>.      <pre><code>NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\ncloudnativesampleapp-service   NodePort   172.21.118.165   &lt;none&gt;        9080:30499/TCP   20s\n</code></pre></li> <li>Set an environment variable <code>APP_URL</code> using the <code>Node's IP</code> and <code>NodePort</code> values     <pre><code>export APP_NODE_PORT=\"$(kubectl get svc cloudnativesampleapp-service -n default -o jsonpath='{.spec.ports[0].nodePort}')\"\nexport APP_URL=\"$NODE_EXTERNAL_IP:$APP_NODE_PORT\"\necho Application=$APP_URL\n</code></pre></li> <li>Access the url using <code>curl</code> <pre><code>curl \"$APP_URL/greeting?name=Carlos\"\n</code></pre> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}\n</code></pre></li> </ul>"},{"location":"labs/devops/argocd/#references_1","title":"References","text":"<ul> <li>ArgoCD</li> </ul>"},{"location":"labs/devops/ibm-toolchain/","title":"IBM Toolchain Lab","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"labs/devops/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     <pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.</p> </li> </ol>"},{"location":"labs/devops/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"labs/devops/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"labs/devops/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"labs/devops/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"labs/devops/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"labs/devops/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"labs/devops/jenkins/","title":"Jenkins Lab","text":"OpenShiftKubernetes"},{"location":"labs/devops/jenkins/#introduction","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using Jenkins to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - Jenkins, Leading open source automation server.</p>"},{"location":"labs/devops/jenkins/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need an IBM cloud account.</li> <li>Create kubernetes cluster using IBM Cloud Kubernetes Service. Here, you can choose an openshift cluster.</li> <li>Install oc command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"labs/devops/jenkins/#continuous-integration","title":"Continuous Integration","text":""},{"location":"labs/devops/jenkins/#install-jenkins","title":"Install Jenkins","text":"<ul> <li>Open the IBM Cloud Openshift cluster.</li> </ul> <ul> <li>Click on the <code>OpenShift web console</code> tab and this will take you to openshift UI.</li> </ul> <ul> <li>Create a new project.</li> </ul> <ul> <li>Search for <code>Jenkins</code>.</li> </ul> <ul> <li>Choose <code>Jenkins (Ephemeral)</code>.</li> </ul> <ul> <li>Install it.</li> </ul> <ul> <li>Wait till the Jenkins installs and the pods are ready.</li> </ul> <ul> <li>Once, it is ready you can access the Jenkins by clicking the link.</li> </ul> <p>Now, click on <code>Log in with OpenShift</code>.</p> <ul> <li>When you gets logged in, you will see the below screen. Click <code>Allow selected permissions</code>.</li> </ul> <p></p> <ul> <li>You will be able to access the Jenkins UI now.</li> </ul> <p></p>"},{"location":"labs/devops/jenkins/#get-the-sample-app","title":"Get the Sample App","text":"<ul> <li>Fork the below repository.</li> </ul> <pre><code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n</code></pre> <ul> <li>Clone the forked repository.</li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"labs/devops/jenkins/#jenkinsfile","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"labs/devops/jenkins/#jenkins-credentials","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p>image::Jenkins_add_creds.png[align=\"center\"] </p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: (Your container registry url, for eg., us.icr.io) ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"labs/devops/jenkins/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of the application, choose <code>Pipeline</code> and click <code>OK</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build Now</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"labs/devops/jenkins/#introduction_1","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using https://jenkins.io/[Jenkins] to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - https://jenkins.io/doc/[Jenkins, Leading open source automation server].</p>"},{"location":"labs/devops/jenkins/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>You need an https://cloud.ibm.com/login[IBM cloud account].</li> <li>Create kubernetes cluster using https://cloud.ibm.com/docs/containers?topic=containers-getting-started[IBM Cloud Kubernetes Service]. Here, you can choose a kubernetes cluster.</li> <li>Install https://kubernetes.io/docs/tasks/tools/install-kubectl/[kubectl] command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"labs/devops/jenkins/#continuous-integration_1","title":"Continuous Integration","text":""},{"location":"labs/devops/jenkins/#install-jenkins_1","title":"Install Jenkins","text":"<ul> <li>Initially log in into your ibm cloud account as follows.</li> </ul> <pre><code>$ ibmcloud login -a cloud.ibm.com -r (region) -g (cluster_name)\n</code></pre> <p>And then download the Kube config files as below.</p> <pre><code>$ ibmcloud ks cluster-config --cluster (cluster_name)\n</code></pre> <p>You can also get the <code>access</code> instructions in <code>IBM Cloud Dashboard -&gt; Kubernetes Clusters -&gt; Click on your Cluster -&gt; Click on Access Tab</code>.</p> <ul> <li>Install Jenkins using helm using the below command. We are not using persistence in this lab.</li> </ul> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\n</code></pre> <p>If it is successfully executed, you will see something like below.</p> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\nNAME:   cloudnative\nLAST DEPLOYED: Wed Aug  7 16:22:55 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==&gt; v1/ConfigMap\nNAME                       DATA  AGE\ncloudnative-jenkins        5     1s\ncloudnative-jenkins-tests  1     1s\n\n==&gt; v1/Deployment\nNAME                 READY  UP-TO-DATE  AVAILABLE  AGE\ncloudnative-jenkins  0/1    1           0          1s\n\n==&gt; v1/Pod(related)\nNAME                                  READY  STATUS    RESTARTS  AGE\ncloudnative-jenkins-57588c86c7-hxqmq  0/1    Init:0/1  0         0s\n\n==&gt; v1/Role\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/RoleBinding\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/Secret\nNAME                 TYPE    DATA  AGE\ncloudnative-jenkins  Opaque  2     1s\n\n==&gt; v1/Service\nNAME                       TYPE          CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE\ncloudnative-jenkins        LoadBalancer  172.21.143.35   169.63.132.124  8080:32172/TCP  1s\ncloudnative-jenkins-agent  ClusterIP     172.21.206.235  (none&gt;          50000/TCP       1s\n\n==&gt; v1/ServiceAccount\nNAME                 SECRETS  AGE\ncloudnative-jenkins  1        1s\n</code></pre> <p>Use the following steps to open Jenkins UI and login.</p> <pre><code>NOTES:\n1. Get your 'admin' user password by running:\nprintf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n2. Get the Jenkins URL to visit by running these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace default -w cloudnative-jenkins'\nexport SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\necho http://$SERVICE_IP:8080/login\n\n3. Login with the password from step 1 and the username: admin\n\n\nFor more information on running Jenkins on Kubernetes, visit:\nhttps://cloud.google.com/solutions/jenkins-on-container-engine\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Jenkins pod is terminated.                            #####\n#################################################################################\n</code></pre> <p>To get the url, run the below commands.</p> <pre><code>$ export SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\n$ echo http://$SERVICE_IP:8080/login\n</code></pre> <p>Once executed, you will see something like below.</p> <pre><code>$ echo http://$SERVICE_IP:8080/login\nhttp://169.63.132.124:8080/login\n</code></pre> <ul> <li>Now, let us login into the Jenkins.</li> </ul> <p></p> <p>The user name will be <code>admin</code> and to get the password, run the below command.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n</code></pre> <p>It returns you the password as follows.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\npassword\n</code></pre> <ul> <li>Once, successfully logged in you will see the Jenkins home page which is as follows.</li> </ul> <p></p>"},{"location":"labs/devops/jenkins/#get-the-sample-app_1","title":"Get the Sample App","text":"<ul> <li> <p>Fork the below repository.</p> <p>https://github.com/ibm-cloud-architecture/cloudnative_sample_app</p> </li> <li> <p>Clone the forked repository.</p> </li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"labs/devops/jenkins/#jenkinsfile_1","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"labs/devops/jenkins/#jenkins-credentials_1","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p></p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: Your container registry url, for eg., us.icr.io ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"labs/devops/jenkins/#jenkins-pipeline_1","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of your application, select <code>Pipeline</code> and then click <code>OK</code>.</li> </ul> <ul> <li>In <code>General</code>, check <code>This project is parameterized</code>. Create a string parameter with name <code>CLOUD</code> and Default value <code>kubernetes</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build with Parameters</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"labs/devops/tekton/","title":"Tekton Lab - OpenShift Pipelines","text":""},{"location":"labs/devops/tekton/#overview","title":"Overview","text":"<p>In this lab, you will learn how to use OpenShift Pipelines, which is based on Tekton and provides a cloud-native, Kubernetes-native CI/CD solution. OpenShift Pipelines is installed via the Red Hat OpenShift Pipelines Operator from OperatorHub.</p>"},{"location":"labs/devops/tekton/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an OpenShift cluster (4.10 or later recommended)</li> <li><code>oc</code> CLI installed and configured</li> <li>Cluster admin privileges (for installing the operator) or the operator already installed</li> </ul>"},{"location":"labs/devops/tekton/#setup","title":"Setup","text":""},{"location":"labs/devops/tekton/#install-the-openshift-pipelines-operator","title":"Install the OpenShift Pipelines Operator","text":"<p>The OpenShift Pipelines Operator can be installed from the OperatorHub in the OpenShift web console or via the CLI.</p>"},{"location":"labs/devops/tekton/#option-1-install-via-web-console-recommended","title":"Option 1: Install via Web Console (Recommended)","text":"<ol> <li>Log in to the OpenShift web console as a cluster administrator</li> <li>Navigate to Operators \u2192 OperatorHub</li> <li>Search for Red Hat OpenShift Pipelines</li> <li>Click on the operator and then click Install</li> <li>Accept the default settings and click Install</li> <li>Wait for the operator to install (status shows \"Succeeded\")</li> </ol>"},{"location":"labs/devops/tekton/#option-2-install-via-cli","title":"Option 2: Install via CLI","text":"<p>Create a subscription for the OpenShift Pipelines Operator:</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: openshift-pipelines-operator\n  namespace: openshift-operators\nspec:\n  channel: latest\n  name: openshift-pipelines-operator-rh\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\nEOF\n</code></pre> <p>Wait for the operator to be ready:</p> <pre><code>oc get csv -n openshift-operators -w\n</code></pre> <p>You should see <code>openshift-pipelines-operator-rh</code> with phase <code>Succeeded</code>.</p>"},{"location":"labs/devops/tekton/#verify-installation","title":"Verify Installation","text":"<p>Once installed, verify that the Tekton components are running:</p> <pre><code>oc get pods -n openshift-pipelines\n</code></pre> <p>You should see pods for the pipelines controller, webhook, and other components in <code>Running</code> state.</p>"},{"location":"labs/devops/tekton/#install-the-tekton-cli-tkn","title":"Install the Tekton CLI (tkn)","text":"<p>The <code>tkn</code> CLI is used to interact with Tekton/OpenShift Pipelines resources.</p> <p>macOS:</p> <pre><code>brew install tektoncd-cli\n</code></pre> <p>Linux:</p> <pre><code># Download the latest release\ncurl -LO https://github.com/tektoncd/cli/releases/latest/download/tkn_Linux_x86_64.tar.gz\ntar xvzf tkn_Linux_x86_64.tar.gz\nsudo mv tkn /usr/local/bin/\n</code></pre> <p>Verify installation:</p> <pre><code>tkn version\n</code></pre>"},{"location":"labs/devops/tekton/#create-a-project","title":"Create a Project","text":"<p>Create a new OpenShift project for this lab:</p> <pre><code>export NAMESPACE=tekton-demo\noc new-project $NAMESPACE\n</code></pre> <p>Note: OpenShift Pipelines automatically creates a <code>pipeline</code> ServiceAccount in each namespace with the necessary permissions.</p>"},{"location":"labs/devops/tekton/#understanding-tekton-concepts","title":"Understanding Tekton Concepts","text":"<p>Before diving into the lab, let's understand the key Tekton concepts:</p> <ul> <li>Task - A collection of steps that run sequentially in a pod</li> <li>TaskRun - An execution of a Task</li> <li>Pipeline - A collection of Tasks that can run in sequence or parallel</li> <li>PipelineRun - An execution of a Pipeline</li> <li>Workspace - A shared storage volume for Tasks in a Pipeline</li> </ul>"},{"location":"labs/devops/tekton/#part-1-creating-tasks","title":"Part 1: Creating Tasks","text":""},{"location":"labs/devops/tekton/#create-a-simple-task","title":"Create a Simple Task","text":"<p>A Task defines a series of steps that run in order. Create a file called <code>task-hello.yaml</code>:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: hello\nspec:\n  params:\n    - name: message\n      type: string\n      default: \"Hello from Tekton!\"\n  steps:\n    - name: say-hello\n      image: registry.access.redhat.com/ubi8/ubi-minimal:latest\n      script: |\n        #!/bin/bash\n        echo \"$(params.message)\"\n</code></pre> <p>Apply the Task:</p> <pre><code>oc apply -f task-hello.yaml -n $NAMESPACE\n</code></pre> <p>List Tasks using the Tekton CLI:</p> <pre><code>tkn task ls -n $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#run-the-task","title":"Run the Task","text":"<p>You can run a Task using the <code>tkn</code> CLI:</p> <pre><code>tkn task start hello --showlog -n $NAMESPACE\n</code></pre> <p>Or create a TaskRun YAML. Create <code>taskrun-hello.yaml</code>:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: TaskRun\nmetadata:\n  generateName: hello-run-\nspec:\n  taskRef:\n    name: hello\n  params:\n    - name: message\n      value: \"Hello from OpenShift Pipelines!\"\n</code></pre> <p>Apply it:</p> <pre><code>oc create -f taskrun-hello.yaml -n $NAMESPACE\n</code></pre> <p>Check the TaskRun status:</p> <pre><code>tkn taskrun ls -n $NAMESPACE\ntkn taskrun logs --last -n $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#create-a-build-task","title":"Create a Build Task","text":"<p>Now let's create a more practical Task that clones a Git repository and builds a container image. We'll use the built-in ClusterTasks provided by OpenShift Pipelines.</p> <p>List available ClusterTasks:</p> <pre><code>tkn clustertask ls\n</code></pre> <p>You should see tasks like <code>git-clone</code>, <code>buildah</code>, <code>openshift-client</code>, and others.</p>"},{"location":"labs/devops/tekton/#part-2-creating-a-pipeline","title":"Part 2: Creating a Pipeline","text":""},{"location":"labs/devops/tekton/#create-a-cicd-pipeline","title":"Create a CI/CD Pipeline","text":"<p>Create a Pipeline that:</p> <ol> <li>Clones a Git repository</li> <li>Builds a container image</li> <li>Deploys to OpenShift</li> </ol> <p>Create <code>pipeline.yaml</code>:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: build-and-deploy\nspec:\n  params:\n    - name: git-url\n      type: string\n      description: The Git repository URL\n      default: https://github.com/openshift/nodejs-ex.git\n    - name: git-revision\n      type: string\n      description: The Git revision to build\n      default: master\n    - name: image-name\n      type: string\n      description: The name of the image to build\n    - name: deployment-name\n      type: string\n      description: The name of the deployment\n      default: nodejs-app\n  workspaces:\n    - name: shared-workspace\n      description: Workspace for sharing data between tasks\n  tasks:\n    - name: fetch-source\n      taskRef:\n        name: git-clone\n        kind: ClusterTask\n      workspaces:\n        - name: output\n          workspace: shared-workspace\n      params:\n        - name: url\n          value: $(params.git-url)\n        - name: revision\n          value: $(params.git-revision)\n        - name: deleteExisting\n          value: \"true\"\n\n    - name: build-image\n      taskRef:\n        name: buildah\n        kind: ClusterTask\n      runAfter:\n        - fetch-source\n      workspaces:\n        - name: source\n          workspace: shared-workspace\n      params:\n        - name: IMAGE\n          value: $(params.image-name)\n        - name: TLSVERIFY\n          value: \"false\"\n\n    - name: deploy\n      taskRef:\n        name: openshift-client\n        kind: ClusterTask\n      runAfter:\n        - build-image\n      params:\n        - name: SCRIPT\n          value: |\n            oc new-app $(params.image-name) --name=$(params.deployment-name) || \\\n            oc set image deployment/$(params.deployment-name) $(params.deployment-name)=$(params.image-name)\n            oc rollout status deployment/$(params.deployment-name)\n</code></pre> <p>Apply the Pipeline:</p> <pre><code>oc apply -f pipeline.yaml -n $NAMESPACE\n</code></pre> <p>List Pipelines:</p> <pre><code>tkn pipeline ls -n $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#create-a-persistentvolumeclaim-for-the-workspace","title":"Create a PersistentVolumeClaim for the Workspace","text":"<p>Pipelines need storage to share data between tasks. Create <code>pvc.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pipeline-workspace\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>Apply the PVC:</p> <pre><code>oc apply -f pvc.yaml -n $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#run-the-pipeline","title":"Run the Pipeline","text":"<p>Set up the image URL using the internal OpenShift registry:</p> <pre><code>export IMAGE_URL=image-registry.openshift-image-registry.svc:5000/$NAMESPACE/nodejs-app\n</code></pre> <p>Run the Pipeline using the <code>tkn</code> CLI:</p> <pre><code>tkn pipeline start build-and-deploy \\\n  -p git-url=https://github.com/openshift/nodejs-ex.git \\\n  -p git-revision=master \\\n  -p image-name=$IMAGE_URL \\\n  -p deployment-name=nodejs-app \\\n  -w name=shared-workspace,claimName=pipeline-workspace \\\n  --showlog \\\n  -n $NAMESPACE\n</code></pre> <p>Or create a PipelineRun YAML. Create <code>pipelinerun.yaml</code>:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: PipelineRun\nmetadata:\n  generateName: build-and-deploy-run-\nspec:\n  pipelineRef:\n    name: build-and-deploy\n  params:\n    - name: git-url\n      value: https://github.com/openshift/nodejs-ex.git\n    - name: git-revision\n      value: master\n    - name: image-name\n      value: image-registry.openshift-image-registry.svc:5000/tekton-demo/nodejs-app\n    - name: deployment-name\n      value: nodejs-app\n  workspaces:\n    - name: shared-workspace\n      persistentVolumeClaim:\n        claimName: pipeline-workspace\n</code></pre> <p>Apply it:</p> <pre><code>oc create -f pipelinerun.yaml -n $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#monitor-the-pipeline","title":"Monitor the Pipeline","text":"<p>Watch the PipelineRun progress:</p> <pre><code>tkn pipelinerun ls -n $NAMESPACE\ntkn pipelinerun logs --last -f -n $NAMESPACE\n</code></pre> <p>You can also view the pipeline in the OpenShift web console under Pipelines \u2192 Pipelines.</p>"},{"location":"labs/devops/tekton/#part-3-using-the-openshift-web-console","title":"Part 3: Using the OpenShift Web Console","text":"<p>OpenShift provides a visual interface for managing Pipelines.</p>"},{"location":"labs/devops/tekton/#view-pipelines-in-the-console","title":"View Pipelines in the Console","text":"<ol> <li>Navigate to Pipelines \u2192 Pipelines in the left menu</li> <li>Select your project (<code>tekton-demo</code>)</li> <li>You should see your <code>build-and-deploy</code> Pipeline</li> <li>Click on it to see details and run history</li> </ol>"},{"location":"labs/devops/tekton/#start-a-pipeline-from-the-console","title":"Start a Pipeline from the Console","text":"<ol> <li>Click on the Pipeline name</li> <li>Click Actions \u2192 Start</li> <li>Fill in the parameters</li> <li>Select the workspace (PVC)</li> <li>Click Start</li> </ol>"},{"location":"labs/devops/tekton/#view-pipelineruns","title":"View PipelineRuns","text":"<ol> <li>Navigate to Pipelines \u2192 PipelineRuns</li> <li>Click on a PipelineRun to see:</li> <li>Task status</li> <li>Logs for each step</li> <li>Duration and timing</li> </ol>"},{"location":"labs/devops/tekton/#part-4-expose-the-application","title":"Part 4: Expose the Application","text":"<p>After the Pipeline completes successfully, expose the application:</p> <pre><code>oc expose deployment nodejs-app --port=8080 -n $NAMESPACE\noc expose service nodejs-app -n $NAMESPACE\n</code></pre> <p>Get the application URL:</p> <pre><code>export APP_URL=$(oc get route nodejs-app -o jsonpath='{.spec.host}' -n $NAMESPACE)\necho \"Application URL: http://$APP_URL\"\n</code></pre> <p>Test the application:</p> <pre><code>curl http://$APP_URL\n</code></pre> <p>Or open the URL in a browser.</p>"},{"location":"labs/devops/tekton/#part-5-triggers-optional","title":"Part 5: Triggers (Optional)","text":"<p>OpenShift Pipelines supports Triggers to automatically start Pipelines based on events like Git webhooks.</p>"},{"location":"labs/devops/tekton/#create-an-eventlistener","title":"Create an EventListener","text":"<p>Create <code>eventlistener.yaml</code>:</p> <pre><code>apiVersion: triggers.tekton.dev/v1beta1\nkind: EventListener\nmetadata:\n  name: build-deploy-listener\nspec:\n  serviceAccountName: pipeline\n  triggers:\n    - name: github-push\n      bindings:\n        - ref: github-push-binding\n      template:\n        ref: build-deploy-template\n---\napiVersion: triggers.tekton.dev/v1beta1\nkind: TriggerBinding\nmetadata:\n  name: github-push-binding\nspec:\n  params:\n    - name: git-url\n      value: $(body.repository.clone_url)\n    - name: git-revision\n      value: $(body.after)\n---\napiVersion: triggers.tekton.dev/v1beta1\nkind: TriggerTemplate\nmetadata:\n  name: build-deploy-template\nspec:\n  params:\n    - name: git-url\n    - name: git-revision\n  resourcetemplates:\n    - apiVersion: tekton.dev/v1\n      kind: PipelineRun\n      metadata:\n        generateName: build-and-deploy-triggered-\n      spec:\n        pipelineRef:\n          name: build-and-deploy\n        params:\n          - name: git-url\n            value: $(tt.params.git-url)\n          - name: git-revision\n            value: $(tt.params.git-revision)\n          - name: image-name\n            value: image-registry.openshift-image-registry.svc:5000/tekton-demo/nodejs-app\n          - name: deployment-name\n            value: nodejs-app\n        workspaces:\n          - name: shared-workspace\n            persistentVolumeClaim:\n              claimName: pipeline-workspace\n</code></pre> <p>Apply the Trigger resources:</p> <pre><code>oc apply -f eventlistener.yaml -n $NAMESPACE\n</code></pre> <p>Expose the EventListener:</p> <pre><code>oc expose svc el-build-deploy-listener -n $NAMESPACE\n</code></pre> <p>Get the webhook URL:</p> <pre><code>echo \"Webhook URL: http://$(oc get route el-build-deploy-listener -o jsonpath='{.spec.host}' -n $NAMESPACE)\"\n</code></pre> <p>You can configure this URL in your Git repository's webhook settings to trigger the Pipeline on push events.</p>"},{"location":"labs/devops/tekton/#cleanup","title":"Cleanup","text":"<p>To clean up all resources created in this lab:</p> <pre><code># Delete all pipeline resources\noc delete pipelinerun --all -n $NAMESPACE\noc delete taskrun --all -n $NAMESPACE\noc delete pipeline --all -n $NAMESPACE\noc delete task --all -n $NAMESPACE\noc delete pvc pipeline-workspace -n $NAMESPACE\n\n# Delete the application\noc delete deployment,svc,route nodejs-app -n $NAMESPACE\n\n# Delete the project (optional)\noc delete project $NAMESPACE\n</code></pre>"},{"location":"labs/devops/tekton/#summary","title":"Summary","text":"<p>In this lab, you learned how to:</p> <ul> <li>Install and configure OpenShift Pipelines using the Operator</li> <li>Create Tasks and TaskRuns</li> <li>Create Pipelines that clone, build, and deploy applications</li> <li>Use Workspaces to share data between Tasks</li> <li>Monitor Pipelines using the <code>tkn</code> CLI and OpenShift web console</li> <li>Set up Triggers for automated Pipeline execution</li> </ul>"},{"location":"labs/devops/tekton/#additional-resources","title":"Additional Resources","text":"<ul> <li>OpenShift Pipelines Documentation</li> <li>Tekton Documentation</li> <li>Tekton Hub - Reusable Tasks and Pipelines</li> <li>OpenShift Pipelines Tutorial</li> </ul>"},{"location":"labs/kubernetes/","title":"Kubernetes Labs Overview","text":"<p>This section contains hands-on labs to help you practice and apply Kubernetes and OpenShift concepts. Each lab builds on the concepts covered in the course materials.</p>"},{"location":"labs/kubernetes/#labs","title":"Labs","text":"Lab Topic Description Lab 1 Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Lab 2 Probes Create some Health &amp; Startup Probes to find what's causing an issue. Lab 3 Debugging Find which service is breaking in your cluster and find out why. Lab 4 Multiple Containers Build a container using legacy container image. Lab 5 Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Lab 6 Pod Configuration Configure a pod to meet compute resource requirements. Lab 7 Rolling Updates Create a Rolling Update for your application. Lab 8 Cron Jobs Create a CronJob to run periodic tasks in your cluster. Lab 9 Services Create two services with certain requirements. Lab 10 Network Policies Create a policy to allow client pods with labels to access secure pod."},{"location":"labs/kubernetes/#additional-labs","title":"Additional Labs","text":"Lab Topic Description IKS Ingress IKS Ingress Controller Configure Ingress on Free IKS Cluster"},{"location":"labs/kubernetes/#solutions","title":"Solutions","text":"<p>Need help? Check out the Lab Solutions for guidance on completing the labs.</p>"},{"location":"labs/kubernetes/lab-solutions/","title":"Lab Solutions","text":"<ul> <li> <p>Lab 1</p> </li> <li> <p>Lab 2</p> </li> <li> <p>Lab 3</p> </li> <li> <p>Lab 4</p> </li> <li> <p>Lab 5</p> </li> <li> <p>Lab 6</p> </li> <li> <p>Lab 7</p> </li> <li> <p>Lab 8</p> </li> <li> <p>Lab 9</p> </li> <li> <p>Lab 10</p> </li> </ul>"},{"location":"labs/kubernetes/ingress-iks/","title":"Kubernetes Lab Ingress Controller IBM Free Kubernetes cluster","text":"<p>The IBM Kubernetes service free clusters consist of a single worker node with 2 CPU and 4 GB of memory for experimenting with Kubernetes. Unlike the fee-based service, these clusters do not include capabilities for application load balancing using ingress out-of-the-box. </p>"},{"location":"labs/kubernetes/ingress-iks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Free IBM Kubernetes Cluster (IKS) - upgrade your account from Lite plan to create one. In the example commands, we'll assume that this cluster is named <code>mycluster</code></li> <li>kubectl - match your cluster API version </li> <li>Log in to IBM Cloud and configure <code>kubectl</code> using the <code>ibmcloud ks cluster config --cluster mycluster</code> command</li> </ul>"},{"location":"labs/kubernetes/ingress-iks/#components","title":"Components","text":"<p>On the IKS cluster, you will install helm charts for a nginx ingress controller from NGINX. This lab already provides the templated yaml files so there is no need to use helm cli.</p>"},{"location":"labs/kubernetes/ingress-iks/#set-up-the-ingress-controller","title":"Set up the ingress controller","text":"<p>Only do this on a free IKS instance These steps assume facts that only apply to free IKS instances:</p> <ul> <li>a single worker where the cluster administrator can create pods that bind to host ports</li> <li>no pre-existing ingress controller or application load balancer</li> </ul> <p>Using the following steps with a paid instance can cause issues. See the IBM Cloud containers documentation for information on exposing applications with the ingress/alb services for paid clusters. You have been warned</p> <ol> <li> <p>Install the NGINX ingress controller with <code>helm</code> using a daemonset and no service resource (which will result in a single pod that binds to ports 80 and 443 on the worker node and will skip creation of a <code>ClusterIP, LoadBalancer, or NodePort</code> for the daemonset).     <pre><code>kubectl apply -f https://cloudnative101.dev/yamls/ingress-controller/iks-ingress-v1.7.1.yaml\n</code></pre></p> </li> <li> <p>You can use free domain <code>.nip.io</code> to get a domain name using one of the IP Address of your worker nodes. Run this command to set your DOMAIN     <pre><code>export DOMAIN=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}').nip.io\necho $DOMAIN\n</code></pre></p> </li> <li> <p>You can test the ingress controller using the <code>$DOMAIN</code>:</p> <p><pre><code>curl -I http://$DOMAIN\n</code></pre> <pre><code>HTTP/1.1 404 Not Found\nServer: nginx/1.17.10\n...\n</code></pre></p> <p>A 404 is expected at this point because unlike the kubernetes nginx ingress, the NGINX version of the ingress controller does not create a default backend deployment.</p> </li> <li> <p>To use the ingress controller deploy a sample application, expose a service.     <pre><code>kubectl create deployment web --image=bitnami/nginx\nkubectl expose deployment web --name=web --port 8080\n</code></pre></p> </li> <li> <p>Now create an Ingress resource     <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: web.$DOMAIN\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\nEOF\necho \"Access your web app at http://web.$DOMAIN\"\n</code></pre></p> <p>Note: The Ingress API moved from <code>networking.k8s.io/v1beta1</code> to <code>networking.k8s.io/v1</code> in Kubernetes 1.19 (stable in 1.22). The new API requires <code>pathType</code> and uses a different backend syntax. 1. List the created ingress <pre><code>kubectl get ingress web\n</code></pre></p> </li> <li> <p>Access your web application    <pre><code>curl http://web.$DOMAIN\n</code></pre>    The output prints the html    <pre><code>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n</code></pre></p> </li> <li> <p>Delete all the resources created     <pre><code>kubectl delete deployment,svc,ingress -l app=web\n</code></pre></p> </li> </ol>"},{"location":"labs/kubernetes/lab1/","title":"Kubernetes Lab 1 - Pod Creation","text":""},{"location":"labs/kubernetes/lab1/#problem","title":"Problem","text":"<ul> <li>Write a pod definition named <code>yoda-service-pod.yml</code> Then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specifications of this pod are as follows:</p> <ul> <li>Use the <code>bitnami/nginx</code> container image.</li> <li>The container needs a containerPort of <code>80</code>.</li> <li>Set the command to run as <code>nginx</code></li> <li>Pass in the <code>-g daemon off; -q</code> args to run nginx in quiet mode.</li> <li>Create the pod in the <code>web</code> namespace.</li> </ul>"},{"location":"labs/kubernetes/lab1/#verification","title":"Verification","text":"<p>When you have completed this lab, use the following commands to validate your solution. The 'get pods' command will</p> <p><code>kubectl get pods -n web</code> <code>kubectl describe pod nginx -n web</code></p>"},{"location":"labs/kubernetes/lab1/solution/","title":"Kubernetes Lab 1 - Pod Creation","text":""},{"location":"labs/kubernetes/lab1/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: web\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    command: [\"nginx\"]\n    args: [\"-g\", \"daemon off;\", \"-q\"]\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"labs/kubernetes/lab10/","title":"Kubernetes Lab 10 - Network Policies","text":""},{"location":"labs/kubernetes/lab10/#problem","title":"Problem","text":"<p>Network policies allow you to control traffic flow between pods. In this lab, you will create a network policy that restricts access to a secure pod, allowing only pods with a specific label to connect.</p>"},{"location":"labs/kubernetes/lab10/#prerequisites","title":"Prerequisites","text":"<p>Network policies require a CNI plugin that supports them (such as Calico, Cilium, or Weave Net). This lab uses Calico.</p>"},{"location":"labs/kubernetes/lab10/#setup-for-minikube","title":"Setup for Minikube","text":"<p>If using minikube, start it with CNI support and install Calico:</p> <pre><code>minikube start --network-plugin=cni\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml\nkubectl -n kube-system get pods | grep calico-node\n</code></pre> <p>Wait for all calico-node pods to be Running before proceeding.</p>"},{"location":"labs/kubernetes/lab10/#setup-for-openshift","title":"Setup for OpenShift","text":"<p>OpenShift includes network policy support by default. No additional setup is required.</p>"},{"location":"labs/kubernetes/lab10/#setup","title":"Setup","text":""},{"location":"labs/kubernetes/lab10/#step-1-create-the-secured-pod","title":"Step 1: Create the secured pod","text":"<p>Save the following to <code>secure-pod.yaml</code> and apply it:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-secure-pod\n  labels:\n    app: secure-app\nspec:\n  containers:\n    - name: nginx\n      image: bitnami/nginx:1.25\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>kubectl apply -f secure-pod.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab10/#step-2-create-the-client-pod-without-the-required-label","title":"Step 2: Create the client pod (without the required label)","text":"<p>Save the following to <code>client-pod.yaml</code> and apply it:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-client-pod\nspec:\n  containers:\n    - name: busybox\n      image: curlimages/curl:8.5.0\n      command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"]\n</code></pre> <pre><code>kubectl apply -f client-pod.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab10/#step-3-get-the-secure-pod-ip-address","title":"Step 3: Get the secure pod IP address","text":"<pre><code>kubectl get pod network-policy-secure-pod -o jsonpath='{.status.podIP}'\n</code></pre> <p>Save this IP for testing connectivity.</p>"},{"location":"labs/kubernetes/lab10/#step-4-test-connectivity-before-network-policy","title":"Step 4: Test connectivity before network policy","text":"<p>Before applying any network policy, verify the client can reach the secure pod:</p> <pre><code>kubectl exec network-policy-client-pod -- curl -s --max-time 5 http://&lt;SECURE_POD_IP&gt;:8080\n</code></pre> <p>You should see the nginx welcome page HTML.</p>"},{"location":"labs/kubernetes/lab10/#tasks","title":"Tasks","text":"<ol> <li>Create a NetworkPolicy that:</li> <li>Applies to pods with label <code>app: secure-app</code></li> <li>Only allows ingress traffic from pods with label <code>allow-access: \"true\"</code></li> <li> <p>Denies all other ingress traffic</p> </li> <li> <p>Test that the policy works by verifying:</p> </li> <li>The client pod (without the label) cannot access the secure pod</li> <li>After adding the label, the client pod can access the secure pod</li> </ol>"},{"location":"labs/kubernetes/lab10/#hints","title":"Hints","text":"<ul> <li>NetworkPolicy uses <code>podSelector</code> to select which pods the policy applies to</li> <li>Use <code>ingress.from.podSelector</code> to specify which pods can send traffic</li> <li>The <code>policyTypes</code> field should include <code>Ingress</code></li> </ul>"},{"location":"labs/kubernetes/lab10/#verification","title":"Verification","text":""},{"location":"labs/kubernetes/lab10/#test-1-verify-access-is-denied-without-the-label","title":"Test 1: Verify access is denied without the label","text":"<p>After applying the network policy, the client pod should NOT be able to reach the secure pod:</p> <pre><code>kubectl exec network-policy-client-pod -- curl -s --max-time 5 http://&lt;SECURE_POD_IP&gt;:8080\n</code></pre> <p>This should timeout or fail.</p>"},{"location":"labs/kubernetes/lab10/#test-2-add-the-required-label-to-the-client-pod","title":"Test 2: Add the required label to the client pod","text":"<pre><code>kubectl label pod network-policy-client-pod allow-access=true\n</code></pre>"},{"location":"labs/kubernetes/lab10/#test-3-verify-access-is-now-allowed","title":"Test 3: Verify access is now allowed","text":"<pre><code>kubectl exec network-policy-client-pod -- curl -s --max-time 5 http://&lt;SECURE_POD_IP&gt;:8080\n</code></pre> <p>You should now see the nginx welcome page.</p>"},{"location":"labs/kubernetes/lab10/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod network-policy-secure-pod network-policy-client-pod\nkubectl delete networkpolicy my-network-policy\n</code></pre>"},{"location":"labs/kubernetes/lab10/solution/","title":"Kubernetes Lab 10 - Network Policies","text":""},{"location":"labs/kubernetes/lab10/solution/#solution","title":"Solution","text":""},{"location":"labs/kubernetes/lab10/solution/#step-1-create-the-networkpolicy","title":"Step 1: Create the NetworkPolicy","text":"<p>Save the following to <code>network-policy.yaml</code>:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: my-network-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: secure-app\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          allow-access: \"true\"\n</code></pre> <p>Apply the policy:</p> <pre><code>kubectl apply -f network-policy.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab10/solution/#understanding-the-policy","title":"Understanding the Policy","text":"<ul> <li><code>podSelector.matchLabels.app: secure-app</code> - This policy applies to pods with the label <code>app: secure-app</code></li> <li><code>policyTypes: [Ingress]</code> - This policy controls incoming traffic</li> <li><code>ingress.from.podSelector.matchLabels.allow-access: \"true\"</code> - Only allow traffic from pods with this label</li> </ul>"},{"location":"labs/kubernetes/lab10/solution/#step-2-test-that-access-is-blocked","title":"Step 2: Test that access is blocked","text":"<p>Get the secure pod IP:</p> <pre><code>SECURE_POD_IP=$(kubectl get pod network-policy-secure-pod -o jsonpath='{.status.podIP}')\n</code></pre> <p>Try to access from the client pod (should fail/timeout):</p> <pre><code>kubectl exec network-policy-client-pod -- curl -s --max-time 5 http://$SECURE_POD_IP:8080\n</code></pre>"},{"location":"labs/kubernetes/lab10/solution/#step-3-add-the-required-label","title":"Step 3: Add the required label","text":"<pre><code>kubectl label pod network-policy-client-pod allow-access=true\n</code></pre>"},{"location":"labs/kubernetes/lab10/solution/#step-4-test-that-access-is-now-allowed","title":"Step 4: Test that access is now allowed","text":"<pre><code>kubectl exec network-policy-client-pod -- curl -s --max-time 5 http://$SECURE_POD_IP:8080\n</code></pre> <p>You should now see the nginx welcome page HTML.</p>"},{"location":"labs/kubernetes/lab10/solution/#verify-the-network-policy","title":"Verify the network policy","text":"<pre><code>kubectl get networkpolicy my-network-policy\nkubectl describe networkpolicy my-network-policy\n</code></pre>"},{"location":"labs/kubernetes/lab2/","title":"Kubernetes Lab 2 - Probes","text":""},{"location":"labs/kubernetes/lab2/#container-health-issues","title":"Container Health Issues","text":"<p>The first issue is caused by application instances entering an unhealthy state and responding to user requests with error messages. Unfortunately, this state does not cause the container to stop, so the Kubernetes cluster is not able to detect this state and restart the container. Luckily, the application has an internal endpoint that can be used to detect whether or not it is healthy. This endpoint is <code>/healthz</code> on port <code>8080</code>.</p> <ul> <li>Your first task will be to create a probe to check this endpoint periodically.</li> <li>If the endpoint returns an error or fails to respond, the probe will detect this and the cluster will restart the container.</li> </ul>"},{"location":"labs/kubernetes/lab2/#container-startup-issues","title":"Container Startup Issues","text":"<p>Another issue is caused by new pods when they are starting up. The application takes a few seconds after startup before it is ready to service requests. As a result, some users are getting error message during this brief time.</p> <ul> <li> <p>To fix this, you will need to create another probe. To detect whether the application is <code>ready</code>, the probe should simply make a request to the root endpoint, <code>/ready</code>, on port <code>8080</code>. If this request succeeds, then the application is ready.</p> </li> <li> <p>Also set a <code>initial delay</code> of <code>5 seconds</code> for the probes.</p> </li> </ul> <p>Here is the Pod yaml file, add the probes, then create the pod in the cluster to test it.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: energy-shield-service\nspec:\n  containers:\n    - name: energy-shield\n      image: ibmcase/energy-shield:1\n</code></pre>"},{"location":"labs/kubernetes/lab2/solution/","title":"Kubernetes Lab 2 - Probes","text":""},{"location":"labs/kubernetes/lab2/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: energy-shield-service\nspec:\n  containers:\n  - name: energy-shield\n    image: ibmcase/energy-shield:1\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 5\n</code></pre>"},{"location":"labs/kubernetes/lab3/","title":"Kubernetes Lab 3 - Debugging","text":""},{"location":"labs/kubernetes/lab3/#problem","title":"Problem","text":"<p>The Hyper Drive isn't working and we need to find out why. Let's debug the <code>hyper-drive</code> deployment so that we can reach light speed again.</p> <p>Here are some tips to help you solve the Hyper Drive:</p> <ul> <li>Check the description of the <code>deployment</code>.</li> <li>Get and save the logs of one of the broken <code>pods</code>.</li> <li>Are the correct <code>ports</code> assigned.</li> <li>Make sure your <code>labels</code> and <code>selectors</code> are correct.</li> <li>Check to see if the <code>Probes</code> are correctly working.</li> <li>To fix the deployment, save then modify the yaml file for redeployment.</li> </ul> <p>Reset the environment:</p> <pre><code>minikube delete\nminikube start\n</code></pre> <p>Setup the environment:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/lab-setup/lab-5-debug-k8s-setup.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab3/#validate","title":"Validate","text":"<p>Once you get the Hyper Drive working again. Verify it by checking the endpoints.</p> <pre><code>kubectl get ep hyper-drive\n</code></pre>"},{"location":"labs/kubernetes/lab3/solution/","title":"Kubernetes Lab 3 - Debugging","text":""},{"location":"labs/kubernetes/lab3/solution/#solution","title":"Solution","text":"<p>Check <code>STATUS</code> column for not Ready <pre><code>    kubectl get pods --all-namespaces\n</code></pre></p> <p>Check the description of the deployment     <pre><code>kubectl describe deployment hyper-drive\n</code></pre>    Save logs for a broken pod</p> <pre><code>kubectl logs &lt;pod name&gt; -n &lt;namespace&gt; &gt; /home/cloud_user/debug/broken-pod-logs.log\n</code></pre> <p>In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080.</p> <p>To fix probe, can't kubectl edit, need to delete and recreate the deployment     <pre><code>kubectl get deployment &lt;deployment name&gt; -n &lt;namespace&gt; -o yaml --export &gt; hyper-drive.yml\n</code></pre></p> <p>Delete pod     <pre><code>kubectl delete deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre>    Can also use <code>kubectl replace</code></p> <p>Edit yaml, and apply     <pre><code>kubectl apply -f hyper-drive.yml -n &lt;namespace&gt;\n</code></pre></p> <p>Verify     <pre><code>kubectl get deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"labs/kubernetes/lab4/","title":"Kubernetes Lab 4 - Manage Multiple Containers","text":""},{"location":"labs/kubernetes/lab4/#problem","title":"Problem","text":"<p>This service has already been packaged into a container image, but there is one special requirement:</p> <ul> <li>The legacy app is hard-coded to only serve content on port <code>8989</code>, but the team wants to be able to access the service using the standard port <code>80</code>.</li> </ul> <p>Your task is to build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port <code>80</code>.</p> <p>This setup will need to meet the following specifications:</p> <ul> <li>The pod should have the name <code>vader-service</code>.</li> <li>The <code>vader-service</code> pod should have a container that runs the legacy vader service image: <code>ibmcase/millennium-falcon:1</code>.</li> <li>The <code>vader-service</code> pod should have an ambassador container that runs the <code>haproxy:2.8</code> image and proxies incoming traffic on port <code>80</code> to the legacy service on port <code>8989</code> (the HAProxy configuration for this is provided below).</li> <li>Port <code>80</code> should be exposed as a <code>containerPort</code>.</li> </ul> <p> <p>Note: You do not need to expose port 8989</p> <p></p> <ul> <li>The HAProxy configuration should be stored in a ConfigMap called <code>vader-service-ambassador-config</code>.</li> <li>The HAProxy config should be provided to the ambassador container using a volume mount that places the data from the ConfigMap in a file at /usr/local/etc/haproxy/haproxy.cfg.   haproxy.cfg should contain the following configuration data:</li> </ul> <pre><code>global\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nlisten http-in\n    bind *:80\n    server server1 127.0.0.1:8989 maxconn 32\n</code></pre> <p>Once your pod is up and running, it's a good idea to test it to make sure you can access the service from within the cluster using port 80. In order to do this, you can create a busybox pod in the cluster, and then run a command to attempt to access the service from within the busybox pod.</p> <p>Create a descriptor for the busybox pod called <code>busybox.yml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n    - name: myapp-container\n      image: curlimages/curl:8.5.0\n      command: [\"sh\", \"-c\", \"while true; do sleep 3600; done\"]\n</code></pre> <p>Create the busybox testing pod.</p> <pre><code>kubectl apply -f busybox.yml\n</code></pre> <p>Use this command to access <code>vader-service</code> using port 80 from within the busybox pod.</p> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:.status.podIP --no-headers):80\n</code></pre> <p>If the service is working, you should get a message that the hyper drive of the millennium falcon needs repair.</p> <p>Relevant Documentation:</p> <ul> <li>Kubernetes Sidecar Logging Agent</li> <li>Shared Volumes</li> <li>Distributed System Toolkit Patterns</li> </ul>"},{"location":"labs/kubernetes/lab4/solution/","title":"Kubernetes Lab 4 - Manage Multiple Containers","text":""},{"location":"labs/kubernetes/lab4/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vader-service-ambassador-config\ndata:\n  haproxy.cfg: |-\n    global\n        daemon\n        maxconn 256\n\n    defaults\n        mode http\n        timeout connect 5000ms\n        timeout client 50000ms\n        timeout server 50000ms\n\n    listen http-in\n        bind *:80\n        server server1 127.0.0.1:8989 maxconn 32\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: vader-service\nspec:\n  containers:\n  - name: millennium-falcon\n    image: ibmcase/millennium-falcon:1\n  - name: haproxy-ambassador\n    image: haproxy:2.8\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /usr/local/etc/haproxy\n  volumes:\n  - name: config-volume\n    configMap:\n      name: vader-service-ambassador-config\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: curlimages/curl:8.5.0\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n</code></pre> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=jsonpath='{.status.podIP}'):80\n</code></pre>"},{"location":"labs/kubernetes/lab5/","title":"Kubernetes Lab 5 - Persistent Volumes","text":""},{"location":"labs/kubernetes/lab5/#problem","title":"Problem","text":"<p>The death star plans can't be lost no matter what happens so we need to make sure we protect them at all costs.</p> <p>In order to do that you will need to do the following:</p> <p>Create a <code>PersistentVolume</code>:</p> <ul> <li> <p>The PersistentVolume should be named <code>postgresql-pv</code>.</p> </li> <li> <p>The volume needs a capacity of <code>1Gi</code>.</p> </li> <li> <p>Use a storageClassName of <code>localdisk</code>.</p> </li> <li> <p>Use the accessMode <code>ReadWriteOnce</code>.</p> </li> <li> <p>Store the data locally on the node using a <code>hostPath</code> volume at the location <code>/mnt/data</code>.</p> </li> </ul> <p>Create a <code>PersistentVolumeClaim</code>:</p> <ul> <li> <p>The PersistentVolumeClaim should be named <code>postgresql-pv-claim</code>.</p> </li> <li> <p>Set a resource request on the claim for <code>500Mi</code> of storage.</p> </li> <li> <p>Use the same storageClassName and accessModes as the PersistentVolume so that this claim can bind to the PersistentVolume.</p> </li> </ul> <p>Create a <code>Postgresql</code> Pod configured to use the <code>PersistentVolumeClaim</code>:</p> <ul> <li> <p>The Pod should be named <code>postgresql-pod</code>.</p> </li> <li> <p>Use the image <code>bitnami/postgresql</code>.</p> </li> <li> <p>Expose the containerPort <code>5432</code>.</p> </li> <li> <p>Set an <code>environment variable</code> called <code>POSTGRES_PASSWORD</code> with the value <code>password</code>.</p> </li> <li> <p>Add the <code>PersistentVolumeClaim</code> as a volume and mount it to the container at the path <code>/bitnami/postgresql/</code>.</p> </li> </ul>"},{"location":"labs/kubernetes/lab5/solution/","title":"Kubernetes Lab 5 - Persistent Volumes","text":""},{"location":"labs/kubernetes/lab5/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgresql-pv\nspec:\n  storageClassName: localdisk\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgresql-pv-claim\nspec:\n  storageClassName: localdisk\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgresql-pod\nspec:\n  containers:\n  - name: postgresql\n    image: bitnami/postgresql\n    ports:\n    - containerPort: 5432\n    env:\n    - name: POSTGRES_PASSWORD\n      value: password\n    volumeMounts:\n    - name: sql-storage\n      mountPath: /bitnami/postgresql/\n  volumes:\n  - name: sql-storage\n    persistentVolumeClaim:\n      claimName: postgresql-pv-claim\n</code></pre> <p>verify via <code>ls /mnt/data</code> on node</p>"},{"location":"labs/kubernetes/lab6/","title":"Kubernetes Lab 6 - Pod Configuration","text":""},{"location":"labs/kubernetes/lab6/#problem","title":"Problem","text":"<ul> <li>Create a pod definition named <code>yoda-service-pod.yml</code>, and then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specifications are as follows:</p> <ul> <li>The current image for the container is <code>bitnami/nginx</code>. You do not need a custom command or args.</li> <li>There is some configuration data the container will need:</li> <li><code>yoda.baby.power=100000000</code></li> <li><code>yoda.strength=10</code></li> <li>It will expect to find this data in a file at <code>/etc/yoda-service/yoda.cfg</code>. Store the configuration data in a ConfigMap called <code>yoda-service-config</code> and provide it to the container as a mounted volume.</li> <li>The container should expect to use <code>64Mi</code> of memory and <code>250m</code> CPU (use resource requests).</li> <li>The container should be limited to <code>128Mi</code> of memory and <code>500m</code> CPU (use resource limits).</li> <li>The container needs access to a database password in order to authenticate with a backend database server. The password is <code>0penSh1ftRul3s!</code>. It should be stored as a Kubernetes secret called <code>yoda-db-password</code> and passed to the container as an environment variable called <code>DB_PASSWORD</code>.</li> <li>The container will need to access the Kubernetes API using the ServiceAccount <code>yoda-svc</code>. Create the service account if it doesn't already exist, and configure the pod to use it.</li> </ul>"},{"location":"labs/kubernetes/lab6/#verification","title":"Verification","text":"<p>To verify your setup is complete, check <code>/etc/yoda-service</code> for the <code>yoda.cfg</code> file and use the <code>cat</code> command to check it's contents.</p> <pre><code>kubectl exec -it yoda-service /bin/bash\ncd /etc/yoda-service\ncat yoda.cfg\n</code></pre>"},{"location":"labs/kubernetes/lab6/solution/","title":"Kubernetes Lab 6 - Pod Configuration","text":""},{"location":"labs/kubernetes/lab6/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: yoda-service-config\ndata:\n  yoda.cfg: |-\n    yoda.baby.power=100000000\n    yoda.strength=10\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: yoda-svc\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: yoda-db-password\nstringData:\n  password: 0penSh1ftRul3s!\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: yoda-service\nspec:\n  serviceAccountName: yoda-svc\n  containers:\n  - name: yoda-service\n    image: bitnami/nginx\n    volumeMounts:\n      - name: config-volume\n        mountPath: /etc/yoda-service\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: yoda-db-password\n          key: password\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n  volumes:\n  - name: config-volume\n    configMap:\n      name: yoda-service-config\n</code></pre>"},{"location":"labs/kubernetes/lab7/","title":"Kubernetes Lab 7 - Rolling Updates","text":""},{"location":"labs/kubernetes/lab7/#problem","title":"Problem","text":"<p>Your company's developers have just finished developing a new version of their jedi-themed mobile game. They are ready to update the backend services that are running in your Kubernetes cluster. There is a deployment in the cluster managing the replicas for this application. The deployment is called <code>jedi-deployment</code>. You have been asked to update the image for the container named <code>jedi-ws</code> in this deployment template to a new version, <code>bitnami/nginx:1.19.0</code>.</p> <p>After you have updated the image using a rolling update, check on the status of the update to make sure it is working. If it is not working, perform a rollback to the previous state.</p>"},{"location":"labs/kubernetes/lab7/#setup","title":"Setup","text":"<p>First, create the initial deployment by saving the following YAML to a file named <code>jedi-deployment.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jedi-deployment\n  labels:\n    app: jedi\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: jedi\n  template:\n    metadata:\n      labels:\n        app: jedi\n    spec:\n      containers:\n      - name: jedi-ws\n        image: bitnami/nginx:1.18.0\n        ports:\n        - containerPort: 8080\n</code></pre> <p>Apply the deployment:</p> <pre><code>kubectl apply -f jedi-deployment.yaml\n</code></pre> <p>Verify the deployment is running:</p> <pre><code>kubectl get deployment jedi-deployment\nkubectl get pods -l app=jedi\n</code></pre>"},{"location":"labs/kubernetes/lab7/#tasks","title":"Tasks","text":"<ol> <li>Update the deployment to use the new image <code>bitnami/nginx:1.19.0</code> for the container named <code>jedi-ws</code></li> <li>Check the rollout status to verify the update is progressing</li> <li>View the rollout history to see the deployment revisions</li> <li>If the update fails, perform a rollback to the previous working version</li> </ol>"},{"location":"labs/kubernetes/lab7/#hints","title":"Hints","text":"<ul> <li>Use <code>kubectl set image</code> to update the container image</li> <li>Use <code>kubectl rollout status</code> to check rollout progress</li> <li>Use <code>kubectl rollout history</code> to view revision history</li> <li>Use <code>kubectl rollout undo</code> to rollback if needed</li> <li>Watch pods in real-time with <code>kubectl get pods -w</code></li> </ul>"},{"location":"labs/kubernetes/lab7/#verification","title":"Verification","text":"<p>After completing the lab, you should be able to:</p> <ol> <li>See the deployment running with the updated image:</li> </ol> <pre><code>kubectl describe deployment jedi-deployment | grep Image\n</code></pre> <ol> <li>View the rollout history showing multiple revisions:</li> </ol> <pre><code>kubectl rollout history deployment/jedi-deployment\n</code></pre>"},{"location":"labs/kubernetes/lab7/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete deployment jedi-deployment\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/","title":"Kubernetes Lab 7 - Rolling Updates","text":""},{"location":"labs/kubernetes/lab7/solution/#solution","title":"Solution","text":""},{"location":"labs/kubernetes/lab7/solution/#step-1-update-the-deployment-to-the-new-version","title":"Step 1: Update the deployment to the new version","text":"<pre><code>kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.19.0\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/#step-2-check-the-progress-of-the-rolling-update","title":"Step 2: Check the progress of the rolling update","text":"<pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre> <p>In another terminal window, watch the pods:</p> <pre><code>kubectl get pods -w\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/#step-3-view-the-rollout-history","title":"Step 3: View the rollout history","text":"<p>Get a list of previous revisions:</p> <pre><code>kubectl rollout history deployment/jedi-deployment\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/#step-4-rollback-if-needed","title":"Step 4: Rollback if needed","text":"<p>If the update fails or you need to rollback, undo the last revision:</p> <pre><code>kubectl rollout undo deployment/jedi-deployment\n</code></pre> <p>Check the status of the rollback:</p> <pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre>"},{"location":"labs/kubernetes/lab7/solution/#verify-the-current-image","title":"Verify the current image","text":"<pre><code>kubectl describe deployment jedi-deployment | grep Image\n</code></pre> <p>Expected output after successful update:</p> <pre><code>Image: bitnami/nginx:1.19.0\n</code></pre>"},{"location":"labs/kubernetes/lab8/","title":"Kubernetes Lab 8 - Cron Jobs","text":""},{"location":"labs/kubernetes/lab8/#problem","title":"Problem","text":"<p>Your commander has a simple data process that is run periodically to check status. They would like to stop doing this manually in order to save time, so you have been asked to implement a cron job in the Kubernetes cluster to run this process.</p> <ul> <li>Create a cron job called xwing-cronjob using the <code>ibmcase/xwing-status:1.0</code> image.</li> <li>Have the job run every second minute with the following cron expression: <code>*/2 * * * *</code>.</li> <li>Pass the argument <code>/usr/sbin/xwing-status.sh</code> to the container.</li> </ul>"},{"location":"labs/kubernetes/lab8/#verification","title":"Verification","text":"<ul> <li>Run <code>kubectl get cronjobs.batch</code> and <code>LAST-SCHEDULE</code> to see last time it ran</li> <li>From a bash shell, run the following to see the logs for all jobs:</li> </ul> <pre><code>jobs=( $(kubectl get jobs --no-headers -o custom-columns=\":metadata.name\") )\necho -e \"Job \\t\\t\\t\\t Pod \\t\\t\\t\\t\\tLog\"\nfor job in \"${jobs[@]}\"\ndo\n   pod=$(kubectl get pods -l job-name=$job --no-headers -o custom-columns=\":metadata.name\")\n   echo -en \"$job \\t $pod \\t\"\n   kubectl logs $pod\ndone\n</code></pre>"},{"location":"labs/kubernetes/lab8/solution/","title":"Kubernetes Lab 8 - Cron Jobs","text":""},{"location":"labs/kubernetes/lab8/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: xwing-cronjob\nspec:\n  schedule: \"*/2 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: xwing-status\n            image: ibmcase/xwing-status:1.0\n            args:\n            - /usr/sbin/xwing-status.sh\n          restartPolicy: OnFailure\n</code></pre> <p>Note: The CronJob API was moved from <code>batch/v1beta1</code> to <code>batch/v1</code> in Kubernetes 1.21. Use <code>batch/v1</code> for clusters running Kubernetes 1.21 or later.</p>"},{"location":"labs/kubernetes/lab8/solution/#verify-the-cronjob","title":"Verify the CronJob","text":"<pre><code>kubectl get cronjob xwing-cronjob\n</code></pre>"},{"location":"labs/kubernetes/lab9/","title":"Kubernetes Lab 9 - Services","text":""},{"location":"labs/kubernetes/lab9/#problem","title":"Problem","text":"<p>We have a <code>jedi-deployment</code> and <code>yoda-deployment</code> that need to communicate with others. The <code>jedi</code> needs to talk to the world(outside the cluster), while <code>yoda</code> only needs to talk to jedi council(others in the cluster).</p>"},{"location":"labs/kubernetes/lab9/#your-task","title":"Your Task","text":"<ul> <li>Examine the two deployments, and create two services that meet the following criteria:</li> </ul> <p>jedi-svc</p> <ul> <li>The service name is <code>jedi-svc</code>.</li> <li>The service exposes the pod replicas managed by the deployment named <code>jedi-deployment</code>.</li> <li>The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.</li> <li>The service type is <code>NodePort</code>.</li> </ul> <p>yoda-svc</p> <ul> <li>The service name is <code>yoda-svc</code>.</li> <li>The service exposes the pod replicas managed by the deployment named <code>yoda-deployment</code>.</li> <li>The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.</li> <li>The service type is <code>ClusterIP</code>.</li> </ul>"},{"location":"labs/kubernetes/lab9/#setup-environment","title":"Setup environment:","text":"<pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-8-service-setup.yaml\n</code></pre>"},{"location":"labs/kubernetes/lab9/solution/","title":"Kubernetes Lab 9 - Services","text":""},{"location":"labs/kubernetes/lab9/solution/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jedi-svc\nspec:\n  type: NodePort\n  selector:\n    app: jedi\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: yoda-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: yoda\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"openshift/","title":"What is Container Orchestration?","text":""},{"location":"openshift/#introduction","title":"Introduction","text":"<p>Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications. Learn how Kubernetes enables cost-effective cloud native development.</p>"},{"location":"openshift/#what-is-openshift","title":"What is OpenShift?","text":"<p>Red Hat OpenShift is an open-source container platform that runs on the Red Hat enterprise Linux operating system and Kubernetes. It is typically referred to as a \"Platform as a Service\" (PaaS) due to its combination of services for enterprise businesses, including the Kubernetes platform and Docker container images. OpenShift offers additional features exclusive to its enterprise platform. It allows for deploying apps on highly available clusters and securing hybrid workloads with developer-focused tools and seamless integration of IBM, CloudPak, and Red Hat content.</p> <p>Red Hat OpenShift provides a uniform platform across public and private clouds for full portability, standardization, and ease of adoption. It offers various forms to meet customer needs, such as Red Hat OpenShift Container Platform (OCP), Red Hat OpenShift Dedicated (OSD), Microsoft Azure Red Hat OpenShift, and Red Hat OpenShift Online.</p> <p>In summary, Red Hat OpenShift is an enterprise-ready Kubernetes container platform with full-stack automated operations for managing hybrid cloud and multi-cloud deployments. It offers multiple offerings to cater to diverse customer requirements and ensures a consistent experience across public and private clouds.</p>"},{"location":"openshift/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes\u2014also known as \u2018k8s\u2019 or \u2018kube\u2019\u2014is a container orchestration platform for scheduling and automating the deployment, management, and scaling of containerized applications.</p> <p>Kubernetes was first developed by engineers at Google before being open sourced in 2014. It is a descendant of \u2018Borg,\u2019 a container orchestration platform used internally at Google. (Kubernetes is Greek for helmsman or pilot, hence the helm in the Kubernetes logo.)</p> <p>Today, Kubernetes and the broader container ecosystem are maturing into a general-purpose computing platform and ecosystem that rivals\u2014if not surpasses\u2014virtual machines (VMs) as the basic building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver a high-productivity Platform-as-a-Service (PaaS) that addresses multiple infrastructure- and operations-related tasks and issues surrounding cloud native development so that development teams can focus solely on coding and innovation.     </p> <ul> <li> <p> Learning Kubernetes</p> <p>Learning Kubernetes through IBM Learning</p> <p> Getting started</p> </li> </ul>"},{"location":"openshift/#presentations","title":"Presentations","text":"<p>Kubernetes Overview </p>"},{"location":"openshift/#predictable-demands-pattern","title":"Predictable Demands Pattern","text":"<p>An application's performance, efficiency, and behaviors are reliant upon it's ability to have the appropriate allocation of resources.  The Predictable Demands pattern is based on declaring the dependencies and resources needed by a given application.  The scheduler will prioritize an application with a defined set of resources and dependencies since it can better manage the workload across nodes in the cluster.  Each application has a different set of dependencies which we will touch on next.</p>"},{"location":"openshift/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>One of the most common runtime dependencies is the exposure of a container's specific port through hostPort.  Different applications can specify the same port through hostPort which reserves the port on each node in the cluster for the specific container.  This declaration restricts multiple containers with the same hostPort to be deployed on the same nodes in the cluster and restricts the scale of pods to the number of nodes you have in the cluster.  </p> <p>Another runtime dependency is file storage for saving the application state.  Kubernetes offers Pod-level storage utilities that are capable of surviving container restarts.  Applications needing to read or write to these storage mechanisms will require nodes that is provided the type of volume required by the application.  If there is no nodes available with the required volume type, then the pod will not be scheduled to be deployed at all.</p> <p>A different kind of dependency is configurations.  ConfigMaps are used by Kubernetes to strategically plan out how to consume it's settings through either environment variables or the filesystem.  Secrets are consumed the same was as a ConfigMap in Kubernetes.  Secrets are a more secure way to distribute environment-specific configurations to containers within the pod. </p>"},{"location":"openshift/#resource-profiles","title":"Resource Profiles","text":"<p>Resource Profiles are definitions for the compute resources required for a container.  Resources are categorized in two ways, compressible and incompressible.  Compressible resources include resources that can be throttled such as CPU or network bandwidth. Incompressible represents resouces that can't be throttled such as memory where there is no other way to release the allocated resource other than killing the container.  The difference between compressible and incompressible is very important when it comes to planning the deployment of pods and containers since the resource allocation can be affected by the limits of each.</p> <p>Every application needs to have a specified minimum and maximum amount of resources that are needed.  The minimum amount is called \"requests\" and the maximum is the \"limits\".  The scheduler uses the requests to determine the assignment of pods to nodes ensuring that the node will have enough capacity to accommodate the pod and all of it's containers required resources.  An example of defined resource limits is below:</p> <p>Different levels of Quality of Service (QoS) are offered based on the specified requests and limits.</p>"},{"location":"openshift/#quality-of-service-levels","title":"Quality of Service Levels","text":"Best Effort Lowest priority pod with no requests or limits set for its containers. These pods will be the first of any pods killed if resources run low. Burstable Limits and requests are defined but they are not equal. The pod will use the minimum amount of resources, but will consume more if needed up to the limit. If the needed resources become scarce then these pods will be killed if no Best Effort pods are left. Guaranteed Highest priority pods with an equal amount of requests and limits. These pods will be the last to be killed if resources run low and no Best Effort or Burstable pods are left."},{"location":"openshift/#pod-priority","title":"Pod Priority","text":"<p>The priority of pods can be defined through a PriorityClass object. The PriorityClass object allows developers to indicate the importance of a pod relative to the other pods in the cluster.  The higher the priority number then the higher the priority of the pod. The scheduler looks at a pods priorityClassName to populate the priority of new pods.  As pods are being placed in the scheduling queue for deployment, the scheduler orders them from highest to lowest.</p> <p>Another key feature for pod priority is the Preemption feature.  The Preemption feature occurs when there are no nodes with enough capacity to place a pod.  If this occurs the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority.  This effectively allows system administrators the ability to control which critical pods get top priority for resources in the cluster as well as controlling which critical workloads are able to be run on the cluster first. If a pod can not be scheduled due to constraints it will continue on with lower-priority nodes.</p> <p>Pod Priority should be used with caution for this gives users the ability to control over the kubernetes scheduler and ability to place or kill pods that may interrupt the cluster's critical functions.  New pods with higher priority than others can quickly evict pods with lower priority that may be critical to a container's performance.  ResourceQuota and PodDisruptionBudget are two tools that help combat this from happening read more here.</p>"},{"location":"openshift/#benefits-of-container-orchestration","title":"Benefits of Container Orchestration","text":"<ul> <li>Simplified Operations - Automates deployment, scaling, and management of containerized applications</li> <li>High Availability - Automatically restarts failed containers and reschedules them on healthy nodes</li> <li>Scalability - Easily scale applications up or down based on demand</li> <li>Resource Optimization - Efficiently allocates resources across the cluster to maximize utilization</li> <li>Service Discovery and Load Balancing - Built-in DNS and load balancing for container communication</li> <li>Self-Healing - Automatically replaces and reschedules containers when nodes fail</li> <li>Rolling Updates and Rollbacks - Deploy new versions with zero downtime and easily rollback if issues arise</li> <li>Secret and Configuration Management - Securely manage sensitive information and application configuration</li> </ul>"},{"location":"openshift/orchestrationConcepts/","title":"Concepts in Container Orchestration","text":""},{"location":"openshift/orchestrationConcepts/#declarative-deployment-pattern","title":"Declarative Deployment Pattern","text":"<p>With a growing number of microservices, reliance on an updating process for the services has become ever more important. Upgrading services is usually accompanied with some downtime for users or an increase in resource usage.  Both of these can lead to an error effecting the performance of the application making the release process a bottleneck.  </p> <p>A way to combat this issue in Kubernetes is through the use of Deployments.  There are different approaches to the updating process that we will cover below. Any of these approaches can be put to use in order to save time for developers during their release cycles which can last from a few minutes to a few months. </p>"},{"location":"openshift/orchestrationConcepts/#rolling-deployment","title":"Rolling Deployment","text":"<p>A Rolling Deployment ensures that there is no downtime during the update process.  Kubernetes creates a new ReplicaSet for the new version of the service to be rolled out.  From there Kubernetes creates set of pods of the new version while leaving the old pods running.  Once the new pods are all up and running they will replace the old pods and become the primary pods users access.</p> <p></p> <p>The upside to this approach is that there is no downtime and the deployment is handled by kubernetes through a deployment like the one below. The downside is with two sets of pods running at one time there is a higher usage of resources that may lead to performance issues for users. </p>"},{"location":"openshift/orchestrationConcepts/#fixed-deployment","title":"Fixed Deployment","text":"<p>A Fixed Deployment uses the Recreate strategy which sets the maxUnavailable setting to the number of declared replicas.  This in effect starts the versions of the pods as the old versions are being killed.  The starting and stopping of containers does create a little bit of downtime for customers while the starting and stopping is taking place, but the positive side is the users will only have to handle one version at a time.</p> <p></p>"},{"location":"openshift/orchestrationConcepts/#blue-green-release","title":"Blue-Green Release","text":"<p>A Blue-Green Release involves a manual process of creating a second deployment of pods with the newest version of the application running as well as keeping the old version of pods running in the cluster.  Once the new pods are up and running properly the administrator shifts the traffic over to the new pods. Below is a diagram showing both versions up and running with the traffic going to the newer (green) pods.</p> <p></p> <p>The downfall to this approach is the use of resources with two separate groups of pods running at the same time which could cause performance issues or complications. However, the advantage of this approach is users only experience one version at a time and it's easy to quickly switch back to the old version with no downtime if an issue arises with the newer version.</p>"},{"location":"openshift/orchestrationConcepts/#canary-release","title":"Canary Release","text":"<p>A Canary Release involves only standing up one pod of the new application code and shifting only a limited amount of new users traffic to that pod.  This approach reduces the number of people exposed to the new service allowing the administrator to see how the new version is performing.  Once the team feels comfortable with the performance of the new service then more pods can be stood up to replace the old pods.  An advantage to this approach is no downtime with any of the services as the new service is being scaled. </p> <p></p>"},{"location":"openshift/orchestrationConcepts/#health-probe-pattern","title":"Health Probe Pattern","text":"<p>The Health Probe pattern revolves the health of applications being communicated to Kubernetes. To be fully-automatable, cloud-applications must be highly observable in order for Kubernetes to know which applications are up and ready to receive traffic and which cannot. Kubernetes can use that information for traffic direction, self-healing, and to achieve the desired state of the application.</p>"},{"location":"openshift/orchestrationConcepts/#process-health-checks","title":"Process Health Checks","text":"<p>The simplest health check in kubernetes is the Process Health Check.  Kubernetes simply probes the application's processes to see if they are running or not. The process check tells kubernetes when a process for an application needs to be restarted or shut down in the case of a failure.</p>"},{"location":"openshift/orchestrationConcepts/#liveness-probes","title":"Liveness Probes","text":"<p>A Liveness Probe is performed by the Kubernetes Kubelet agent and asks the container to confirm it's health.  A simple process check can return that the container is healthy, but the container to users may not be performing correctly.  The liveness probe addresses this issue but asking the container for its health from outside of the container itself. If a failure is found it may require that the container be restarted to get back to normal health.  A liveness probe can perform the following actions to check health:</p> <ul> <li>HTTP GET and expects a success which is code 200-399.</li> <li>A TCP Socket Probe and expects a successful connection.</li> <li>A Exec Probe which executes a command and expects a successful exit code (0).</li> </ul> <p>The action chosen to be performed for testing depends on the nature of the application and which action fits best. Always keep in mind that a failing health check results in a restart of the container from Kubernetes, so make sure the right health check is in place if the underlying issue can't be fixed.</p>"},{"location":"openshift/orchestrationConcepts/#readiness-probes","title":"Readiness Probes","text":"<p>A Readiness Probe is very similar to a Liveness probe, but the resulting action to a failed Readiness probe is different.  When a liveness probe fails the container is restarted and, in some scenarios, a simple restart won't fix the issue, which is where a readiness probe comes in.  A failed readiness probe won't restart the container but will disconnect it from the traffic endpoint.  Removing a container from traffic allows it to get up and running smoothly before being tossed into service unready to handle requests from users.  Readiness probes give an application time to catch up and make itself ready again to handle more traffic versus shutting down completely and simply creating a new pod. In most cases, liveness and readiness probes are run together on the same application to make sure that the container has time to get up and running properly as well as stays healthy enough to handle the traffic. </p>"},{"location":"openshift/orchestrationConcepts/#managed-lifecycle-pattern","title":"Managed Lifecycle Pattern","text":"<p>The Managed Lifecycle pattern describes how containers need to adapt their lifecycles based on the events that are communicated from a managing platform such as Kubernetes.  Containers do not have control of their own lifecycles.  It's the managing platforms that allow them to live or die, get traffic or have none, etc.  This pattern covers how the different events can affect those lifecycle decisions.</p>"},{"location":"openshift/orchestrationConcepts/#sigterm","title":"SIGTERM","text":"<p>The SIGTERM is a signal that is sent from the managing platform to a container or pod that instructs the pod or container to shutdown or restart.  This signal can be sent due to a failed liveness test or a failure inside the container.  SIGKILL allows the container to cleaning and properly shut itself down versus SIGKILL, which we will get to next. Once received, the application will shutdown as quickly as it can, allowing other processes to stop properly and cleaning up other files.  Each application will have a different shutdown time based on the tasks needed to be done.</p>"},{"location":"openshift/orchestrationConcepts/#sigkill","title":"SIGKILL","text":"<p>SIGKILL is a signal sent to a container or pod forcing it to shutdown.  A SIGKILL is normally sent after the SIGTERM signal.  There is a default 30 second grace period between the time that SIGTERM is sent to the application and SIGKILL is sent.  The grace period can be adjusted for each pod using the .spec.terminationGracePeriodSeconds field. The overall goal for containerized applications should be aimed to have designed and implemented quick startup and shutdown operations.</p>"},{"location":"openshift/orchestrationConcepts/#poststart","title":"postStart","text":"<p>The postStart hook is a command that is run after the creation of a container and begins asynchronously with the container's primary process. PostStart is put in place in order to give the container time to warm up and check itself during startup.  During the postStart loop the container will be labeled in \"pending\" mode in kubernetes while running through it's initial processes.  If the postStart function errors out it will do so with a nonzero exit code and the container process will be killed by Kubernetes.  Careful planning must be done when deciding what logic goes into the postStart function because if it fails the container will also fail to start.  Both postStart and preStop have two handler types that they run:</p> <ul> <li> <p>exec: Runs a command directly in the container.</p> </li> <li> <p>httpGet: Executes an HTTP GET request against an opened port on the pod container.</p> </li> </ul>"},{"location":"openshift/orchestrationConcepts/#prestop","title":"preStop","text":"<p>The preStop hook is a call that blocks a container from terminating too quickly and makes sure the container has a graceful shutdown.  The preStop call must finish before the container is deleted by the container runtime.  The preStop signal does not stop the container from being deleted completely, it is only an alternative to a SIGTERM signal for a graceful shutdown. </p>"},{"location":"openshift/configuration/","title":"Container Configuration","text":""},{"location":"openshift/configuration/#command-and-argument","title":"Command and Argument","text":"<p>When you create a Pod, you can define a command and arguments for the containers that run in the Pod.</p> <p>The command and arguments that you define in the configuration file override the default command and arguments provided by the container image</p> <ul> <li>Dockerfile vs Kubernetes</li> <li>Dockerfile Entrypoint -&gt; k8s command</li> <li>Dockerfile CMD -&gt; k8s args</li> </ul>"},{"location":"openshift/configuration/#ports","title":"Ports","text":"<p>When you create a Pod, you can specify the port number the container exposes, as best practice is good to put a <code>name</code>, this way a service can specify targetport by name reference.</p>"},{"location":"openshift/configuration/#environment-variable","title":"Environment Variable","text":"<p>When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the container configuration</p> <p>A Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields</p>"},{"location":"openshift/configuration/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Container Commands </p> <p>Environment Variables </p> <p>Pod Exposing </p>"},{"location":"openshift/configuration/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cmd-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n  restartPolicy: Never\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-arg-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n      args: [\"Hello World\"]\n  restartPolicy: Never\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-port-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-env-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: c\n      image: busybox\n      env:\n        - name: DEMO_GREETING\n          value: \"Hello from the environment\"\n      command: [\"echo\"]\n      args: [\"$(DEMO_GREETING)\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-inter-pod\n  labels:\n    app: jedi\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n          name: http\n      env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n      command: [\"echo\"]\n      args: [\"$(MY_NODE_NAME) $(MY_POD_NAME) $(MY_POD_IP)\"]\n</code></pre>"},{"location":"openshift/configuration/#resource-requirements","title":"Resource Requirements","text":"<p>When you specify a Pod, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on.</p> <p>CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes.</p>"},{"location":"openshift/configuration/#resources_1","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Compute Resources </p> <p>Memory Management </p>"},{"location":"openshift/configuration/#references_1","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n</code></pre> <p>Namespaced defaults mem</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\nspec:\n  limits:\n    - default:\n        memory: 512Mi\n      defaultRequest:\n        memory: 256Mi\n      type: Container\n</code></pre> <p>Namespaced defaults mem</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-limit-range\nspec:\n  limits:\n    - default:\n        cpu: 1\n      defaultRequest:\n        cpu: 0.5\n      type: Container\n</code></pre>"},{"location":"openshift/configuration/#activities","title":"Activities","text":"Task Description Link Try It Yourself Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration"},{"location":"openshift/configuration/rbac/","title":"Role-Based Access Control (RBAC)","text":"<p>Role-Based Access Control (RBAC) is a method of regulating access to resources based on the roles of individual users. RBAC uses the <code>rbac.authorization.k8s.io</code> API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.</p>"},{"location":"openshift/configuration/rbac/#rbac-components","title":"RBAC Components","text":"<p>RBAC authorization uses four kinds of Kubernetes objects:</p> Object Scope Description Role Namespace Grants permissions within a specific namespace ClusterRole Cluster-wide Grants permissions cluster-wide or to cluster-scoped resources RoleBinding Namespace Binds a Role or ClusterRole to users within a namespace ClusterRoleBinding Cluster-wide Binds a ClusterRole to users across the entire cluster"},{"location":"openshift/configuration/rbac/#how-rbac-works","title":"How RBAC Works","text":"<ol> <li>Roles/ClusterRoles define what actions can be performed on which resources</li> <li>RoleBindings/ClusterRoleBindings define who can perform those actions</li> <li>Permissions are purely additive (there are no \"deny\" rules)</li> </ol>"},{"location":"openshift/configuration/rbac/#resources","title":"Resources","text":"OpenShiftKubernetes <p>RBAC Overview </p> <p>Default Cluster Roles </p> <p>RBAC Authorization </p> <p>Using RBAC </p>"},{"location":"openshift/configuration/rbac/#references","title":"References","text":"<p>Role - Grants read access to pods in a namespace</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>RoleBinding - Binds the Role to a user</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n  - kind: User\n    name: jane\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>ClusterRole - Grants read access to secrets cluster-wide</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>ClusterRoleBinding - Binds ClusterRole to a group</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-secrets-global\nsubjects:\n  - kind: Group\n    name: developers\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Role for a ServiceAccount with deployment permissions</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: production\n  name: deployment-manager\nrules:\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: deployment-manager-binding\n  namespace: production\nsubjects:\n  - kind: ServiceAccount\n    name: deploy-bot\n    namespace: production\nroleRef:\n  kind: Role\n  name: deployment-manager\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"openshift/configuration/rbac/#common-verbs","title":"Common Verbs","text":"Verb Description <code>get</code> Read a specific resource <code>list</code> List resources of a type <code>watch</code> Watch for changes to resources <code>create</code> Create new resources <code>update</code> Update existing resources <code>patch</code> Partially update resources <code>delete</code> Delete resources <code>deletecollection</code> Delete multiple resources"},{"location":"openshift/configuration/rbac/#default-clusterroles","title":"Default ClusterRoles","text":"Role Description <code>cluster-admin</code> Full access to all resources <code>admin</code> Full access within a namespace <code>edit</code> Read/write access to most resources in a namespace <code>view</code> Read-only access to most resources in a namespace OpenShiftKubernetes Get Roles in Namespace<pre><code>oc get roles\n</code></pre> Get ClusterRoles<pre><code>oc get clusterroles\n</code></pre> Get RoleBindings<pre><code>oc get rolebindings\n</code></pre> Describe a Role<pre><code>oc describe role pod-reader\n</code></pre> Check if User Can Perform Action<pre><code>oc auth can-i create pods --as=jane\n</code></pre> Add Role to User (OpenShift)<pre><code>oc adm policy add-role-to-user edit jane -n myproject\n</code></pre> Add Cluster Role to User<pre><code>oc adm policy add-cluster-role-to-user cluster-admin admin-user\n</code></pre> Get Roles in Namespace<pre><code>kubectl get roles\n</code></pre> Get ClusterRoles<pre><code>kubectl get clusterroles\n</code></pre> Get RoleBindings<pre><code>kubectl get rolebindings\n</code></pre> Describe a Role<pre><code>kubectl describe role pod-reader\n</code></pre> Check if User Can Perform Action<pre><code>kubectl auth can-i create pods --as=jane\n</code></pre> Check All Permissions for User<pre><code>kubectl auth can-i --list --as=jane\n</code></pre> Create Role Imperatively<pre><code>kubectl create role pod-reader --verb=get,list,watch --resource=pods\n</code></pre> Create RoleBinding Imperatively<pre><code>kubectl create rolebinding read-pods --role=pod-reader --user=jane\n</code></pre>"},{"location":"openshift/configuration/config-map/","title":"Config Maps","text":"<p>ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable.</p> <p>You can data from a ConfigMap in 3 different ways.</p> <ul> <li>As a single environment variable specific to a single key</li> <li>As a set of environment variables from all keys</li> <li>As a set of files, each key represented by a file on mounted volume</li> </ul>"},{"location":"openshift/configuration/config-map/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Mapping Volumes </p> <p>ConfigMaps </p>"},{"location":"openshift/configuration/config-map/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-cm\ndata:\n  color: blue\n  location: naboo\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"echo\"]\n      args: [\"color is $(MY_VAR)\"]\n      env:\n        - name: MY_VAR\n          valueFrom:\n            configMapKeyRef:\n              name: my-cm\n              key: color\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"ls -l /etc/config; echo located at $(cat /etc/config/location)\",\n        ]\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: my-cm\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"env | sort\"]\n      envFrom:\n        - configMapRef:\n            name: my-cm\n  restartPolicy: Never\n</code></pre>"},{"location":"openshift/configuration/limit-ranges/","title":"Limit Ranges","text":"<p>In an OpenShift Container Platform cluster, containers run with unlimited compute resources. By using limit ranges, you can restrict the amount of resources consumed for the following objects within a project.</p> <ul> <li> <p>Pods/Containers: You can set minimum and maximum requirements for CPU and memory</p> </li> <li> <p>Image Streams: You can set limits on the number of images and tags in an ImageStream object</p> </li> <li> <p>Images: You can limit the size of images that can be pushed to a registry</p> </li> <li> <p>Persistent Volume Claims (PVC): You can restrict the size of the PVCs that can be requested</p> </li> </ul> <p>A LimitRange object allows you to restrict the amount of resources that can be consumed in a project. Any request that is made to create or modify a resource will be validated against any LimitRange objects in the project. If any of the constraints listed in the LimitRange object are violated, then the resource request is rejected.</p>"},{"location":"openshift/configuration/limit-ranges/#creating-a-limit-range","title":"Creating a Limit Range","text":"<p>To create a LimitRange object you can follow the example below:</p> <pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\"\nspec:\n  limits:\n    - type: \"Pod\"\n      max:\n        cpu: \"2\"\n        memory: \"1Gi\"\n      min:\n        cpu: \"200m\"\n        memory: \"6Mi\"\n    - type: \"Container\"\n      max:\n        cpu: \"2\"\n        memory: \"1Gi\"\n      min:\n        cpu: \"100m\"\n        memory: \"4Mi\"\n      default:\n        cpu: \"300m\"\n        memory: \"200Mi\"\n      defaultRequest:\n        cpu: \"200m\"\n        memory: \"100Mi\"\n      maxLimitRequestRatio:\n        cpu: \"10\"\n    - type: openshift.io/Image\n      max:\n        storage: 1Gi\n    - type: openshift.io/ImageStream\n      max:\n        openshift.io/image-tags: 20\n        openshift.io/images: 30\n    - type: \"PersistentVolumeClaim\"\n      min:\n        storage: \"2Gi\"\n      max:\n        storage: \"50Gi\"\n</code></pre>"},{"location":"openshift/configuration/limit-ranges/#container-limits","title":"Container Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"Container\"\n      max:\n        cpu: \"2\" [2]\n        memory: \"1Gi\" [3]\n      min:\n        cpu: \"100m\" [4]\n        memory: \"4Mi\" [5]\n      default:\n        cpu: \"300m\" [6]\n        memory: \"200Mi\" [7]\n      defaultRequest:\n        cpu: \"200m\" [8]\n        memory: \"100Mi\" [9]\n      maxLimitRequestRatio:\n        cpu: \"10\" [10]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum amount of CPU that a single container in a pod can request</p> <p>[3] The maximum amount of memory that a single container in a pod can request</p> <p>[4] The minimum amount of CPU that a single container in a pod can request</p> <p>[5] The minimum amount of memory that a single container in a pod can request</p> <p>[6] The default amount of CPU that a container can use if not specified in the Pod spec</p> <p>[7] The default amount of memory that a container can use if not specified in teh Pod spec</p> <p>[8] The default amount of CPU that a contianer can request if not specified in the Pod spec</p> <p>[9] The default amount of memory that a container can request if not specified in the Pod spec</p> <p>[10] The maximum limit-to-request ratio for a container</p>"},{"location":"openshift/configuration/limit-ranges/#pod-limits","title":"Pod Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"Pod\"\n      max:\n        cpu: \"2\" [2]\n        memory: \"1Gi\" [3]\n      min:\n        cpu: \"200m\" [4]\n        memory: \"6Mi\" [5]\n      maxLimitRequestRatio:\n        cpu: \"10\" [6]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum amount of CPU that a pod can request across all containers</p> <p>[3] The maximum amount of memory that a pod can request across all containers</p> <p>[4] The minimum amount of CPU that a pod can request across all containers</p> <p>[5] The minimum amount of memory that a pod can request across all containers</p> <p>[6] The maximum limit-to-request ration for a container</p>"},{"location":"openshift/configuration/limit-ranges/#image-limits","title":"Image Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: openshift.io/Image\n      max:\n        storage: 1Gi [2]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum size of an image that can be pushed to a registry</p>"},{"location":"openshift/configuration/limit-ranges/#image-stream-limits","title":"Image Stream Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: openshift.io/ImageStream\n      max:\n        openshift.io/image-tags: 20 [2]\n        openshift.io/images: 30 [3]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The maximum number of unique image tags in the imagestream.spec.tags parameter in imagestream spec</p> <p>[3] The maximum number of unique image regerenes in the imagestream.status.tags parameter in the imagestream spec</p>"},{"location":"openshift/configuration/limit-ranges/#persistent-volume-claim-limits","title":"Persistent Volume Claim Limits","text":"<pre><code>apiVersion: \"v1\"\nkind: \"LimitRange\"\nmetadata:\n  name: \"resource-limits\" [1]\nspec:\n  limits:\n    - type: \"PersistentVolumeClaim\"\n      min:\n        storage: \"2Gi\" [2]\n      max:\n        storage: \"50Gi\" [3]\n</code></pre> <p>[1] The name of the LimitRange object</p> <p>[2] The minimum amount of storage that can be requested in a persistent volume claim</p> <p>[3] The maximum amount of storage that can be requested in a persistent volume claim</p>"},{"location":"openshift/configuration/secrets/","title":"Secrets","text":"<p>Kubernetes secret objects let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.</p> <p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.</p>"},{"location":"openshift/configuration/secrets/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Image Pull Secrets</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up   and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> </ul> <p>Image Pull Secrets </p> <p>Secret Commands </p> <p>Secrets </p> <p>Secret Distribution </p>"},{"location":"openshift/configuration/secrets/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\nstringData:\n  admin: administrator\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret-config\ntype: Opaque\nstringData:\n  config.yaml: |-\n    apiUrl: \"https://my.api.com/api/v1\"\n    username: token\n    password: thesecrettoken\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      env:\n        - name: SECRET_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: username\n      envFrom:\n        - secretRef:\n            name: mysecret\n      volumeMounts:\n        - name: config\n          mountPath: \"/etc/secrets\"\n  volumes:\n    - name: config\n      secret:\n        secretName: mysecret-config\n</code></pre> OpenShiftKubernetes <p>Create files needed for rest of example</p> <pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> <p>Creating Secret from files</p> <pre><code>oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> <p>Getting Secret</p> <pre><code>oc get secrets\n</code></pre> <p>Gets the Secret's Description</p> <pre><code>oc describe secrets/db-user-pass\n</code></pre> <p>Create files needed for rest of example <pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> Creates the Secret from the files <pre><code>kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> Gets the Secret <pre><code>kubectl get secrets\n</code></pre> Gets the Secret's Description <pre><code>kubectl describe secrets/db-user-pass\n</code></pre></p>"},{"location":"openshift/configuration/security-contexts/","title":"Security Contexts","text":"<p>A security context defines privilege and access control settings for a Pod or Container.</p> <p>To specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod.</p>"},{"location":"openshift/configuration/security-contexts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Managing Security Contexts </p> <p>Security Contexts </p>"},{"location":"openshift/configuration/security-contexts/#references","title":"References","text":"<p>Setup minikube VM with users</p> <pre><code>minikube ssh\n</code></pre> <pre><code>su -\n</code></pre> <pre><code>echo \"container-user-0:x:2000:2000:-:/home/container-user-0:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-user-1:x:2001:2001:-:/home/container-user-1:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-group-0:x:3000:\" &gt;&gt;/etc/group\necho \"container-group-1:x:3001:\" &gt;&gt;/etc/group\nmkdir -p /etc/message/\necho \"Hello, World!\" | sudo tee -a /etc/message/message.txt\nchown 2000:3000 /etc/message/message.txt\nchmod 640 /etc/message/message.txt\n</code></pre> <p>Using the this <code>securityContext</code> the container will be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2000\n    runAsGroup: 3000\n    fsGroup: 3000\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Using the this <code>securityContext</code> the container should NOT be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2001\n    runAsGroup: 3001\n    fsGroup: 3001\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Run to see the errors</p> OpenShiftKubernetes Get Pod Logs<pre><code>oc logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.text': Permission denied\n</code></pre> Get Pod Logs<pre><code>kubectl logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.txt': Permission denied\n</code></pre>"},{"location":"openshift/configuration/service-accounts/","title":"Service Accounts","text":"<p>A service account provides an identity for processes that run in a Pod.</p> <p>When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default).</p> <p>User accounts are for humans. Service accounts are for processes, which run in pods.</p> <p>User accounts are intended to be global. Names must be unique across all namespaces of a cluster, future user resource will not be namespaced. Service accounts are namespaced.</p>"},{"location":"openshift/configuration/service-accounts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Service Accounts </p> <p>Using Service Accounts </p> <p>Service Accounts </p> <p>Service Account Configuration </p>"},{"location":"openshift/configuration/service-accounts/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: my-service-account\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: build-robot-secret\n  annotations:\n    kubernetes.io/service-account.name: my-service-account\ntype: kubernetes.io/service-account-token\n</code></pre> OpenshiftKubernetes Create a Service Account<pre><code>oc create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>oc describe sa &lt;service_account_name&gt;\n</code></pre> Create a Service Account<pre><code>kubectl create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>kubectl describe sa &lt;service_account_name&gt;\n</code></pre>"},{"location":"openshift/core-concepts/","title":"Kubernetes API Primitives","text":"<p>Kubernetes API primitive, also known as Kubernetes objects, are the basic building blocks of any application running in Kubernetes</p> <p>Examples:</p> <ul> <li>Pod</li> <li>Node</li> <li>Service</li> <li>ServiceAccount</li> </ul> <p>Two primary members</p> <ul> <li>Spec, desired state</li> <li>Status, current state</li> </ul>"},{"location":"openshift/core-concepts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Pods </p> <p>Nodes </p> <p>Objects </p> <p>Kube Basics </p>"},{"location":"openshift/core-concepts/#references","title":"References","text":"OpenShiftKubernetes List API-Resources<pre><code>oc api-resources\n</code></pre> List API-Resources<pre><code>kubectl api-resources\n</code></pre>"},{"location":"openshift/core-concepts/namespaces-projects/","title":"Projects/Namespaces","text":"<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects.</p> <p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p> <p>Namespaces are a way to divide cluster resources between multiple users (via resource quota).</p> <p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace. In practice namespaces are used to deploy different versions based on stages of the CICD pipeline (dev, test, stage, prod)</p>"},{"location":"openshift/core-concepts/namespaces-projects/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Working with Projects </p> <p>Creating Projects </p> <p>Configure Project Creation </p> <p>Namespaces </p>"},{"location":"openshift/core-concepts/namespaces-projects/#references","title":"References","text":"Namespace YAML<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre> Pod YAML specifiying Namespace<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  namespace: dev\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes Getting all namespaces/projects<pre><code>oc projects\n</code></pre> Create a new Project<pre><code>oc new-project dev\n</code></pre> Viewing Current Project<pre><code>oc project\n</code></pre> Setting Namespace in Context<pre><code>oc project dev\n</code></pre> Viewing Project Status<pre><code>oc status\n</code></pre> Getting all namespaces<pre><code>kubectl get namespaces\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"openshift/deployments/","title":"Deployments","text":"<p>A Deployment provides declarative updates for Pods and ReplicaSets.</p> <p>You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p> <p>The following are typical use cases for Deployments:</p> <ul> <li>Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li> <li>Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li> <li>Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li> <li>Scale up the Deployment to facilitate more load.</li> <li>Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li> <li>Use the status of the Deployment as an indicator that a rollout has stuck.</li> <li>Clean up older ReplicaSets that you don\u2019t need anymore.</li> </ul>"},{"location":"openshift/deployments/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Deployments </p> <p>Managing Deployment Processes </p> <p>DeploymentConfig Strategies </p> <p>Route Based Deployment Strategies </p> <p>Deployments </p> <p>Scaling Deployments </p>"},{"location":"openshift/deployments/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenshiftKubernetes Create a Deployment<pre><code>oc apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>oc get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>oc describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>oc edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>oc scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>kubectl get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>kubectl describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>kubectl edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>kubectl scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>kubectl delete deployment my-deployment\n</code></pre>"},{"location":"openshift/deployments/#activities","title":"Activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application Rolling Updates"},{"location":"openshift/deployments/daemonsets/","title":"DaemonSets","text":"<p>A DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected.</p> <p>DaemonSets are typically used for cluster-wide services that need to run on every node.</p>"},{"location":"openshift/deployments/daemonsets/#common-use-cases","title":"Common Use Cases","text":"Use Case Examples Logging Collectors Fluentd, Filebeat, Logstash Monitoring Agents Prometheus Node Exporter, Datadog Agent Storage Daemons Ceph, GlusterFS Network Plugins Calico, Cilium, Weave Net Security Agents Falco, Twistlock"},{"location":"openshift/deployments/daemonsets/#how-daemonsets-work","title":"How DaemonSets Work","text":"<ul> <li>When you create a DaemonSet, it automatically creates a pod on each node</li> <li>When a new node joins the cluster, a pod is scheduled on it</li> <li>When a node is removed, the pod is garbage collected</li> <li>Deleting a DaemonSet cleans up all the pods it created</li> </ul>"},{"location":"openshift/deployments/daemonsets/#resources","title":"Resources","text":"OpenShiftKubernetes <p>DaemonSets </p> <p>DaemonSets </p> <p>Perform Rolling Update on DaemonSet </p>"},{"location":"openshift/deployments/daemonsets/#references","title":"References","text":"<p>Basic DaemonSet for logging agent</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-system\n  labels:\n    app: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      containers:\n        - name: fluentd\n          image: fluentd:v1.16\n          resources:\n            limits:\n              memory: 200Mi\n            requests:\n              cpu: 100m\n              memory: 200Mi\n          volumeMounts:\n            - name: varlog\n              mountPath: /var/log\n            - name: containers\n              mountPath: /var/lib/docker/containers\n              readOnly: true\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: varlog\n          hostPath:\n            path: /var/log\n        - name: containers\n          hostPath:\n            path: /var/lib/docker/containers\n</code></pre> <p>DaemonSet running only on specific nodes using nodeSelector</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: gpu-monitor\nspec:\n  selector:\n    matchLabels:\n      app: gpu-monitor\n  template:\n    metadata:\n      labels:\n        app: gpu-monitor\n    spec:\n      nodeSelector:\n        gpu: \"true\"\n      containers:\n        - name: gpu-monitor\n          image: nvidia/dcgm-exporter:latest\n</code></pre> <p>DaemonSet with tolerations to run on control plane nodes</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      tolerations:\n        - key: node-role.kubernetes.io/control-plane\n          operator: Exists\n          effect: NoSchedule\n      containers:\n        - name: node-exporter\n          image: prom/node-exporter:latest\n          ports:\n            - containerPort: 9100\n              hostPort: 9100\n</code></pre> OpenShiftKubernetes Create DaemonSet<pre><code>oc apply -f daemonset.yaml\n</code></pre> Get DaemonSets<pre><code>oc get daemonsets -A\n</code></pre> Describe DaemonSet<pre><code>oc describe daemonset fluentd -n kube-system\n</code></pre> Check DaemonSet Pods<pre><code>oc get pods -l app=fluentd -o wide\n</code></pre> Delete DaemonSet<pre><code>oc delete daemonset fluentd\n</code></pre> Create DaemonSet<pre><code>kubectl apply -f daemonset.yaml\n</code></pre> Get DaemonSets<pre><code>kubectl get daemonsets -A\n</code></pre> Describe DaemonSet<pre><code>kubectl describe daemonset fluentd -n kube-system\n</code></pre> Check DaemonSet Pods<pre><code>kubectl get pods -l app=fluentd -o wide\n</code></pre> Delete DaemonSet<pre><code>kubectl delete daemonset fluentd\n</code></pre>"},{"location":"openshift/deployments/daemonsets/#daemonset-vs-deployment","title":"DaemonSet vs Deployment","text":"Feature Deployment DaemonSet Replicas User-defined count One per node (automatic) Scaling Manual or HPA Automatic with cluster Node Coverage Scheduled by kube-scheduler Guaranteed per node Use Case Application workloads Node-level services"},{"location":"openshift/deployments/hpa/","title":"Horizontal Pod Autoscaler (HPA)","text":"<p>The Horizontal Pod Autoscaler automatically scales the number of pods in a deployment, replica set, or stateful set based on observed CPU utilization, memory usage, or custom metrics. This allows your applications to handle varying loads efficiently.</p>"},{"location":"openshift/deployments/hpa/#how-hpa-works","title":"How HPA Works","text":"<ol> <li>HPA continuously monitors the metrics of pods in the target workload</li> <li>It calculates the desired number of replicas based on the metrics and target values</li> <li>The controller adjusts the replica count to match demand</li> <li>Scale-up and scale-down happen automatically within configured bounds</li> </ol>"},{"location":"openshift/deployments/hpa/#metrics-types","title":"Metrics Types","text":"Metric Type Description Example Resource CPU or memory utilization <code>cpu</code>, <code>memory</code> Pods Custom metrics from pods <code>requests_per_second</code> Object Metrics from other Kubernetes objects <code>queue_length</code> from a Service External Metrics from outside the cluster Cloud provider metrics"},{"location":"openshift/deployments/hpa/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Horizontal Pod Autoscaler </p> <p>Custom Metrics Autoscaler </p> <p>Horizontal Pod Autoscaling </p> <p>HPA Walkthrough </p>"},{"location":"openshift/deployments/hpa/#prerequisites","title":"Prerequisites","text":"<p>For HPA to work with CPU and memory metrics, you need:</p> <ul> <li>Metrics Server installed in your cluster (provides resource metrics)</li> <li>Resource requests defined on your containers (HPA uses these as the baseline)</li> </ul>"},{"location":"openshift/deployments/hpa/#references","title":"References","text":"<p>Basic HPA based on CPU</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: web-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: web-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n</code></pre> <p>HPA with CPU and Memory</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 75\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n</code></pre> <p>HPA with Scaling Behavior</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: controlled-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down\n      policies:\n        - type: Percent\n          value: 10\n          periodSeconds: 60  # Scale down max 10% per minute\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 15  # Can double every 15 seconds\n        - type: Pods\n          value: 4\n          periodSeconds: 15  # Or add 4 pods every 15 seconds\n      selectPolicy: Max\n</code></pre> <p>Deployment with Resource Requests (required for HPA)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web-app\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n        - name: web\n          image: nginx\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 500m\n              memory: 512Mi\n          ports:\n            - containerPort: 80\n</code></pre> OpenShiftKubernetes Create HPA Imperatively<pre><code>oc autoscale deployment web-app --min=2 --max=10 --cpu-percent=70\n</code></pre> Get HPAs<pre><code>oc get hpa\n</code></pre> Describe HPA<pre><code>oc describe hpa web-app-hpa\n</code></pre> Watch HPA Status<pre><code>oc get hpa -w\n</code></pre> Delete HPA<pre><code>oc delete hpa web-app-hpa\n</code></pre> Check Metrics Server<pre><code>oc top pods\n</code></pre> Create HPA Imperatively<pre><code>kubectl autoscale deployment web-app --min=2 --max=10 --cpu-percent=70\n</code></pre> Get HPAs<pre><code>kubectl get hpa\n</code></pre> Describe HPA<pre><code>kubectl describe hpa web-app-hpa\n</code></pre> Watch HPA Status<pre><code>kubectl get hpa -w\n</code></pre> Delete HPA<pre><code>kubectl delete hpa web-app-hpa\n</code></pre> Check Metrics Server<pre><code>kubectl top pods\n</code></pre>"},{"location":"openshift/deployments/hpa/#understanding-hpa-output","title":"Understanding HPA Output","text":"<p>When you run <code>kubectl get hpa</code>, you'll see output like:</p> <pre><code>NAME          REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nweb-app-hpa   Deployment/web-app    45%/70%   2         10        3          5m\n</code></pre> Column Description TARGETS Current/Target utilization (45% current, 70% target) MINPODS Minimum replicas MAXPODS Maximum replicas REPLICAS Current number of pods"},{"location":"openshift/deployments/hpa/#best-practices","title":"Best Practices","text":"<ol> <li>Set Resource Requests - Always define CPU/memory requests on containers</li> <li>Start Conservative - Begin with higher target utilization and adjust</li> <li>Use Stabilization Windows - Prevent thrashing with scale-down delays</li> <li>Monitor Behavior - Watch HPA decisions and adjust thresholds</li> <li>Consider Pod Disruption Budgets - Ensure availability during scale-down</li> <li>Test Under Load - Validate HPA behavior before production deployment</li> </ol>"},{"location":"openshift/deployments/hpa/#troubleshooting","title":"Troubleshooting","text":"Issue Possible Cause Solution <code>&lt;unknown&gt;/70%</code> in TARGETS Metrics server not running Install metrics server Not scaling up Resource requests not set Add requests to containers Scaling too aggressively Default behavior too fast Add scaling policies Not scaling down Stabilization window Wait for window to pass"},{"location":"openshift/deployments/statefulsets/","title":"StatefulSets","text":"<p>StatefulSets are workload API objects used to manage stateful applications. Unlike Deployments, StatefulSets maintain a sticky identity for each pod and provide guarantees about the ordering and uniqueness of these pods.</p> <p>StatefulSets are valuable for applications that require one or more of the following:</p> <ul> <li>Stable, unique network identifiers</li> <li>Stable, persistent storage</li> <li>Ordered, graceful deployment and scaling</li> <li>Ordered, automated rolling updates</li> </ul>"},{"location":"openshift/deployments/statefulsets/#how-statefulsets-work","title":"How StatefulSets Work","text":"<p>StatefulSets create pods with predictable names following the pattern <code>{statefulset-name}-{ordinal}</code>. For example, a StatefulSet named <code>mysql</code> with 3 replicas creates pods: <code>mysql-0</code>, <code>mysql-1</code>, <code>mysql-2</code>.</p> <p>Key Characteristics:</p> <ul> <li>Pods are created sequentially (0, 1, 2...) and terminated in reverse order</li> <li>Each pod gets a stable DNS hostname: <code>{pod-name}.{service-name}.{namespace}.svc.cluster.local</code></li> <li>PersistentVolumeClaims are retained when pods are deleted (data persists)</li> <li>Scaling down does not delete associated PVCs</li> </ul>"},{"location":"openshift/deployments/statefulsets/#when-to-use-statefulsets","title":"When to Use StatefulSets","text":"Use Case Examples Databases MySQL, PostgreSQL, MongoDB Message Queues Kafka, RabbitMQ Distributed Systems Elasticsearch, Cassandra, Zookeeper Applications requiring stable network identity Leader election scenarios"},{"location":"openshift/deployments/statefulsets/#resources","title":"Resources","text":"OpenShiftKubernetes <p>StatefulSets </p> <p>StatefulSets </p> <p>StatefulSet Basics </p>"},{"location":"openshift/deployments/statefulsets/#references","title":"References","text":"<p>Headless Service for StatefulSet (required for stable network identities)</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  ports:\n    - port: 3306\n      name: mysql\n  clusterIP: None  # Headless service\n  selector:\n    app: mysql\n</code></pre> <p>StatefulSet Definition</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: \"mysql\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n        - name: mysql\n          image: mysql:8.0\n          ports:\n            - containerPort: 3306\n              name: mysql\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: mysql-secret\n                  key: password\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/mysql\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        storageClassName: \"standard\"\n        resources:\n          requests:\n            storage: 10Gi\n</code></pre> OpenShiftKubernetes Create StatefulSet<pre><code>oc apply -f statefulset.yaml\n</code></pre> Get StatefulSets<pre><code>oc get statefulsets\n</code></pre> Describe StatefulSet<pre><code>oc describe statefulset mysql\n</code></pre> Scale StatefulSet<pre><code>oc scale statefulset mysql --replicas=5\n</code></pre> Delete StatefulSet (keeps PVCs)<pre><code>oc delete statefulset mysql\n</code></pre> Delete StatefulSet and PVCs<pre><code>oc delete statefulset mysql --cascade=foreground\noc delete pvc -l app=mysql\n</code></pre> Create StatefulSet<pre><code>kubectl apply -f statefulset.yaml\n</code></pre> Get StatefulSets<pre><code>kubectl get statefulsets\n</code></pre> Describe StatefulSet<pre><code>kubectl describe statefulset mysql\n</code></pre> Scale StatefulSet<pre><code>kubectl scale statefulset mysql --replicas=5\n</code></pre> Delete StatefulSet (keeps PVCs)<pre><code>kubectl delete statefulset mysql\n</code></pre> Delete StatefulSet and PVCs<pre><code>kubectl delete statefulset mysql --cascade=foreground\nkubectl delete pvc -l app=mysql\n</code></pre>"},{"location":"openshift/deployments/statefulsets/#statefulset-vs-deployment","title":"StatefulSet vs Deployment","text":"Feature Deployment StatefulSet Pod Names Random (e.g., <code>nginx-5d4c6f7b8-x2k9j</code>) Predictable (e.g., <code>mysql-0</code>) Pod Creation Parallel Sequential Pod Deletion Any order Reverse order Network Identity Ephemeral Stable DNS Storage Shared PVC Per-pod PVC Use Case Stateless apps Stateful apps"},{"location":"openshift/deployments/updates/","title":"Rolling Updates and Rollbacks","text":"<p>Updating a Deployment A Deployment\u2019s rollout is triggered if and only if the Deployment\u2019s Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</p> <p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0.</p> <p>Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.</p> <p>Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment\u2019s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).</p> <p>A Deployment\u2019s revision is created when a Deployment\u2019s rollout is triggered. This means that the new revision is created if and only if the Deployment\u2019s Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment\u2019s Pod template part is rolled back.</p>"},{"location":"openshift/deployments/updates/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Rollouts </p> <p>Rolling Back </p> <p>Updating a Deployment </p> <p>Rolling Back a Deployment </p>"},{"location":"openshift/deployments/updates/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenShiftKubernetes Get Deployments<pre><code>oc get deployments\n</code></pre> Sets new image for Deployment<pre><code>oc set image deployment/my-deployment nginx=bitnami/nginx:1.16.1 --record\n</code></pre> Check the status of a rollout<pre><code>oc rollout status deployment my-deployment\n</code></pre> Get Replicasets<pre><code>oc get rs\n</code></pre> Get Deployment Description<pre><code>oc describe deployment my-deployment\n</code></pre> Get Rollout History<pre><code>oc rollout history deployment my-deployment\n</code></pre> Undo Rollout<pre><code>oc rollback my-deployment\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"openshift/deployments/updates/#activities","title":"Activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application Rolling Updates"},{"location":"openshift/operators/","title":"What are OpenShift Operators","text":""},{"location":"openshift/operators/#overview","title":"Overview","text":"<p>Operators and Red Hat OpenShift Container Platform Red Hat\u00ae OpenShift\u00ae Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service.</p> <p>Red Hat OpenShift uses the power of Operators to run the entire platform in an autonomous fashion while exposing configuration natively through Kubernetes objects, allowing for quick installation and frequent, robust updates. In addition to the automation advantages of Operators for managing the platform, Red Hat OpenShift makes it easier to find, install, and manage Operators running on your clusters.</p> <p>Included in Red Hat OpenShift is the Embedded OperatorHub, a registry of certified Operators from software vendors and open source projects. Within the Embedded OperatorHub you can browse and install a library of Operators that have been verified to work with Red Hat OpenShift and that have been packaged for easy lifecycle management.</p>"},{"location":"openshift/operators/operatorCatalog/","title":"What is the Operator Catalog","text":"<p>The Operator Catalog, also known as OperatorHub, is a registry of Kubernetes Operators that have been packaged for easy discovery, installation, and lifecycle management. In Red Hat OpenShift, the Embedded OperatorHub provides a curated marketplace of Operators that have been verified to work with the platform.</p>"},{"location":"openshift/operators/operatorCatalog/#understanding-the-operator-catalog","title":"Understanding the Operator Catalog","text":"<p>The Operator Catalog serves as a central repository where cluster administrators and developers can browse, discover, and install Operators. It provides a standardized way to distribute and manage Operators across Kubernetes and OpenShift clusters.</p>"},{"location":"openshift/operators/operatorCatalog/#key-features","title":"Key Features","text":"<ul> <li>Centralized Discovery - Browse available Operators from a single interface</li> <li>Verified Operators - Access Operators that have been tested and certified</li> <li>Easy Installation - Install Operators with just a few clicks</li> <li>Automatic Updates - Subscribe to Operators for automatic version updates</li> <li>Dependency Management - Automatically handle Operator dependencies</li> </ul>"},{"location":"openshift/operators/operatorCatalog/#types-of-operator-sources","title":"Types of Operator Sources","text":"<p>The Operator Catalog in OpenShift aggregates Operators from multiple sources:</p>"},{"location":"openshift/operators/operatorCatalog/#red-hat-operators","title":"Red Hat Operators","text":"<p>Operators packaged and shipped by Red Hat. These are fully supported as part of an OpenShift subscription and include:</p> <ul> <li>Red Hat AMQ Streams</li> <li>Red Hat OpenShift Serverless</li> <li>Red Hat OpenShift Service Mesh</li> <li>Red Hat OpenShift Pipelines (Tekton)</li> <li>Red Hat OpenShift GitOps (ArgoCD)</li> </ul>"},{"location":"openshift/operators/operatorCatalog/#certified-operators","title":"Certified Operators","text":"<p>Operators from independent software vendors (ISVs) that have been certified by Red Hat. These Operators have passed certification requirements and are supported by the vendor:</p> <ul> <li>MongoDB Enterprise</li> <li>Crunchy PostgreSQL</li> <li>Elasticsearch (ECK)</li> <li>Datadog</li> <li>Splunk</li> </ul>"},{"location":"openshift/operators/operatorCatalog/#community-operators","title":"Community Operators","text":"<p>Operators from the open-source community. These are not officially supported but provide access to a wide range of software:</p> <ul> <li>Prometheus Operator</li> <li>Grafana Operator</li> <li>Jaeger Operator</li> <li>Strimzi (Kafka)</li> <li>ArgoCD Community</li> </ul>"},{"location":"openshift/operators/operatorCatalog/#custom-catalogs","title":"Custom Catalogs","text":"<p>Organizations can also create their own custom Operator catalogs to distribute internal Operators or curated collections of Operators.</p>"},{"location":"openshift/operators/operatorCatalog/#accessing-the-operator-catalog","title":"Accessing the Operator Catalog","text":""},{"location":"openshift/operators/operatorCatalog/#through-the-openshift-web-console","title":"Through the OpenShift Web Console","text":"<ol> <li>Navigate to Operators \u2192 OperatorHub in the left menu</li> <li>Browse or search for Operators</li> <li>Filter by category, source, or capability level</li> <li>Click on an Operator to view details and installation options</li> </ol>"},{"location":"openshift/operators/operatorCatalog/#through-the-cli","title":"Through the CLI","text":"<p>You can also interact with the Operator Catalog using the <code>oc</code> command:</p> <pre><code># List available PackageManifests (Operators)\noc get packagemanifests -n openshift-marketplace\n\n# Get details about a specific Operator\noc describe packagemanifest &lt;operator-name&gt; -n openshift-marketplace\n\n# List CatalogSources\noc get catalogsources -n openshift-marketplace\n</code></pre>"},{"location":"openshift/operators/operatorCatalog/#operator-capability-levels","title":"Operator Capability Levels","text":"<p>Operators in the catalog are classified by their capability level, indicating their maturity:</p> Level Name Description 1 Basic Install Automated installation and configuration 2 Seamless Upgrades Automated upgrades between versions 3 Full Lifecycle Backup, restore, and failure recovery 4 Deep Insights Metrics, alerts, and log processing 5 Auto Pilot Horizontal/vertical scaling, auto-tuning"},{"location":"openshift/operators/operatorCatalog/#catalogsource-custom-resource","title":"CatalogSource Custom Resource","text":"<p>The Operator Catalog is defined using <code>CatalogSource</code> custom resources. Each CatalogSource points to an index of Operators:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: CatalogSource\nmetadata:\n  name: my-operator-catalog\n  namespace: openshift-marketplace\nspec:\n  sourceType: grpc\n  image: quay.io/my-org/my-operator-index:latest\n  displayName: My Operator Catalog\n  publisher: My Organization\n  updateStrategy:\n    registryPoll:\n      interval: 30m\n</code></pre>"},{"location":"openshift/operators/operatorCatalog/#managing-catalog-sources","title":"Managing Catalog Sources","text":""},{"location":"openshift/operators/operatorCatalog/#viewing-available-catalogs","title":"Viewing Available Catalogs","text":"<pre><code>oc get catalogsources -n openshift-marketplace\n</code></pre> <p>Default catalogs in OpenShift include:</p> <ul> <li><code>certified-operators</code> - Red Hat certified Operators</li> <li><code>community-operators</code> - Community Operators</li> <li><code>redhat-marketplace</code> - Red Hat Marketplace Operators</li> <li><code>redhat-operators</code> - Red Hat Operators</li> </ul>"},{"location":"openshift/operators/operatorCatalog/#disabling-default-catalogs","title":"Disabling Default Catalogs","text":"<p>In restricted environments, you may need to disable default catalogs:</p> <pre><code>oc patch OperatorHub cluster --type json \\\n  -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]'\n</code></pre>"},{"location":"openshift/operators/operatorCatalog/#best-practices","title":"Best Practices","text":"<ol> <li>Use certified Operators when possible for production workloads</li> <li>Review Operator permissions before installation</li> <li>Test Operators in non-production environments first</li> <li>Monitor Operator health after deployment</li> <li>Keep Operators updated by subscribing to automatic updates</li> </ol>"},{"location":"openshift/operators/operatorUsage/","title":"How to use Operators","text":"<p>Operators extend Kubernetes functionality by automating the deployment and management of complex applications. This guide walks you through the process of finding, installing, and using Operators in your OpenShift cluster.</p>"},{"location":"openshift/operators/operatorUsage/#installing-an-operator","title":"Installing an Operator","text":"<p>There are two primary ways to install Operators in OpenShift: through the web console and through the CLI.</p>"},{"location":"openshift/operators/operatorUsage/#installing-via-the-web-console","title":"Installing via the Web Console","text":"<ol> <li>Navigate to OperatorHub</li> <li>Log in to the OpenShift web console</li> <li> <p>Go to Operators \u2192 OperatorHub</p> </li> <li> <p>Find the Operator</p> </li> <li>Use the search bar to find your desired Operator</li> <li> <p>Filter by category, source, or capability level</p> </li> <li> <p>Review Operator Details</p> </li> <li>Click on the Operator to view its description</li> <li>Review the provided APIs (Custom Resources)</li> <li> <p>Check the documentation and prerequisites</p> </li> <li> <p>Install the Operator</p> </li> <li>Click Install</li> <li>Choose the installation mode:<ul> <li>All namespaces - Operator watches all namespaces</li> <li>Specific namespace - Operator watches only the selected namespace</li> </ul> </li> <li>Select the update channel and approval strategy</li> <li> <p>Click Install</p> </li> <li> <p>Verify Installation</p> </li> <li>Navigate to Operators \u2192 Installed Operators</li> <li>Confirm the Operator status shows \"Succeeded\"</li> </ol>"},{"location":"openshift/operators/operatorUsage/#installing-via-the-cli","title":"Installing via the CLI","text":"<p>Create a <code>Subscription</code> resource to install an Operator:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: my-operator\n  namespace: openshift-operators\nspec:\n  channel: stable\n  name: my-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n  installPlanApproval: Automatic\n</code></pre> <p>Apply the subscription:</p> <pre><code>oc apply -f subscription.yaml\n</code></pre> <p>Verify the installation:</p> <pre><code># Check the subscription status\noc get subscription my-operator -n openshift-operators\n\n# Check the ClusterServiceVersion (CSV)\noc get csv -n openshift-operators\n\n# Check the Operator pod\noc get pods -n openshift-operators\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#creating-custom-resources","title":"Creating Custom Resources","text":"<p>Once an Operator is installed, you can create instances of the applications it manages using Custom Resources (CRs).</p>"},{"location":"openshift/operators/operatorUsage/#finding-available-apis","title":"Finding Available APIs","text":"<p>Each Operator provides one or more Custom Resource Definitions (CRDs). To see what APIs an Operator provides:</p> <pre><code># List CRDs installed by Operators\noc get crd | grep &lt;operator-name&gt;\n\n# Describe a specific CRD\noc describe crd &lt;crd-name&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#creating-a-custom-resource-via-web-console","title":"Creating a Custom Resource via Web Console","text":"<ol> <li>Go to Operators \u2192 Installed Operators</li> <li>Click on the installed Operator</li> <li>Navigate to the desired API tab</li> <li>Click Create and fill in the form or edit the YAML</li> </ol>"},{"location":"openshift/operators/operatorUsage/#creating-a-custom-resource-via-cli","title":"Creating a Custom Resource via CLI","text":"<p>Example: Creating a PostgreSQL cluster with the Crunchy PostgreSQL Operator:</p> <pre><code>apiVersion: postgres-operator.crunchydata.com/v1beta1\nkind: PostgresCluster\nmetadata:\n  name: my-postgres\n  namespace: my-project\nspec:\n  postgresVersion: 15\n  instances:\n    - name: instance1\n      replicas: 2\n      dataVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n  backups:\n    pgbackrest:\n      repos:\n        - name: repo1\n          volume:\n            volumeClaimSpec:\n              accessModes:\n                - ReadWriteOnce\n              resources:\n                requests:\n                  storage: 5Gi\n</code></pre> <p>Apply the custom resource:</p> <pre><code>oc apply -f postgres-cluster.yaml\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#managing-operator-instances","title":"Managing Operator Instances","text":""},{"location":"openshift/operators/operatorUsage/#viewing-custom-resources","title":"Viewing Custom Resources","text":"<pre><code># List all instances of a custom resource type\noc get &lt;resource-type&gt; -n &lt;namespace&gt;\n\n# Example: List all PostgresClusters\noc get postgresclusters -n my-project\n\n# Get detailed information\noc describe &lt;resource-type&gt; &lt;resource-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#updating-custom-resources","title":"Updating Custom Resources","text":"<p>Modify the custom resource to update the managed application:</p> <pre><code># Edit directly\noc edit &lt;resource-type&gt; &lt;resource-name&gt; -n &lt;namespace&gt;\n\n# Or apply an updated YAML file\noc apply -f updated-resource.yaml\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#deleting-custom-resources","title":"Deleting Custom Resources","text":"<pre><code>oc delete &lt;resource-type&gt; &lt;resource-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>The Operator will automatically clean up all associated resources.</p>"},{"location":"openshift/operators/operatorUsage/#managing-operator-subscriptions","title":"Managing Operator Subscriptions","text":""},{"location":"openshift/operators/operatorUsage/#viewing-subscriptions","title":"Viewing Subscriptions","text":"<pre><code># List all subscriptions\noc get subscriptions -A\n\n# Get subscription details\noc describe subscription &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#updating-an-operator","title":"Updating an Operator","text":"<p>If using manual approval, approve pending updates:</p> <pre><code># Find the InstallPlan\noc get installplan -n &lt;namespace&gt;\n\n# Approve the InstallPlan\noc patch installplan &lt;installplan-name&gt; -n &lt;namespace&gt; \\\n  --type merge --patch '{\"spec\":{\"approved\":true}}'\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#changing-update-channel","title":"Changing Update Channel","text":"<pre><code>oc patch subscription &lt;name&gt; -n &lt;namespace&gt; \\\n  --type merge --patch '{\"spec\":{\"channel\":\"&lt;new-channel&gt;\"}}'\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#uninstalling-an-operator","title":"Uninstalling an Operator","text":""},{"location":"openshift/operators/operatorUsage/#step-1-delete-custom-resources","title":"Step 1: Delete Custom Resources","text":"<p>First, delete all instances managed by the Operator:</p> <pre><code>oc delete &lt;resource-type&gt; --all -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#step-2-delete-the-subscription","title":"Step 2: Delete the Subscription","text":"<pre><code>oc delete subscription &lt;operator-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#step-3-delete-the-clusterserviceversion","title":"Step 3: Delete the ClusterServiceVersion","text":"<pre><code>oc delete csv &lt;csv-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#step-4-optional-delete-crds","title":"Step 4: (Optional) Delete CRDs","text":"<p>If no longer needed, delete the Custom Resource Definitions:</p> <pre><code>oc delete crd &lt;crd-name&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#troubleshooting-operators","title":"Troubleshooting Operators","text":""},{"location":"openshift/operators/operatorUsage/#check-operator-logs","title":"Check Operator Logs","text":"<pre><code># Find the Operator pod\noc get pods -n &lt;operator-namespace&gt;\n\n# View logs\noc logs &lt;operator-pod&gt; -n &lt;operator-namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#check-events","title":"Check Events","text":"<pre><code># Namespace events\noc get events -n &lt;namespace&gt;\n\n# Events for a specific resource\noc describe &lt;resource-type&gt; &lt;resource-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"openshift/operators/operatorUsage/#common-issues","title":"Common Issues","text":"Issue Possible Cause Solution Operator stuck in \"Pending\" Missing dependencies Check InstallPlan for required resources Custom Resource not reconciling Operator not running Check Operator pod status and logs Installation failed Insufficient permissions Verify RBAC and cluster admin access Upgrade blocked Manual approval required Approve the pending InstallPlan"},{"location":"openshift/operators/operatorUsage/#best-practices","title":"Best Practices","text":"<ol> <li>Start with a test namespace - Test Operators in non-production before deploying widely</li> <li>Use specific versions - Pin to specific channels for production stability</li> <li>Monitor Operator health - Set up alerts for Operator pod failures</li> <li>Review RBAC - Understand what permissions the Operator requires</li> <li>Plan for upgrades - Use manual approval in production for controlled upgrades</li> <li>Clean up properly - Always delete CRs before uninstalling Operators</li> </ol>"},{"location":"openshift/pods/","title":"Pods","text":"<p>A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster.</p> <p>A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.</p>"},{"location":"openshift/pods/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> About Pods</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Cluster Configuration for Pods</p> <p>Configure your cluster to work for your specific needs.</p> <p> Learn more</p> </li> <li> <p> Pod Autoscaling</p> <p>Use a horizontal pod autoscaler (HPA) to specify how OCP should automatically scale up or down your deployment.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Pod Overview</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Pod Lifecycle</p> <p>Read about the lifecycle process for pods and what each phase means.</p> <p> Learn more</p> </li> <li> <p> Pod Usage</p> <p>How do you use pods? Read about it here.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes <p>Create Pod using yaml file</p> <pre><code>oc apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>oc get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>oc get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>oc describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>oc logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>oc delete pod myapp-pod\n</code></pre> <p>Create Pod using yaml file</p> <pre><code>kubectl apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>kubectl get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>kubectl describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>kubectl logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>kubectl delete pod myapp-pod\n</code></pre>"},{"location":"openshift/pods/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Pods Create a Pod YAML file to meet certain parameters Pod Creation"},{"location":"openshift/pods/scheduling/","title":"Taints, Tolerations &amp; Node Affinity","text":"<p>Kubernetes provides several mechanisms to control which nodes pods can be scheduled on. These features help you ensure workloads run on appropriate nodes and enable advanced scheduling scenarios.</p>"},{"location":"openshift/pods/scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Taints are applied to nodes and repel pods from being scheduled on them. Tolerations are applied to pods and allow (but don't require) pods to be scheduled on nodes with matching taints.</p>"},{"location":"openshift/pods/scheduling/#taint-effects","title":"Taint Effects","text":"Effect Description <code>NoSchedule</code> Pods without matching toleration won't be scheduled <code>PreferNoSchedule</code> Scheduler tries to avoid placing pods, but not guaranteed <code>NoExecute</code> Evicts existing pods and prevents new scheduling"},{"location":"openshift/pods/scheduling/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Dedicated Nodes: Reserve nodes for specific workloads (GPU, high-memory)</li> <li>Node Maintenance: Drain nodes for updates without affecting other workloads</li> <li>Specialized Hardware: Ensure only appropriate workloads use expensive resources</li> </ul>"},{"location":"openshift/pods/scheduling/#node-affinity","title":"Node Affinity","text":"<p>Node Affinity allows you to constrain which nodes a pod can be scheduled on based on node labels. It's more expressive than <code>nodeSelector</code>.</p>"},{"location":"openshift/pods/scheduling/#affinity-types","title":"Affinity Types","text":"Type Description <code>requiredDuringSchedulingIgnoredDuringExecution</code> Hard requirement - pod won't schedule if not met <code>preferredDuringSchedulingIgnoredDuringExecution</code> Soft preference - scheduler tries to meet it"},{"location":"openshift/pods/scheduling/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Controlling Pod Placement </p> <p>Node Affinity </p> <p>Taints and Tolerations </p> <p>Node Affinity </p>"},{"location":"openshift/pods/scheduling/#references","title":"References","text":""},{"location":"openshift/pods/scheduling/#taints-and-tolerations_1","title":"Taints and Tolerations","text":"<p>Adding a taint to a node</p> <pre><code>kubectl taint nodes node1 dedicated=gpu:NoSchedule\n</code></pre> <p>Pod with toleration</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  containers:\n    - name: cuda-app\n      image: nvidia/cuda:latest\n  tolerations:\n    - key: \"dedicated\"\n      operator: \"Equal\"\n      value: \"gpu\"\n      effect: \"NoSchedule\"\n</code></pre> <p>Tolerate all taints with a specific key</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: tolerant-pod\nspec:\n  containers:\n    - name: app\n      image: nginx\n  tolerations:\n    - key: \"dedicated\"\n      operator: \"Exists\"\n      effect: \"NoSchedule\"\n</code></pre> <p>Toleration for control plane nodes</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: control-plane-pod\nspec:\n  containers:\n    - name: app\n      image: nginx\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\n      operator: \"Exists\"\n      effect: \"NoSchedule\"\n</code></pre>"},{"location":"openshift/pods/scheduling/#node-affinity_1","title":"Node Affinity","text":"<p>Required node affinity (hard requirement)</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: affinity-pod\nspec:\n  containers:\n    - name: app\n      image: nginx\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: topology.kubernetes.io/zone\n                operator: In\n                values:\n                  - us-east-1a\n                  - us-east-1b\n</code></pre> <p>Preferred node affinity (soft preference)</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: preferred-affinity-pod\nspec:\n  containers:\n    - name: app\n      image: nginx\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          preference:\n            matchExpressions:\n              - key: node-type\n                operator: In\n                values:\n                  - high-memory\n        - weight: 50\n          preference:\n            matchExpressions:\n              - key: node-type\n                operator: In\n                values:\n                  - standard\n</code></pre> <p>Combined taints, tolerations, and affinity</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gpu-workload\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gpu-workload\n  template:\n    metadata:\n      labels:\n        app: gpu-workload\n    spec:\n      containers:\n        - name: cuda-app\n          image: nvidia/cuda:latest\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: accelerator\n                    operator: In\n                    values:\n                      - nvidia-tesla-v100\n</code></pre>"},{"location":"openshift/pods/scheduling/#pod-anti-affinity","title":"Pod Anti-Affinity","text":"<p>Pod anti-affinity ensures pods are spread across different nodes or zones for high availability.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web-server\n  template:\n    metadata:\n      labels:\n        app: web-server\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                      - web-server\n              topologyKey: \"kubernetes.io/hostname\"\n</code></pre> OpenShiftKubernetes Add Taint to Node<pre><code>oc adm taint nodes node1 dedicated=gpu:NoSchedule\n</code></pre> Remove Taint from Node<pre><code>oc adm taint nodes node1 dedicated=gpu:NoSchedule-\n</code></pre> View Node Taints<pre><code>oc describe node node1 | grep -A5 Taints\n</code></pre> Label a Node<pre><code>oc label nodes node1 node-type=high-memory\n</code></pre> View Node Labels<pre><code>oc get nodes --show-labels\n</code></pre> Add Taint to Node<pre><code>kubectl taint nodes node1 dedicated=gpu:NoSchedule\n</code></pre> Remove Taint from Node<pre><code>kubectl taint nodes node1 dedicated=gpu:NoSchedule-\n</code></pre> View Node Taints<pre><code>kubectl describe node node1 | grep -A5 Taints\n</code></pre> Label a Node<pre><code>kubectl label nodes node1 node-type=high-memory\n</code></pre> View Node Labels<pre><code>kubectl get nodes --show-labels\n</code></pre>"},{"location":"openshift/pods/scheduling/#comparison","title":"Comparison","text":"Feature Use Case Scope nodeSelector Simple label matching Node labels Taints/Tolerations Repel pods from nodes Node-level Node Affinity Complex node selection rules Node labels Pod Affinity Co-locate pods together Pod labels Pod Anti-Affinity Spread pods apart Pod labels"},{"location":"openshift/pods/health-checks/","title":"Health and Monitoring","text":""},{"location":"openshift/pods/health-checks/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"<p>A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers:</p> <p>ExecAction: Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0.</p> <p>TCPSocketAction: Performs a TCP check against the Container\u2019s IP address on a specified port. The diagnostic is considered successful if the port is open.</p> <p>HTTPGetAction: Performs an HTTP Get request against the Container\u2019s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</p> <p>The kubelet can optionally perform and react to three kinds of probes on running Containers:</p> <p>livenessProbe: Indicates whether the Container is running. Runs for the lifetime of the Container.</p> <p>readinessProbe: Indicates whether the Container is ready to service requests. Only runs at start.</p>"},{"location":"openshift/pods/health-checks/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Application Health</p> <p>A health check periodically performs diagnostics on a running container using any combination of the readiness, liveness, and startup health checks.</p> <p> Learn more</p> </li> <li> <p> Virtual Machine Health</p> <p>Use readiness and liveness probes to detect and handle unhealthy virtual machines (VMs).</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Container Probes</p> <p>To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.</p> <p> Learn more</p> </li> <li> <p> Configure Probes</p> <p>Read about how to configure liveness, readiness and startup probes for containers.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: app\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello, Kubernetes! &amp;&amp; sleep 3600\"]\n      livenessProbe:\n        exec:\n          command: [\"echo\", \"alive\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      livenessProbe:\n        tcpSocket:\n          port: 8080\n        initialDelaySeconds: 10\n      readinessProbe:\n        httpGet:\n          path: /\n          port: 8080\n        periodSeconds: 10\n</code></pre>"},{"location":"openshift/pods/health-checks/#container-logging","title":"Container Logging","text":"<p>Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.</p> <p>Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.</p>"},{"location":"openshift/pods/health-checks/#resources_1","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Logs Command</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> <li> <p> Cluster Logging</p> <p>As a cluster administrator, you can deploy logging on an OpenShift Container Platform cluster, and use it to collect and aggregate node system audit logs, application container logs, and infrastructure logs.</p> <p> Learn more</p> </li> <li> <p> Logging Collector</p> <p>The collector collects log data from each node, transforms the data, and forwards it to configured outputs.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Logging</p> <p>Application logs can help you understand what is happening inside your application and are particularly useful for debugging problems and monitoring cluster activity.</p> <p> Getting started</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references_1","title":"References","text":"Pod Example<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n    - name: count\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done',\n        ]\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre>"},{"location":"openshift/pods/health-checks/#monitoring-applications","title":"Monitoring Applications","text":"<p>To scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application\u2019s resource usage at each of these levels. This information allows you to evaluate your application\u2019s performance and where bottlenecks can be removed to improve overall performance.</p> <p>Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.</p>"},{"location":"openshift/pods/health-checks/#resources_2","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Monitoring Application Health</p> <p>OpenShift Container Platform applications have a number of options to detect and handle unhealthy containers.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Monitoring Resource Usage</p> <p>You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster.</p> <p> Learn more</p> </li> <li> <p> Resource Metrics</p> <p>For Kubernetes, the Metrics API offers a basic set of metrics to support automatic scaling and similar use cases.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/health-checks/#references_2","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 500m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 700m\n          memory: 128Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=500&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 200m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 300m\n          memory: 64Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=200&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> OpenShiftKubernetes <pre><code>oc get projects\noc api-resources -o wide\noc api-resources -o name\n\noc get nodes,ns,po,deploy,svc\n\noc describe node --all\n</code></pre> <p>Verify Metrics is enabled <pre><code>kubectl get --raw /apis/metrics.k8s.io/\n</code></pre></p> <p>Get Node Description <pre><code>kubectl describe node\n</code></pre></p> <p>Check Resource Usage <pre><code>kubectl top pods\nkubectl top nodes\n</code></pre></p> <p></p> <p></p>"},{"location":"openshift/pods/health-checks/#activities","title":"Activities","text":"Task Description Link Try It Yourself Probes Create some Health &amp; Startup Probes to find what's causing an issue. Probes"},{"location":"openshift/pods/jobs/","title":"Jobs and CronJobs","text":"<p>Jobs</p> <p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</p> <p>CronJobs</p> <p>One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format.</p> <p>All CronJob schedule: times are based on the timezone of the master where the job is initiated.</p>"},{"location":"openshift/pods/jobs/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Jobs </p> <p>CronJobs </p> <p>Jobs to Completion </p> <p>Cron Jobs </p> <p>Automated Tasks with Cron </p>"},{"location":"openshift/pods/jobs/#references","title":"References","text":"<p>It computes \u03c0 to 2000 places and prints it out</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>Running in parallel</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  parallelism: 2\n  completions: 3\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: hello\n              image: busybox\n              args:\n                - /bin/sh\n                - -c\n                - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre> OpenShiftKubernetes <p>Gets Jobs <pre><code>oc get jobs\n</code></pre> Gets Job Description <pre><code>oc describe job pi\n</code></pre> Gets Pods from the Job <pre><code>oc get pods\n</code></pre> Deletes Job <pre><code>oc delete job pi\n</code></pre> Gets CronJob <pre><code>oc get cronjobs\n</code></pre> Describes CronJob <pre><code>oc describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>oc get pods\n</code></pre> Deletes CronJob <pre><code>oc delete cronjobs pi\n</code></pre></p> <p>Gets Jobs <pre><code>kubectl get jobs\n</code></pre> Gets Job Description <pre><code>kubectl describe job pi\n</code></pre> Gets Pods from the Job <pre><code>kubectl get pods\n</code></pre> Deletes Job <pre><code>kubectl delete job pi\n</code></pre> Gets CronJob <pre><code>kubectl get cronjobs\n</code></pre> Describes CronJob <pre><code>kubectl describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>kubectl get pods\n</code></pre> Deletes CronJob <pre><code>kubectl delete cronjobs pi\n</code></pre></p>"},{"location":"openshift/pods/jobs/#activities","title":"Activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Create a CronJob to run periodic tasks in your cluster. Cron Jobs"},{"location":"openshift/pods/multi-container/","title":"Multi-Containers Pod","text":"<p>Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers. By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers.</p>"},{"location":"openshift/pods/multi-container/#resources","title":"Resources","text":"Kubernetes <ul> <li> <p> Sidecar Logging</p> <p>Application logs can help you understand what is happening inside your application.</p> <p> Learn more</p> </li> <li> <p> Shared Volume Communication</p> <p>Read about how to use a Volume to communicate between two Containers running in the same Pod.</p> <p> Learn more</p> </li> <li> <p> Toolkit Patterns</p> <p>Read Brendan Burns' blog post about \"The Distributed System ToolKit: Patterns for Composite Containers\".</p> <p> Learn more</p> </li> <li> <p> Brendan Burns Paper</p> <p>Read Brendan Burns' paper about design patterns for container-based distributed systems.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/multi-container/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n    - name: shared-data\n      emptyDir: {}\n  containers:\n    - name: app\n      image: bitnami/nginx\n      volumeMounts:\n        - name: shared-data\n          mountPath: /app\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      volumeMounts:\n        - name: shared-data\n          mountPath: /pod-data\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo Hello from the side container &gt; /pod-data/index.html &amp;&amp; sleep 3600\",\n        ]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      securityContext:\n        capabilities:\n          add:\n            - SYS_PTRACE\n      stdin: true\n      tty: true\n</code></pre> OpenShiftKubernetes <p>Attach Pods Together <pre><code>oc attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre></p> <p>Attach Pods Together <pre><code>kubectl attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre></p>"},{"location":"openshift/pods/multi-container/#activities","title":"Activities","text":"Task Description Link Try It Yourself Multiple Containers Build a container using legacy container image. Multiple Containers"},{"location":"openshift/pods/tagging/","title":"Labels, Selectors, and Annotations","text":"<p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.</p> <p>You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.</p>"},{"location":"openshift/pods/tagging/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> CLI Label Commands</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Labels</p> <p>Labels can be used to organize and to select subsets of objects.</p> <p> Learn more</p> </li> <li> <p> Annotations</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/tagging/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  labels:\n    app: foo\n    tier: frontend\n    env: dev\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"\n    gitrepo: \"https://github.com/csantanapr/knative\"\nspec:\n  containers:\n    - name: app\n      image: bitnami/nginx\n</code></pre> OpenShiftKubernetes <p>Change Labels on Objects <pre><code>oc label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get pods -L tier,env\n</code></pre> <pre><code>oc get pods -l app\n</code></pre> <pre><code>oc get pods -l tier=frontend\n</code></pre> <pre><code>oc get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>oc get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>oc get pods -l 'tier!=backend'\n</code></pre> <pre><code>oc get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>oc delete pod my-pod\n</code></pre></p> <p>Change Labels on Objects <pre><code>kubectl label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get pods -L tier,env\n</code></pre> <pre><code>kubectl get pods -l app\n</code></pre> <pre><code>kubectl get pods -l tier=frontend\n</code></pre> <pre><code>kubectl get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>kubectl get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>kubectl get pods -l 'tier!=backend'\n</code></pre> <pre><code>kubectl get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>kubectl delete pod my-pod\n</code></pre></p>"},{"location":"openshift/pods/troubleshooting/","title":"Debugging Applications","text":"<p>Kubernetes provides tools to help troubleshoot and debug problems with applications.</p> <p>Usually is getting familiar with how primitives objects interact with each other, checking the status of objects, and finally checking logs for any last resource clues.</p>"},{"location":"openshift/pods/troubleshooting/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Debugging ODO</p> <p>OpenShift Toolkit is an IDE plugin available on VS Code and JetBrains IDEs, that allows you to do all things that <code>odo</code> does, i.e. create, test, debug and deploy cloud-native applications on a cloud-native environment in simple steps.</p> <p> Getting started</p> </li> </ul> <ul> <li> <p> Debugging Applications</p> <p>Read about how to debug applications that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> <li> <p> Debugging Services</p> <p>You've run your Pods through a Deployment and created a Service, but you get no response when you try to access it. What do you do?</p> <p> Learn more</p> </li> <li> <p> Debugging Replication Controllers</p> <p>Read about how to debug replication controllers that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> </ul>"},{"location":"openshift/pods/troubleshooting/#references","title":"References","text":"OpenShiftKubernetes <p>MacOS/Linux/Windows command: <pre><code>oc apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre></p> <p>Expose the service using port-forward <pre><code>oc port-forward service/my-service 8080:80 -n debug\n</code></pre></p> <p>Try to access the service <pre><code>curl http://localhost:8080\n</code></pre></p> <p>Try Out these Commands to Debug <pre><code>oc get pods --all-namespaces\n</code></pre> <pre><code>oc project debug\n</code></pre> <pre><code>oc get deployments\n</code></pre> <pre><code>oc describe pod\n</code></pre> <pre><code>oc explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>oc explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>oc edit deployment\n</code></pre> <pre><code>oc logs\n</code></pre> <pre><code>oc get service\n</code></pre> <pre><code>oc get ep\n</code></pre> <pre><code>oc describe service\n</code></pre> <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get deployment --show-labels\n</code></pre></p> <p>MacOS/Linux/Windows command: <pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre></p> <p>Expose the service using port-forward <pre><code>kubectl port-forward service/my-service 8080:80 -n debug\n</code></pre></p> <p>Try to access the service <pre><code>curl http://localhost:8080\n</code></pre></p> <p>Try Out these Commands to Debug <pre><code>kubectl get pods --all-namespaces\n</code></pre> <pre><code>kubectl config set-context --current --namespace=debug\n</code></pre> <pre><code>kubectl get deployments\n</code></pre> <pre><code>kubectl describe pod\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>kubectl edit deployment\n</code></pre> <pre><code>kubectl logs\n</code></pre> <pre><code>kubectl get service\n</code></pre> <pre><code>kubectl get ep\n</code></pre> <pre><code>kubectl describe service\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get deployment --show-labels\n</code></pre></p>"},{"location":"openshift/pods/troubleshooting/#activities","title":"Activities","text":"Task Description Link Try It Yourself Debugging Find which service is breaking in your cluster and find out why. Debugging"},{"location":"openshift/services-networking/","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"openshift/services-networking/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"openshift/services-networking/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"openshift/services-networking/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress"},{"location":"openshift/services-networking/ingress/","title":"Ingress","text":"<p>An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress can provide load balancing, SSL termination and name-based virtual hosting.</p> <p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.</p>"},{"location":"openshift/services-networking/ingress/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Ingress Operator </p> <p>Using Ingress Controllers </p> <p>Ingress </p> <p>Ingress Controllers </p> <p>Minikube Ingress </p>"},{"location":"openshift/services-networking/ingress/#references","title":"References","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\n</code></pre> OpenShiftKubernetes View Ingress Status<pre><code>oc describe clusteroperators/ingress\n</code></pre> Describe default Ingress Controller<pre><code>oc describe --namespace=openshift-ingress-operator ingresscontroller/default\n</code></pre> <p>Describe default Ingress Controller<pre><code>kubectl get pods -n kube-system | grep ingress\n</code></pre> <pre><code>kubectl create deployment web --image=bitnami/nginx\n</code></pre> <pre><code>kubectl expose deployment web --name=web --port 8080\n</code></pre> <pre><code>kubectl get svc web\n</code></pre> <pre><code>kubectl get ingress\n</code></pre> <pre><code>kubcetl describe ingress example-ingress\n</code></pre> <pre><code>curl hello-world.info --resolve hello-world.info:80:&lt;ADDRESS&gt;\n</code></pre></p>"},{"location":"openshift/services-networking/ingress/#activities","title":"Activities","text":"Task Description Link Try It Yourself IKS Ingress Controller Configure Ingress on Free IKS Cluster Setting IKS Ingress"},{"location":"openshift/services-networking/network-policies/","title":"Network Policies","text":"<p>Network Policies are Kubernetes resources that control traffic flow between pods and network endpoints. By default, pods are non-isolated and accept traffic from any source. Network Policies allow you to specify how pods can communicate with each other and with other network endpoints.</p>"},{"location":"openshift/services-networking/network-policies/#how-network-policies-work","title":"How Network Policies Work","text":"<ul> <li>Network Policies are namespace-scoped</li> <li>They use labels to select pods and define rules for traffic</li> <li>Policies are additive - if any policy allows a connection, it is allowed</li> <li>If no policies select a pod, all traffic is allowed (default behavior)</li> <li>Once a pod is selected by any Network Policy, it rejects all traffic not explicitly allowed</li> </ul>"},{"location":"openshift/services-networking/network-policies/#policy-types","title":"Policy Types","text":"Type Description Ingress Controls incoming traffic to selected pods Egress Controls outgoing traffic from selected pods"},{"location":"openshift/services-networking/network-policies/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Network Policies </p> <p>Creating Network Policies </p> <p>Network Policies </p> <p>Declare Network Policy </p>"},{"location":"openshift/services-networking/network-policies/#references","title":"References","text":"<p>Default Deny All Ingress Traffic</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}  # Selects all pods in namespace\n  policyTypes:\n    - Ingress\n</code></pre> <p>Default Deny All Egress Traffic</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n    - Egress\n</code></pre> <p>Allow Traffic from Specific Pods</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: frontend\n      ports:\n        - protocol: TCP\n          port: 8080\n</code></pre> <p>Allow Traffic from Specific Namespace</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-monitoring\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: monitoring\n</code></pre> <p>Allow Egress to Specific CIDR and DNS</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-egress\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Egress\n  egress:\n    # Allow DNS\n    - to: []\n      ports:\n        - protocol: UDP\n          port: 53\n    # Allow specific external IPs\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/8\n            except:\n              - 10.0.1.0/24\n      ports:\n        - protocol: TCP\n          port: 443\n</code></pre> <p>Complete Example: Database Access Policy</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n      tier: backend\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    # Only allow backend pods to connect\n    - from:\n        - podSelector:\n            matchLabels:\n              tier: backend\n        - namespaceSelector:\n            matchLabels:\n              environment: production\n      ports:\n        - protocol: TCP\n          port: 5432\n  egress:\n    # Allow DNS lookups\n    - to: []\n      ports:\n        - protocol: UDP\n          port: 53\n</code></pre> OpenShiftKubernetes Create Network Policy<pre><code>oc apply -f network-policy.yaml\n</code></pre> Get Network Policies<pre><code>oc get networkpolicies\n</code></pre> Describe Network Policy<pre><code>oc describe networkpolicy allow-frontend-to-backend\n</code></pre> Delete Network Policy<pre><code>oc delete networkpolicy default-deny-ingress\n</code></pre> Get Network Policies in All Namespaces<pre><code>oc get networkpolicies -A\n</code></pre> Create Network Policy<pre><code>kubectl apply -f network-policy.yaml\n</code></pre> Get Network Policies<pre><code>kubectl get networkpolicies\n</code></pre> Describe Network Policy<pre><code>kubectl describe networkpolicy allow-frontend-to-backend\n</code></pre> Delete Network Policy<pre><code>kubectl delete networkpolicy default-deny-ingress\n</code></pre> Get Network Policies in All Namespaces<pre><code>kubectl get networkpolicies -A\n</code></pre>"},{"location":"openshift/services-networking/network-policies/#best-practices","title":"Best Practices","text":"<ol> <li>Start with Default Deny - Create a default deny policy, then explicitly allow required traffic</li> <li>Use Labels Consistently - Establish a labeling convention for apps, tiers, and environments</li> <li>Test Policies - Verify policies work as expected before applying to production</li> <li>Document Policies - Keep track of what traffic flows are allowed and why</li> <li>Monitor Traffic - Use network monitoring tools to detect policy violations</li> </ol>"},{"location":"openshift/services-networking/network-policies/#activities","title":"Activities","text":"Task Description Link Try It Yourself Network Policies Create a policy to allow client pods with labels to access secure pod. Network Policies"},{"location":"openshift/services-networking/routes/","title":"Routes","text":"<p>OpenShift Only</p> <p>Routes are Openshift objects that expose services for external clients to reach them by name.</p> <p>Routes can insecured or secured on creation using certificates.</p> <p>The new route inherits the name from the service unless you specify one using the --name option.</p>"},{"location":"openshift/services-networking/routes/#resources","title":"Resources","text":"OpenShift <p>Routes </p> <p>Route Configuration </p> <p>Secured Routes </p>"},{"location":"openshift/services-networking/routes/#references","title":"References","text":"<p>Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n</code></pre> <p>Secured Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n  tls:\n    termination: edge\n</code></pre>"},{"location":"openshift/services-networking/routes/#commands","title":"Commands","text":"OpenShift Create Route from YAML<pre><code>oc apply -f route.yaml\n</code></pre> Get Route<pre><code>oc get route\n</code></pre> Describe Route<pre><code>oc get route &lt;route_name&gt;\n</code></pre> Get Route YAML<pre><code>oc get route &lt;route_name&gt; -o yaml\n</code></pre>"},{"location":"openshift/services-networking/services/","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"openshift/services-networking/services/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"openshift/services-networking/services/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"openshift/services-networking/services/#activities","title":"Activities","text":"Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services"},{"location":"openshift/state-persistence/","title":"State Persistence","text":"<p>State persistence in the context of Kubernetes/OpenShift refers to the ability to maintain and retain the state or data of applications even when they are stopped, restarted, or moved between nodes.</p> <p>This is achieved through the use of volumes, persistent volumes (PVs), and persistent volume claims (PVCs). Volumes provide a way to store and access data in a container, while PVs serve as the underlying storage resources provisioned by the cluster. PVCs act as requests made by applications for specific storage resources from the available PVs. By utilizing PVs and PVCs, applications can ensure that their state is preserved and accessible across pod restarts and migrations, enabling reliable and consistent data storage and retrieval throughout the cluster.</p>"},{"location":"openshift/state-persistence/#resources","title":"Resources","text":"<p>Volumes </p> <p>Persistent Volumes </p> <p>Persistent Volume Claims </p>"},{"location":"openshift/state-persistence/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>In this example, we define a PVC named my-pvc with the following specifications:</p> <ul> <li>accessModes specify that the volume can be mounted as read-write by a single node at a time (\"ReadWriteOnce\")</li> <li>resources.requests.storage specifies the requested storage size for the PVC (\"1Gi\")</li> </ul>"},{"location":"openshift/state-persistence/#activities","title":"Activities","text":"Task Description Link Try It Yourself Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes"},{"location":"openshift/state-persistence/pv-pvc/","title":"PersistentVolumes and Claims","text":"<p>Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.</p> <p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.</p> <p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only).</p> <p>While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.</p> <p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.</p> <p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (ROX, RWX) is only possible within one namespace.</p>"},{"location":"openshift/state-persistence/pv-pvc/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Persistent Storage </p> <p>Persistent Volume Types </p> <p>Expanding Peristent Volumes </p> <p>Persistent Volumes </p> <p>Writing Portable Configurations </p> <p>Configuring Persistent Volume Storage </p>"},{"location":"openshift/state-persistence/pv-pvc/#references","title":"References","text":"<pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: my-pv\nspec:\n  storageClassName: local-storage\n  capacity:\n    storage: 128Mi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data-1\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: local-storage\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\n</code></pre> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: nginx\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo $(date):$HOSTNAME Hello Kubernetes! &gt;&gt; /mnt/data/message.txt &amp;&amp; sleep 3600\",\n        ]\n      volumeMounts:\n        - mountPath: \"/mnt/data\"\n          name: my-data\n  volumes:\n    - name: my-data\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre> OpenShiftKubernetes Get the Persistent Volumes in Project<pre><code>oc get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>oc get pvc\n</code></pre> Get a specific Persistent Volume<pre><code>oc get pv &lt;pv_claim&gt;\n</code></pre> Get the Persistent Volume<pre><code>kubectl get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>kubectl get pvc\n</code></pre>"},{"location":"openshift/state-persistence/pv-pvc/#activities","title":"Activities","text":"Task Description Link Try It Yourself Persistent Volumes Create a Persistent Volume that's accessible from a Pod. Persistent Volumes"},{"location":"openshift/state-persistence/volumes/","title":"Volumes","text":"<p>On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. First, when a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. Second, when running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems.</p> <p>Docker also has a concept of volumes, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another Container.</p> <p>A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.</p>"},{"location":"openshift/state-persistence/volumes/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Volume Lifecycle </p> <p>Volumes </p>"},{"location":"openshift/state-persistence/volumes/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n      name: busybox\n      volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n  volumes:\n    - name: cache-volume\n      emptyDir: {}\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n    - image: bitnami/nginx\n      name: test-container\n      volumeMounts:\n        - mountPath: /test-pd\n          name: test-volume\n  volumes:\n    - name: test-volume\n      hostPath:\n        # directory location on host\n        path: /data\n        # this field is optional\n        type: Directory\n</code></pre>"}]}